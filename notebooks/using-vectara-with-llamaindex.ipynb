{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara and LlamaIndex\n",
    "\n",
    "In this notebook we are going to show how to use Vectara with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. An integrated API for processing input data, including text extraction from documents and ML-based chunking.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedding model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embeddings and retrieves the most relevant text segmentsthrough [hybrid search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and a variety of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking) strategies, including a [multilingual reranker](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker), [maximal marginal relevance (MMR) reranker](https://docs.vectara.com/docs/learn/mmr-reranker), [user-defined function reranker](https://docs.vectara.com/docs/learn/user-defined-function-reranker), and a [chain reranker](https://docs.vectara.com/docs/learn/chain-reranker) that provides a way to chain together multiple reranking methods to achieve better control over the reranking, combining the strengths of various reranking methods.\n",
    "\n",
    "4. An option to create a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview) with a wide selection of LLM summarizers (including Vectara's [Mockingbird](https://vectara.com/blog/mockingbird-is-a-rag-specific-llm-that-beats-gpt-4-gemini-1-5-pro-in-rag-output-quality/), trained specifically for RAG-based tasks), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits of using Vectara RAG-as-a-service to build your application are:\n",
    "* **Accuracy and Quality**: Vectara provides an end-to-end platform that focuses on eliminating hallucinations, reducing bias, and safeguarding copyright integrity.\n",
    "* **Security**: Vectara's platform provides acess control--protecting against prompt injection attacks--and meets SOC2 and HIPAA compliance.\n",
    "* **Explainability**: Vectara makes it easy to troubleshoot bad results by clearly explaining rephrased queries, LLM prompts, retrieved results, and agent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33079b",
   "metadata": {},
   "source": [
    "## About LlamaIndex\n",
    "\n",
    "LlamaIndex is a \"data framework\" to help you build LLM apps:\n",
    "\n",
    "1. It includes **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "2. It provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\n",
    "3. It provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "\n",
    "LlamaIndex's high-level API allows beginner users to use LlamaIndex to ingest and query their data in just a few lines of code, whereas its lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "Vectara is implemented in LlamaIndex as a [Managed Service](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices.html#vectara), abstracting all of Vectara's powerful API so they are easily integrated into LlamaIndex.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\n",
    "\n",
    "To get started with Vectara, [sign up](https://console.vectara.com/signup?utm_source=vectara&utm_medium=signup&utm_term=DevRel&utm_content=example-notebooks&utm_campaign=vectara-signup-DevRel-example-notebooks) (if you haven't already) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \n",
    "\n",
    "Once you have these, you can provide them as environment variables, which will be used by the LlamaIndex code later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U llama-index llama-index-indices-managed-vectara arxiv\n",
    "\n",
    "import os\n",
    "# os.environ['VECTARA_API_KEY'] = \"<YOUR_VECTARA_API_KEY>\"\n",
    "# os.environ['VECTARA_CORPUS_ID'] = \"<YOUR_VECTARA_CORPUS_ID>\"\n",
    "# os.environ['VECTARA_CUSTOMER_ID'] = \"<YOUR_VECTARA_CUSTOMER_ID>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Loading Data Into Vectara\n",
    "\n",
    "As mentioned above, Vectara is a RAG managed service, and in many cases data may be uploaded to the index ahead of time (e.g. by using [Airbyte](https://docs.airbyte.com/integrations/destinations/vectara), directly via Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) or using tools like [vectara-ingest](https://github.com/vectara/vectara-ingest)), but another easy way is via the VectaraIndex constructor: `from_documents()`.\n",
    "\n",
    "For this notebook, we will assume the Vectara corpus is empty and will load PDF documents from Arxiv, using Python's [arxiv](https://github.com/lukasschwab/arxiv.py) library. We will pull in data from the top papers related to \"climate change\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40947545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"(ti:embedding model) OR (ti:sentence embedding)\",\n",
    "  max_results = 100,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae7bd09-6569-48cc-8c54-8bf491c0e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2402.14776v2',\n",
       " 'http://arxiv.org/abs/2007.01852v2',\n",
       " 'http://arxiv.org/abs/1910.13291v1',\n",
       " 'http://arxiv.org/abs/2104.06719v1',\n",
       " 'http://arxiv.org/abs/1511.08198v3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.entry_id for p in papers][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "Next, download the Arxiv paper, and upload them into Vectara using the `add_file()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File temp/1909.03104v2.Efficient_Sentence_Embedding_using_Discrete_Cosine_Transform.pdf failed to load with error HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from llama_index.indices.managed.vectara import VectaraIndex\n",
    "\n",
    "data_folder = 'temp'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Create Vectara Index\n",
    "index = VectaraIndex()\n",
    "\n",
    "# Upload content for all papers\n",
    "for paper in papers:\n",
    "    try:\n",
    "        paper_fname = paper.download_pdf(data_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"File {paper_fname} failed to load with error {e}\")\n",
    "        continue\n",
    "    metadata = {\n",
    "        'url': paper.pdf_url,\n",
    "        'title': paper.title,\n",
    "        'author': str(paper.authors[0]),\n",
    "        'published': str(paper.published.date())\n",
    "    }\n",
    "    index.insert_file(file_path=paper_fname, metadata=metadata)\n",
    "\n",
    "shutil.rmtree(data_folder)\n",
    "del papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea571",
   "metadata": {},
   "source": [
    "Two important things to note here:\n",
    "1. Vectara processes each file uploaded on the backend, and performs appropriate chunking. So you don't need to apply any local processing, or choose a chunking strategy. \n",
    "2. We have used the fields `url`, `title`, `author`, and `published` as metadata fields (for simplicity, author is the first author if there are multiple). You will need to make sure those fields are defined in your Vectara corpus as [filterable metadata fields](https://docs.vectara.com/docs/learn/metadata-search-filtering/filter-overview) to ensure we can filter by them in query time.\n",
    "\n",
    "So that's it for upload. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Querying with the VectaraIndex\n",
    "We can now ask questions using the `VectaraIndex` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb174ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is sentence embedding?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, sentence embedding can be summarized as follows:\n",
      "\n",
      "Sentence embedding is a form of word or sentence representation that maps text data into vectors, which can be a set of real numbers (a vector) [1]. It is a term used to represent words or sentences in a text that encodes the meaning of the word or the sentence in n-dimensional space [1]. The goal of sentence embedding is to make the embeddings of two sentences that are similar to get closer in this vector space [2]. This is achieved by training the sentence embedding model to capture the meaning of the sentence, and it is expected that sentences that are closer in the vector space are more similar [1].\n",
      "\n",
      "There are different approaches to sentence embedding, including traditional word embedding, static word embedding, contextualized word embedding, and two-sentence embeddings approach, with non-parameterized and parameterized models [1]. Sentence embedding has gained attention in recent years, particularly in the context of natural language processing (NLP), information extraction (IE), and neural machine translation (NMT) tasks [2].\n",
      "\n",
      "Sources: [1], [2]\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True, summary_num_results=5,\n",
    "    summary_response_lang=\"eng\",\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\"\n",
    ")\n",
    "res = query_engine.query(query)\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52878fd2",
   "metadata": {},
   "source": [
    "Note that the response here is fully generated by Vectara. There is no additional LLM involved (or API key you need to setup). The response also includes citations (marked in square brackets), which provide links to references used to generate this response by Vectara. \n",
    "<br>\n",
    "The `res` object includes the actual response to the user query, but also has the citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b110a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (1, 'http://arxiv.org/pdf/1910.13291v1'),\n",
       " (2, 'http://arxiv.org/pdf/2305.03010v1'),\n",
       " (3, 'http://arxiv.org/pdf/2305.03010v1'),\n",
       " (4, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (5, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (6, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (7, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (8, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (9, 'http://arxiv.org/pdf/1904.05542v1')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(res.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cce7c7-c664-4001-8c84-f80c5d499796",
   "metadata": {},
   "source": [
    "## Using Streaming\n",
    "\n",
    "You can also stream the Vectara response simply by specifying `streaming=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd30027-d975-4440-a1c2-66d1bd30bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, sentence embedding is a representation of a sentence in a vector space that encodes the meaning of the sentence. It is a form of word or sentence representation that prepares texts in an understandable format for a machine [1]. Sentence embeddings are expected to map sentences that are closer in the vector space to be more similar [1]. There are different approaches to sentence embeddings, including non-parameterized and parameterized models [2]. Sentence embeddings have gained attention in recent years, particularly in natural language processing (NLP), information extraction (IE), and neural machine translation (NMT) tasks [2].\n",
      "\n",
      "Sources: [1], [2]"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True,\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\",\n",
    "    streaming=True)\n",
    "\n",
    "res = query_engine.query(query)\n",
    "\n",
    "# print streamed output chunk by chunk\n",
    "for chunk in res.response_gen:\n",
    "    print(chunk.delta or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cd598-ce73-4c9e-9aa6-5d8604fd6c14",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "Vectara supports three types of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking):\n",
    "1. [Maximal Marginal Relevance](https://docs.vectara.com/docs/learn/mmr-reranker), or MMR, provides a reranking that can promote diversity in results at the cost of relevance.\n",
    "2. [Slingshot](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker) is a mulitilingual reranker that increases the accuracy of retrieved results across 100+ languages and is available to Vectara Scale customers.\n",
    "3. [User Defined Functions](https://docs.vectara.com/docs/learn/user-defined-function-reranker) allow you to create your own functions for reranking search results, unlocking better retrieval in a wide variety of use cases, such as sorting by recency or price of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49914a",
   "metadata": {},
   "source": [
    " Let's see an example of how to use MMR: We will run the same query but this time we will use MMR where `mmr_diversity_bias=0.3` provides a tradeoff between relevance and diversity (0.0 is full relevance, 1.0 is only diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72832e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a method of representing words or sentences in a text by encoding their meaning in n-dimensional space. It involves mapping text data into vectors of real numbers, where words or sentences closer in the vector space are more similar. Different types of embeddings exist, such as traditional, static, and contextualized word embeddings, as well as non-parameterized and parameterized models for sentence embeddings. This approach aims to prepare texts in a machine-understandable format, facilitating various natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    reranker=\"mmr\",\n",
    "    rerank_k=50,\n",
    "    mmr_diversity_bias=0.3,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17020d34-5176-4910-bb88-5c5685513804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (1, 'http://arxiv.org/pdf/2305.03010v1'),\n",
       " (2, 'http://arxiv.org/pdf/2305.15077v2'),\n",
       " (3, 'http://arxiv.org/pdf/2402.12890v1'),\n",
       " (4, 'http://arxiv.org/pdf/2404.17606v1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e76fd1",
   "metadata": {},
   "source": [
    "As you can see, the results are now reranked in a way that provides more diversity instead of maximizing pure relevance. This in turn results in a different set of chunks used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562956f5-b9b7-4288-8100-f443a5dc3e4d",
   "metadata": {},
   "source": [
    "Now let's see an example with a user defined function. We may be interested in getting results that are the most semantically similar to our question, but we also want the most up-to-date information. Thus, we can bias our search results so that the papers that are not only semantically similar but also published more recently are used to answer our query. We can do this by using the available time functions (to see other built-in functions, see the UDF Reranker [documentation](https://docs.vectara.com/docs/learn/user-defined-function-reranker)).\n",
    "\n",
    "Vectara also supports chain-reranking, which provides a way to chain together multiple reranking methods to achieve better control over the reranking, and combining the strengths of various reranking methods. A great way to use the UDF reranker is in a chain: first the multilingual reranker, followed by the maximal marginal relevance (MMR) reranker, and then a user-defined function, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d32ff2f1-ccd2-4fd0-99df-fd977182df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = index.as_query_engine(\n",
    "#     similarity_top_k = 50,\n",
    "#     reranker=\"chain\",\n",
    "    # rerank_chain=[\n",
    "    #     {\n",
    "    #         \"type\": \"slingshot\"\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"type\": \"mmr\",\n",
    "    #         \"diversity_bias\": 0.3\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"type\": \"mmr\",\n",
    "    #         \"diversity_bias\": 0.7,\n",
    "    #         \"limit\": 5\n",
    "    #     }\n",
    "    # ]\n",
    "# )\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k = 50,\n",
    "    reranker=\"mmr\",\n",
    "    # udf_expression=\"max(0, 10 * get('$.score') - hours(seconds((to_unix_timestamp(now()) - to_unix_timestamp(datetime_parse(get('$.document_metadata.published'), 'yyyy-MM-dd'))))) / 24 / 365)\"\n",
    ")\n",
    "\n",
    "# udf_expression=\"max(0, 10 * get('$.score') - hours(seconds((to_unix_timestamp(now()) - to_unix_timestamp(datetime_parse(get('$.document_metadata.published'), 'yyyy-MM-dd'))))) / 24 / 365)\"\n",
    "\n",
    "        # {\n",
    "        #     \"type\": \"udf\",\n",
    "        #     \"user_function\": \"get('$.score') + 10\"\n",
    "        # }\n",
    "\n",
    "# response = query_engine.query(\"What innovations have been made to sentence embedding models?\")\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "01b96eb2-9e88-4c6a-90c4-676dddcc5ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '2017-03-07'),\n",
       " (1, '2023-05-04'),\n",
       " (2, '2024-04-05'),\n",
       " (3, '2024-02-22'),\n",
       " (4, '2023-11-09'),\n",
       " (5, '2023-07-06'),\n",
       " (6, '2022-04-02'),\n",
       " (7, '2020-05-22'),\n",
       " (8, '2023-05-04'),\n",
       " (9, '2022-04-28'),\n",
       " (10, '2022-04-22'),\n",
       " (11, '2024-04-05'),\n",
       " (12, '2016-05-16'),\n",
       " (13, '2022-04-28'),\n",
       " (14, '2021-10-02'),\n",
       " (15, '2018-03-29'),\n",
       " (16, '2020-07-03'),\n",
       " (17, '2022-04-28'),\n",
       " (18, '2019-06-04'),\n",
       " (19, '2018-08-16'),\n",
       " (20, '2023-11-09'),\n",
       " (21, '2020-03-09'),\n",
       " (22, '2022-04-28'),\n",
       " (23, '2018-10-20'),\n",
       " (24, '2022-10-20'),\n",
       " (25, '2023-11-16'),\n",
       " (26, '2024-04-05'),\n",
       " (27, '2024-04-05'),\n",
       " (28, '2019-08-14'),\n",
       " (29, '2018-06-16'),\n",
       " (30, '2019-08-14'),\n",
       " (31, '2020-04-21'),\n",
       " (32, '2024-05-30'),\n",
       " (33, '2024-05-30'),\n",
       " (34, '2022-04-02'),\n",
       " (35, '2020-06-05'),\n",
       " (36, '2022-05-10'),\n",
       " (37, '2020-11-02'),\n",
       " (38, '2023-07-31'),\n",
       " (39, '2018-06-03'),\n",
       " (40, '2024-02-22'),\n",
       " (41, '2023-05-24'),\n",
       " (42, '2022-05-31'),\n",
       " (43, '2023-05-04'),\n",
       " (44, '2018-02-12'),\n",
       " (45, '2016-07-10'),\n",
       " (46, '2015-11-25'),\n",
       " (47, '2023-11-09'),\n",
       " (48, '2018-08-27'),\n",
       " (49, '2021-04-14')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['published']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb7457-50e2-4b24-8416-daf65e9a4d5c",
   "metadata": {},
   "source": [
    "Notice how many of the papers used to generate the final summary were published in the recent past and they still give us information to generate a relevant response that answers our question.\n",
    "\n",
    "Also notice how we use a max() function with 0 in our user-defined expression. This is to ensure that all of our reranking scores are non-negative. Additionally, since we multiplied the original score by 10 and its value ranges from 0 to 1, we throw away any search results that are older than 10 years old for generating our final response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863b914",
   "metadata": {},
   "source": [
    "So far we've used Vectara's internal summarization capability, which is the best way for most users.\n",
    "\n",
    "You can still use Llama-Index's standard VectorStore `as_query_engine()` method, in which case Vectara's summarization won't be used, and you would be using an external LLM (e.g. OpenAI's GPT-4) and a custom prompt from LlamaIndex to generate the summary. For this option just set `summary_enabled=False`\n",
    "\n",
    "For this functionality, you will need to specify your own OpenAI API key in the environment:\n",
    "\n",
    "> `os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b0a49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a technique used in natural language processing to represent sentences as vectors in a high-dimensional space. These vectors encode the meaning of the sentences, allowing for various applications such as machine translation, information retrieval, and text classification. The goal is for sentences with similar meanings to be close to each other in this vector space, facilitating tasks that require understanding of sentence semantics.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    summary_enabled=False,\n",
    "    llm=llm\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebad3cf-b09e-4af5-be3d-ba2a3e3e8d73",
   "metadata": {},
   "source": [
    "## Using Vectara Chat\n",
    "\n",
    "Vectara now fully supports Chat in its platform, where the chat history is maintained by Vectara and so you don't have to worry about keeping history and integrating it with your RAG pipeline. \n",
    "\n",
    "To use it, simply call `as_chat_engine()`.\n",
    "\n",
    "(Chat mode always uses Vectara's summarization so you don't have to explicitly specify `summary_enabled=True` like before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42af08f5-dc13-4991-96f4-fdd294b0ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e3368be-46a3-4b2f-9589-de3383c65201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a sentence embedding model?\n",
      "\n",
      "Response: A sentence embedding model is a method that represents input sentences as fixed-dimensional vectors, regardless of sentence length. These models have shown significant enhancements in various natural language processing tasks like information retrieval, question answering, and machine translation. They are particularly beneficial for tasks where sentence-level representations are crucial, enabling improved performance compared to traditional word embeddings. Sentence embedding models are trained to capture semantic meanings and relationships within sentences, providing a more efficient way to process and analyze textual data [4].\n",
      "\n",
      "Question: What are some known models?\n",
      "\n",
      "Response: Some known sentence embedding models include non-parameterized models like averaging word embeddings using methods such as average-pooling, min-pooling, and max-pooling, as well as parameterized models like BERT, RoBERTa, GenSen, and DSE. These models aim to represent entire sentences with their semantic information efficiently. Additionally, models like InferSent, Universal Sentence Encoder (USE), and Sentence-BERT offer different approaches to generating sentence embeddings, either based on word embeddings or contextual embedding methods.\n",
      "\n",
      "Question: How are they different than token embedding models\n",
      "\n",
      "Response: Sentence embedding models differ from token embedding models in the way they process and represent text. While token embedding models focus on individual words or tokens, sentence embedding models consider the entire sentence as a whole. Sentence embedding models typically pool token embeddings, often by averaging them, to generate a representative embedding for the entire sentence. On the other hand, token embedding models may disperse token representations across the embedding space. Additionally, sentence embedding models can be fine-tuned for specific tasks and show better performance when averaging all token representations compared to using a single token embedding like [CLS]. Furthermore, normalizing sentence embeddings with whitening algorithms can consistently enhance performance. These differences highlight how sentence embedding models capture the overall context and meaning of a sentence, while token embedding models focus on individual words or tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'What is a sentence embedding model?',\n",
    "    'What are some known models?',\n",
    "    'How are they different than token embedding models'\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Question: {q}\\n\")\n",
    "    response = ce.chat(q).response\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111b1fa-5efc-4ee6-99f0-21c73dbd5433",
   "metadata": {},
   "source": [
    "Of course streaming works as well with Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8b24622-bc73-4bee-929e-377f06335a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8f16016-be7c-411f-aafd-b03d8816d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The individuals behind SBERT are Reimers and Gurevych, as mentioned in the search results [1], [7]."
     ]
    }
   ],
   "source": [
    "response = ce.stream_chat(\"Who is behind SBERT?\")\n",
    "for chunk in response.chat_stream:\n",
    "    print(chunk.delta or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ad93e-0692-4ad5-99a1-329788252b03",
   "metadata": {},
   "source": [
    "# Advanced RAG with Vectara and LLamaIndex\n",
    "\n",
    "## Agentic RAG\n",
    "\n",
    "Vectara also has its own package, [vectara-agentic](https://github.com/vectara/py-vectara-agentic), built on top of many features from LlamaIndex to easily implement agentic RAG applications. It allows you to create your own AI assistant with RAG query tools and other custom tools, such as making API calls to retrieve information from financial websites. You can find the full documentation for vectara-agentic [here](https://vectara.github.io/vectara-agentic-docs/).\n",
    "\n",
    "Let's create a ReAct Agent with a single RAG tool using vectara-agentic (to create a ReAct agent, specify `VECTARA_AGENTIC_AGENT_TYPE` as `\"REACT\"` in your environment).\n",
    "\n",
    "Vectara does not yet have an LLM capable of acting as an agent for planning and tool use, so we will need to use another LLM as the driver of the agent resoning.\n",
    "\n",
    "In this demo, we are using OpenAI's GPT4o. Please make sure you have `OPENAI_API_KEY` defined in your environment or specify another LLM with the corresponding key (for the full list of supported LLMs, check out our [documentation](https://vectara.github.io/vectara-agentic-docs/introduction.html#try-it-yourself) for setting up your environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "648b415f-303a-4d0a-aef1-9d58aa6380ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U vectara-agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c77fd093-522d-435e-954e-f571a16a64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No observer set.\n",
      "Added user message to memory: Tell me about the latest innovations in sentence embedding models.\n",
      "=== Calling Function ===\n",
      "Calling function: ask_embeddings with args: {\"query\":\"latest innovations in sentence embedding models\"}\n",
      "Got output: \n",
      "                    Response: '''The latest innovations in sentence embedding models involve the use of large language models such as LLaMA and Mistral, which have achieved notable breakthroughs in fine-tuning scenarios [1]. However, research on computationally efficient direct inference methods for sentence representation is still in its nascent stage [1]. Recent studies have focused on refining training objectives with contrastive loss and using various training datasets, often involving translation pairs [5]. The integration of large language models and prompting methods has further advanced the capabilities of sentence embedding models [5]. Additionally, the use of uniformity loss and alignment loss in sentence embedding models has been shown to improve performance [4].'''\n",
      "                    References:\n",
      "                    [1]: page='1'; title='Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models'; section='2'; PTEX.Fullbanner='This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'; CreationDate='D:20240516003614Z'; Keywords=''; Producer='pdfTeX-1.40.25'; Author=''; Title=''; Creator='LaTeX with hyperref'; ModDate='D:20240516003614Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2404.03921v2'; author='Bowen Zhang'; published='2024-04-05'; framework='llama_index'.\n",
      "[4]: page='4'; section='95'; PTEX.Fullbanner='This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2'; CreationDate='D:20220426000538Z'; Keywords=''; Producer='pdfTeX-1.40.21'; Author=''; Title=''; Creator='LaTeX with hyperref'; ModDate='D:20220426000538Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2204.10931v1'; title='arXiv:2204.10931v1  [cs.CL]  22 Apr 2022'; author='Miaoran Zhang'; published='2022-04-22'; framework='llama_index'.\n",
      "[5]: page='2'; title='Introduction'; breadcrumb='[\"Related Work\"]'; section='3'; PTEX.Fullbanner='This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'; CreationDate='D:20240531010252Z'; Keywords=''; Producer='pdfTeX-1.40.25'; Author=''; Title=''; Creator='LaTeX with hyperref'; ModDate='D:20240531010252Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2205.15744v2'; author='Zhuoyuan Mao'; published='2022-05-31'; framework='llama_index'.\n",
      "\n",
      "                \n",
      "========================\n",
      "\n",
      "Time taken: 10.190776109695435\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The latest innovations in sentence embedding models include the use of large language models like LLaMA and Mistral, which have made significant advancements in fine-tuning scenarios. However, research on computationally efficient direct inference methods for sentence representation is still emerging. Recent studies have focused on refining training objectives with contrastive loss and using various training datasets, often involving translation pairs. The integration of large language models and prompting methods has further enhanced the capabilities of sentence embedding models. Additionally, the use of uniformity loss and alignment loss in sentence embedding models has been shown to improve performance.\n",
       "\n",
       "For more detailed information, you can refer to the following sources:\n",
       "- [Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models](http://arxiv.org/pdf/2404.03921v2) by Bowen Zhang, published in 2024.\n",
       "- [arXiv:2204.10931v1](http://arxiv.org/pdf/2204.10931v1) by Miaoran Zhang, published in 2022.\n",
       "- [Introduction](http://arxiv.org/pdf/2205.15744v2) by Zhuoyuan Mao, published in 2022."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vectara_agentic.agent import Agent\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "agent = Agent.from_corpus(\n",
    "    data_description=\"sentence embeddings\",\n",
    "    assistant_specialty=\"sentence embeddings research\",\n",
    "    tool_name=\"ask_embeddings\",\n",
    "    vectara_summary_num_results=5,\n",
    "    vectara_summarizer=\"mockingbird-1.0-2024-07-16\",\n",
    "    vectara_reranker=\"mmr\",\n",
    "    vectara_rerank_k=50,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "response = agent.chat(\n",
    "    \"Tell me about the latest innovations in sentence embedding models.\"\n",
    ")\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43963c4",
   "metadata": {},
   "source": [
    "## Using Auto Retriever with Vectara\n",
    "\n",
    "LlamaIndex's auto-retriever functionality is really cool. \n",
    "It is most useful when you have metadata fields (like in our case of papers from Arxiv), and would like a query that references a metadata field to be automatically interpreted in the right way.\n",
    "\n",
    "For example, if I ask \"what is a paper about climate change risks published after 2020\", the auto-retriever would (behind the scences) interpret ths into a query \"what is a paper about climate change risks\" along with a filter condition of \"published > 2020\"\n",
    "\n",
    "Let's see how this works with the Vectara Index.\n",
    "First, we have to define a `VectorStoreInfo` structure that defines the meta data fields the auto-retriever knows about to do its job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24cc7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"information about a paper\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"published\",\n",
    "            description=\"The date the paper was published\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"author\",\n",
    "            description=\"The author of the paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"title\",\n",
    "            description=\"The title of the papers\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"url\",\n",
    "            description=\"The URL for this paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23712d46",
   "metadata": {},
   "source": [
    "Auto-retrieval is implemented before calling Vectara as a query transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f0333",
   "metadata": {},
   "source": [
    "Now we can define the `VectaraAutoRetriever`, which can perform auto-retrieval using Vectara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92de30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding?\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2018-08-16',\n",
       "  'This\\nproblem can be alleviated by obtaining more of para-\\nphrase sentence pairs. Conclusion Sentence embedding is one of the most important text\\nprocessing techniques in NLP. To date,  various sen-\\ntence embedding models have been proposed and have\\nyielded good performances in document classification\\nand sentiment analysis tasks. However, the fundamen-\\ntal ability of sentence embedding methods, i.e., how\\neffectively the meanings of the original sentences are\\npreserved  in  the  embedded  vectors,  cannot  be  fully\\nevaluated through such indirect methods.'),\n",
       " ('2018-08-16',\n",
       "  'Paraphrase Thought:  Sentence Embedding Module Imitating\\n                        Human Language Recognition Myeongjun Jang 1 Abstract\\nSentence embedding is an important research\\ntopic in natural language processing. It is es-\\nsential to generate a good embedding vector\\nthat  fully  reflects  the  semantic  meaning  of\\na sentence in order to achieve an enhanced\\nperformance  for  various  natural  language\\nprocessing  tasks,   such  as  machine  trans-\\nlation  and  document  classification. Thus\\nfar, various sentence embedding models have\\nbeen proposed, and their feasibility has been\\ndemonstrated through good performances on\\ntasks following embedding, such as sentiment\\nanalysis  and  sentence  classification.'),\n",
       " ('2018-08-16',\n",
       "  'Pilsung Kang 1 Introduction Sentence embedding, which transforms sentences into\\nlow-dimensional  vector  values  reflecting  their  mean-\\nings,  is a highly important task in natural language\\nprocessing  (NLP). By  mapping  unstructured  text\\ndata into a certain form of structured representation,\\nthe embedding vector can enhance the performances\\nof  various  NLP  tasks,  such  as  machine  translation\\n(Artetxe et al., 2017; Lee et al., 2016; Zhao & Zhang,\\n2016), document classification (Conneau et al., 2017b;\\nZhou et al., 2016), and sentence matching (Wan et al.,\\n2016). As sentence embedding plays an import role\\nin NLP, various methods (Kiros et al., 2015; Pagliar-\\ndini et al., 2017; Hill et al., 2016; Arora et al., 2017;\\nConneau  et  al.,  2017a;  Chen,  2017)  have  been  pro-\\nposed since the advent of the Doc2vec method (Le &\\nMikolov, 2014).'),\n",
       " ('2016-06-15',\n",
       "  '2    Siamese CBOW\\n\\nWe present the Siamese Continuous Bag of Words\\n(CBOW)  model,  a  neural  network  for  efï¬cient\\nestimation of high-quality sentence embeddings. Quality should manifest itself in embeddings of\\nsemantically close sentences being similar to one\\nanother, and embeddings of semantically different\\nsentences being dissimilar. An efï¬cient and sur-\\nprisingly successful way of computing a sentence\\nembedding is to average the embeddings of its\\nconstituent words. Recent work uses pre-trained\\nword embeddings (such as word2vec and GloVe)\\nfor this task, which are not optimized for sentence\\nrepresentations. Following these approaches, we\\ncompute sentence embeddings by averaging word\\nembeddings, but we optimize word embeddings\\ndirectly for the purpose of being averaged.'),\n",
       " ('2017-03-09',\n",
       "  'Instead of using a vector, we use a 2-D matrix\\nto represent the embedding, with each row of the matrix attending on a different\\npart of the sentence. We also propose a self-attention mechanism and a special\\nregularization term for the model. As a side effect, the embedding comes with an\\neasy way of visualizing what speciï¬c parts of the sentence are encoded into the\\nembedding. We evaluate our model on 3 different tasks: author proï¬ling, senti-\\nment classiï¬cation and textual entailment. Results show that our model yields a\\nsigniï¬cant performance gain compared to other sentence embedding methods in\\nall of the 3 tasks.'),\n",
       " ('2018-06-03',\n",
       "  'We use a one-hot\\nvector representation for every word and obtain a word embedding ci for each word using a Temporal CNN (Zhang et al., 2015; Palangi et al., 2016) module that we parameterize through a function G(Xi; We)\\nwhere We are the weights of the temporal CNN. Now this word embedding is fed to an LSTM-based\\nencoder which provides encoding features of the sentence. We use LSTM (Hochreiter and Schmidhuber,\\n1997) due to its capability of capturing long term memory (Palangi et al., 2016). As the words are\\npropagated through the network, the network collects more and more semantic information about the\\nsentence.'),\n",
       " ('2017-03-07',\n",
       "  'Con-\\nceptually, the model can be interpreted as a natu-\\nral extension of the word-contexts from C-BOW\\n(Mikolov et al., 2013b,a) to a larger sentence con-\\ntext, with the sentence words being speciï¬cally\\noptimized towards additive combination over the\\nsentence, by means of the unsupervised objective\\nfunction. Formally, we learn a source (or context) embed-\\nding vw and target embedding uw for each word w\\nin the vocabulary, with embedding dimension h\\nand k  = jVj as in (1). The sentence embedding\\nis deï¬ned as the average of the source word em-\\nbeddings of its constituent words, as in (2). We\\naugment this model furthermore by also learning\\nsource embeddings for not only unigrams but also\\nn-grams present in each sentence, and averaging\\nthe n-gram embeddings along with the words, i.e.,\\nthe sentence embedding vS for S is modeled as X'),\n",
       " ('2018-02-12',\n",
       "  'Background Background Sentence Embeddings Introduction\\nA hallmark of human intelligence is compositionality:  the\\nability,  in  the words  of  von  Humboldt,  to â€œmake  inï¬nite\\nuse of ï¬nite means.â€ The failure of neural network models\\nto achieve compositionality has been a recurring (and con-\\ntroversial) theme in cognitive science (Fodor & Pylyshyn,\\n1988; Gershman & Tenenbaum, 2015; Lake & Baroni, 2017).'),\n",
       " ('2017-03-09',\n",
       "  '(See Figure 3a and 3b). The second way of visualization can be achieved by summing up over all the annotation vectors,\\nand then normalizing the resulting weight vector to sum up to 1. Since it sums up all aspects of\\nsemantics of a sentence, it yields a general view of what the embedding mostly focuses on. We can\\nï¬gure out which words the embedding takes into account a lot, and which ones are skipped by the\\nembedding. See Figure 3c and 3d.'),\n",
       " ('2018-09-30',\n",
       "  'InferSent               4096          24           81.1      86.3       92.4        90.2        84.6       88.2      76.2/83.1        88.4           86.3\\nSent2Vec                700          6.5          75.8      80.3       91.1        85.9           -          86.4      72.5/80.8           -                 -\\nSkipThought-LN        4800         336          79.4      83.1       93.7        89.3        82.9       88.4              -               85.8           79.5\\n FastSent                 300            2            70.8      78.4       88.7        80.6           -          76.8      72.2/80.3           -                 -\\n       a la carte              4800        N/A         81.8      84.3       93.8        87.6        86.7       89.0              -                  -                 -\\n SDAE                 2400         192          74.6      78.0       90.8        86.9           -          78.4      73.7/80.7           -                 -\\n  QT                    4800          28           82.4      86.0       94.8        90.2        87.6       92.4      76.9/84.0        87.4              -\\n STN                   4096         168          82.5      87.7       94.0        90.9        83.2       93.0      78.6/84.4        88.8           87.8\\n  USE                    512         N/A        81.36    86.08     93.66      87.14      86.24     96.60            -                  -                 - Table 3: Results on supervised tasks. Sentence embeddings are ï¬xed for downstream supervised tasks. Best results\\nfor each task are underlined, best results from models in the same category are in bold. SIF results are extracted\\nfrom Arora et al. (2017) and RuÂ¨ckleÂ´ et al. (2018), and training time is collected from Logeswaran and Lee (2018).')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "\n",
    "retriever = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "res = retriever.retrieve(\"What is sentence embedding, based on papers before 2019?\")\n",
    "[(r.metadata['published'], r.text) for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7450bc",
   "metadata": {},
   "source": [
    "As you can see, the Auto Retriever was able to translate the natural language text into a shorter query and a proper condition (in this case `doc.published < 2019`).\n",
    "\n",
    "We can also of course ask a question directly: we use the `VectaraQueryEngine` which can work with the `VectaraAutoRetriever` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "327ffbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding?\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n",
      "Sentence embedding is a crucial technique in Natural Language Processing (NLP) that involves transforming sentences into low-dimensional vector representations to capture their semantic meanings. Various models have been developed to create embedding vectors that enhance performance in tasks like document classification, sentiment analysis, and machine translation. These models aim to preserve the original sentence meanings effectively within the embedded vectors, ultimately improving the efficiency of NLP tasks. Different methods and approaches have been proposed to optimize sentence embeddings, ensuring that semantically similar sentences have similar embeddings while semantically different ones are dissimilar. Overall, sentence embedding plays a vital role in NLP by providing structured representations of unstructured text data, leading to improved performance across a range of language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara.query import VectaraQueryEngine\n",
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "\n",
    "ar = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=llm,\n",
    "    summary_enabled=True,\n",
    "    summary_num_results=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query_engine = VectaraQueryEngine(retriever=ar)\n",
    "response = query_engine.query(\"What is sentence embedding, based on papers before 2019?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5faef-9fe9-4acc-a55d-ad7c379697fd",
   "metadata": {},
   "source": [
    "## Advanced querying with QueryFusionRetriever\n",
    "\n",
    "The QueryFusion [Retriever](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion.html#reciprocal-rerank-fusion-retriever) is an advanced query mechanism whereby the original query is pre-processed to generate N variations. Each of these rephrased queries is then run against the Vectara engine and rank-fusion is used to combine the best results. \n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c662bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT is not specifically described as a dual encoder in the provided context. It is mentioned as one of the base encoder models used in a study, but the specific architecture type of SBERT itself is not detailed in the excerpts. SBERT typically employs a sentence embedding approach, but further specifics on whether it uses a dual encoder or another type of deep learning architecture are not provided in the context.\n"
     ]
    }
   ],
   "source": [
    "query = \"is SBERT a dual encoder? what type of DL architecture does it use?\"\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    summary_enabled=False,\n",
    "    llm=llm,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "896e8b7e-c028-4ec6-bb7c-0c05dfb86321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What is SBERT and how does it work as a dual encoder?\n",
      "2. Comparison of SBERT with other dual encoder models in natural language processing.\n",
      "3. Deep learning architecture used in SBERT for sentence embeddings.\n",
      "4. Advantages and limitations of using a dual encoder like SBERT in machine learning tasks.\n",
      "SBERT is not a dual encoder. It is a text encoder model that does not use a dual-encoder architecture. Instead, SBERT uses a single embedding space for encoding text.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import nest_asyncio\n",
    "\n",
    "rf_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(similarity_top_k=2)],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=5,  # this includes the origianl query; set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()     # apply nested async to run in a notebook\n",
    "query_engine = RetrieverQueryEngine.from_args(rf_retriever)\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6b3fd",
   "metadata": {},
   "source": [
    "We can see how the QueryFusionRetriever created additional query variations (they are displayed since we used `verbose=True`) and then the overall response includes the results fused together. This is very helpful in this case because the QueryFusionRetriever creates sub-questions that inquire about the specific architecture of SBERT which is relevant context to answering this question properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c98981",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we've seen various examples for using Vectara with LlamaIndex, which provides the following benefits:\n",
    "* Vectara provides a complete RAG pipeline, so you don't have to deal with a lot of the details around data ingestion: pre-processing, chunking, embedding, etc. Instead all these steps are handled automatically and efficiently in Vectara. \n",
    "* Being a platform, Vectara uses its own internal Embedding model (Boomerang), its own vector storage, and its own LLM (Mockingbird) for summarization, so you don't have to maintain separate API keys and relationships with additional vendors or install other products.\n",
    "* Vectara is built for large scale GenAI applications, and with the tools provided by LlamaIndex like Auto Retrieval and Query Fusion, you can easily build and test advanced RAG applications at enteprise scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
