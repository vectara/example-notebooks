{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara and LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted GenAI and semantic search platform that provides an easy-to-use API for document indexing and querying. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. A way to extract text from document files and chunk them into sentences.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedding model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embeddings and retrieves the most relevant text segments (including support for [hybrid search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking)).\n",
    "\n",
    "4. An option to create a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview) with a wide selection of LLM summarizers (including Vectara's [Mockingbird](https://vectara.com/blog/mockingbird-is-a-rag-specific-llm-that-beats-gpt-4-gemini-1-5-pro-in-rag-output-quality/), trained specifically for RAG-based tasks), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits of using Vectara for a RAG application are:\n",
    "* **Ease of use**: Vectara provides an end-to-end, fully functional, highly scalable, and robust RAG pipeline, so as a user you don't have to code up these pieces and maintain them over time.\n",
    "* **Scalability and Security**: Building GenAI applications may seem easy at first, but the DIY approach can become overwhelming beyond simple examples. Vectara provides instant scalablility to millions of documents, while maintaing data security and privacy, as well as latency SLAs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33079b",
   "metadata": {},
   "source": [
    "## About LlamaIndex\n",
    "\n",
    "LlamaIndex is a \"data framework\" to help you build LLM apps:\n",
    "\n",
    "1. It includes **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "2. It provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\n",
    "3. It provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "\n",
    "LlamaIndex's high-level API allows beginner users to use LlamaIndex to ingest and query their data in just a few lines of code, whereas its lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "Vectara is implemented in LlamaIndex as a [Managed Service](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices.html#vectara), abstracting all of Vectara's powerful API so they are easily integrated into LlamaIndex.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\n",
    "\n",
    "To get started with Vectara, [sign up](https://vectara.com/integrations/llamaindex) (if you haven't already) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \n",
    "\n",
    "Once you have these, you can provide them as environment variables, which will be used by the LlamaIndex code later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U llama-index llama-index-indices-managed-vectara arxiv\n",
    "\n",
    "import os\n",
    "# os.environ['VECTARA_API_KEY'] = \"<YOUR_VECTARA_API_KEY>\"\n",
    "# os.environ['VECTARA_CORPUS_ID'] = \"<YOUR_VECTARA_CORPUS_ID>\"\n",
    "# os.environ['VECTARA_CUSTOMER_ID'] = \"<YOUR_VECTARA_CUSTOMER_ID>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Loading Data Into Vectara\n",
    "\n",
    "As mentioned above, Vectara is a RAG managed service, and in many cases data may be uploaded to the index ahead of time (e.g. by using [Airbyte](https://docs.airbyte.com/integrations/destinations/vectara), directly via Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) or using tools like [vectara-ingest](https://github.com/vectara/vectara-ingest)), but another easy way is via the VectaraIndex constructor: `from_documents()`.\n",
    "\n",
    "For this notebook, we will assume the Vectara corpus is empty and will load PDF documents from Arxiv, using Python's [arxiv](https://github.com/lukasschwab/arxiv.py) library. We will pull in data from the top papers related to \"climate change\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40947545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"(ti:embedding model) OR (ti:sentence embedding)\",\n",
    "  max_results = 100,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae7bd09-6569-48cc-8c54-8bf491c0e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2402.14776v2',\n",
       " 'http://arxiv.org/abs/2007.01852v2',\n",
       " 'http://arxiv.org/abs/1910.13291v1',\n",
       " 'http://arxiv.org/abs/2104.06719v1',\n",
       " 'http://arxiv.org/abs/1511.08198v3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.entry_id for p in papers][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "Next, download the Arxiv paper, and upload them into Vectara using the `add_file()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /opt/anaconda3/envs/llama_index/lib/python3.10/site-\n",
      "[nltk_data]     packages/llama_index/core/_static/nltk_cache...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File temp/1909.03104v2.Efficient_Sentence_Embedding_using_Discrete_Cosine_Transform.pdf failed to load with error HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from llama_index.indices.managed.vectara import VectaraIndex\n",
    "\n",
    "data_folder = 'temp'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Create Vectara Index\n",
    "index = VectaraIndex()\n",
    "\n",
    "# Upload content for all papers\n",
    "for paper in papers:\n",
    "    try:\n",
    "        paper_fname = paper.download_pdf(data_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"File {paper_fname} failed to load with error {e}\")\n",
    "        continue\n",
    "    metadata = {\n",
    "        'url': paper.pdf_url,\n",
    "        'title': paper.title,\n",
    "        'author': str(paper.authors[0]),\n",
    "        'published': str(paper.published.date())\n",
    "    }\n",
    "    index.insert_file(file_path=paper_fname, metadata=metadata)\n",
    "\n",
    "shutil.rmtree(data_folder)\n",
    "del papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea571",
   "metadata": {},
   "source": [
    "Two important things to note here:\n",
    "1. Vectara processes each file uploaded on the backend, and performs appropriate chunking. So you don't need to apply any local processing, or choose a chunking strategy. \n",
    "2. We have used the fields `url`, `title`, `author`, and `published` as metadata fields (for simplicity, author is the first author if there are multiple). You will need to make sure those fields are defined in your Vectara corpus as [filterable metadata fields](https://docs.vectara.com/docs/learn/metadata-search-filtering/filter-overview) to ensure we can filter by them in query time.\n",
    "\n",
    "So that's it for upload. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Querying with the VectaraIndex\n",
    "We can now ask questions using the `VectaraIndex` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb174ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is sentence embedding?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7301225-07c2-4aec-b67b-ae9a0a7f3e4b",
   "metadata": {},
   "source": [
    "MAY WANT TO CHANGE THIS TO MOCKINGBIRD FOR DEMONSTRATIVE PURPOSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, sentence embedding is a technique that represents words or sentences in a text in a vector space, where words or sentences that are closer in the vector space are more similar [2]. It is a vital technique in practical applications, enabling the extraction of parallel data from extensive web corpora across two languages, enhancing the training of high-quality machine translation and multilingual language models [1]. Sentence embedding is also instrumental for zero-shot cross-lingual retrieval, an essential method for enabling cross-lingual searches on e-commerce platforms like Amazon [1].\n",
      "\n",
      "In the context of machine learning, sentence embedding is a form of word or sentence representation by learned representations that prepare texts in an understandable format for a machine [2]. It is used in various applications, including entailment classification, where a softmax inference classifier is used to classify sentences based on their embeddings [4][5].\n",
      "\n",
      "Sources: [1], [2], [4], [5]\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True, summary_num_results=5,\n",
    "    summary_response_lang=\"eng\",\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\"\n",
    ")\n",
    "res = query_engine.query(query)\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52878fd2",
   "metadata": {},
   "source": [
    "Note that the response here is fully generated by Vectara. There is no additional LLM involved (or API key you need to setup). The response also includes citations (marked in square brackets), which provide links to references used to generate this response by Vectara. \n",
    "<br>\n",
    "The `res` object includes the actual response to the user query, but also has the citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b110a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (1, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (2, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (3, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (5, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (6, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (7, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (8, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (9, 'http://arxiv.org/pdf/2404.03921v2')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(res.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cce7c7-c664-4001-8c84-f80c5d499796",
   "metadata": {},
   "source": [
    "## Using Streaming\n",
    "\n",
    "You can also stream the Vectara response simply by specifying `streaming=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abd30027-d975-4440-a1c2-66d1bd30bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, sentence embedding is a technique that represents words or sentences in a text in a vector space, where words or sentences that are closer in the vector space are more similar [2]. It is a vital technique in practical applications, enabling the extraction of parallel data from extensive web corpora across two languages, enhancing the training of high-quality machine translation and multilingual language models [1]. Sentence embedding is also instrumental for zero-shot cross-lingual retrieval, an essential method for enabling cross-lingual searches on e-commerce platforms like Amazon [1].\n",
      "\n",
      "In the context of machine learning, sentence embedding is a form of word or sentence representation by learned representations that prepare texts in an understandable format for a machine [2]. It is used in various applications, including entailment classification, softmax inference, and language translation [4, 5, 6, 7].\n",
      "\n",
      "Sources: [1], [2], [4], [5], [6], [7]"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True,\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\",\n",
    "    streaming=True)\n",
    "\n",
    "res = query_engine.query(query)\n",
    "\n",
    "# print streamed output chunk by chunk\n",
    "for chunk in res.response_gen:\n",
    "    print(chunk.delta or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cd598-ce73-4c9e-9aa6-5d8604fd6c14",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "Vectara supports three types of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking):\n",
    "1. [Maximal Marginal Relevance](https://docs.vectara.com/docs/learn/mmr-reranker), or MMR, provides a reranking that can promote diversity in results at the cost of relevance.\n",
    "2. [Slingshot](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker) is a mulitilingual reranker that increases the accuracy of retrieved results across 100+ languages and is available to Vectara Scale customers.\n",
    "3. [User Defined Functions](https://docs.vectara.com/docs/learn/user-defined-function-reranker) allow you to create your own functions for reranking search results, unlocking better retrieval in a wide variety of use cases, such as sorting by recency or price of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49914a",
   "metadata": {},
   "source": [
    " Let's see an example of how to use MMR: We will run the same query but this time we will use MMR where `mmr_diversity_bias=0.3` provides a tradeoff between relevance and diversity (0.0 is full relevance, 1.0 is only diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72832e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a technique used to represent sentences in a text in a way that encodes the meaning of the sentence in a multi-dimensional space. It allows for the extraction of parallel data across languages, aids in machine translation, multilingual language models, and cross-lingual retrieval. Various models exist, such as LASER, SBERT-distill, and LaBSE, each with different training efficiencies and architectures. Sentence embedding is crucial in natural language processing tasks like document classification and sentiment analysis, ensuring the preservation of original sentence meanings in embedded vectors.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    reranker=\"mmr\",\n",
    "    rerank_k=50,\n",
    "    mmr_diversity_bias=0.3,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17020d34-5176-4910-bb88-5c5685513804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (1, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (2, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (3, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/pdf/1808.05505v3')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e76fd1",
   "metadata": {},
   "source": [
    "As you can see, the results are now reranked in a way that provides more diversity instead of maximizing pure relevance. This in turn results in a different set of chunks used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562956f5-b9b7-4288-8100-f443a5dc3e4d",
   "metadata": {},
   "source": [
    "Now let's see an example with a user defined function. We may be interested in getting results that are the most semantically similar to our question, but we also want the most up-to-date information. Thus, we can bias our search results so that the papers that are not only semantically similar but also published more recently are used to answer our query. We can do this by using the available time functions (to see other built-in functions, see the UDF Reranker [documentation](https://docs.vectara.com/docs/learn/user-defined-function-reranker))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62145cd-a366-4f04-9e5e-b9c2affcb970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innovations in sentence embedding models include leveraging pooling strategies, similarity fine-tuning in a contrastive framework, utilizing prompts, implementing a two-step training process, and developing models for different languages like French and Japanese. Additionally, advancements involve open-sourcing code and pre-trained models, exploring unsupervised, task-independent models, and enhancing efficiency by using language models like BERT for sentence embeddings. These innovations aim to improve performance and explore new methods for sentence representation [1][2][3][4][5].\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    reranker=\"udf\",\n",
    "    udf_expression=\"max(0, 10 * get('$.score') - hours(seconds((to_unix_timestamp(now()) - to_unix_timestamp(datetime_parse(get('$.published'), 'yyyy-MM-dd'))))) / 24 / 365)\",\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What innovations have been made to sentence embedding models?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b96eb2-9e88-4c6a-90c4-676dddcc5ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '2024-05-30'),\n",
       " (1, '2023-01-19'),\n",
       " (2, '2017-03-07'),\n",
       " (3, '2020-05-22'),\n",
       " (4, '2024-04-05')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['published']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb7457-50e2-4b24-8416-daf65e9a4d5c",
   "metadata": {},
   "source": [
    "Notice how many of the papers used to generate the final summary were published in the recent past and they still give us information to generate a relevant response that answers our question.\n",
    "\n",
    "Also notice how we use a max() function with 0 in our user-defined expression. This is to ensure that all of our reranking scores are non-negative. Additionally, since we multiplied the original score by 10 and its value ranges from 0 to 1, we throw away any search results that are older than 10 years old for generating our final response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863b914",
   "metadata": {},
   "source": [
    "So far we've used Vectara's internal summarization capability, which is the best way for most users.\n",
    "\n",
    "You can still use Llama-Index's standard VectorStore `as_query_engine()` method, in which case Vectara's summarization won't be used, and you would be using an external LLM (e.g. OpenAI's GPT-4) and a custom prompt from LlamaIndex to generate the summary. For this option just set `summary_enabled=False`\n",
    "\n",
    "For this functionality, you will need to specify your own OpenAI API key in the environment:\n",
    "\n",
    "> `os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b0a49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a technique used in natural language processing to represent sentences as vectors in a high-dimensional space. These vectors encode the meaning of the sentences, allowing for various applications such as machine translation, information retrieval, and text classification. The goal is for sentences with similar meanings to be close to each other in this vector space, facilitating tasks that require understanding of sentence semantics.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    summary_enabled=False,\n",
    "    llm=llm\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebad3cf-b09e-4af5-be3d-ba2a3e3e8d73",
   "metadata": {},
   "source": [
    "## Using Vectara Chat\n",
    "\n",
    "Vectara now fully supports Chat in its platform, where the chat history is maintained by Vectara and so you don't have to worry about keeping history and integrating it with your RAG pipeline. \n",
    "\n",
    "To use it, simply call `as_chat_engine()`.\n",
    "\n",
    "(Chat mode always uses Vectara's summarization so you don't have to explicitly specify `summary_enabled=True` like before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42af08f5-dc13-4991-96f4-fdd294b0ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e3368be-46a3-4b2f-9589-de3383c65201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a sentence embedding model?\n",
      "\n",
      "Response: A sentence embedding model is a method to represent input sentences as fixed-dimensional vectors, regardless of sentence length. These models have shown significant enhancements in various natural language processing tasks like information retrieval, question answering, and machine translation. They are trained to adapt to specific domains by fine-tuning on synthesized data before further fine-tuning on labeled datasets, leading to improved performance [4] [3]. Additionally, some models are designed for cross-lingual applications, training towards both sentence-level and token-level alignment to improve representation translation [5].\n",
      "\n",
      "Question: What are some known models?\n",
      "\n",
      "Response: Well-known sentence embedding models include BERT, RoBERTa, ELMO, GenSen, DSE, FastSent, Quick-Thought, InferSent, Universal Sentence Encoder (USE), XLNet, and Sentence-BERT. These models utilize various methods such as fine-tuning, contextual word representations, and special tokens to generate accurate sentence embeddings for different tasks and domains.\n",
      "\n",
      "Question: How are they different than token embedding models\n",
      "\n",
      "Response: Sentence embedding models differ from token embedding models in the way they generate representations. Sentence embedding models typically pool word or token embeddings, often by averaging, and then fine-tune this process to create embeddings for specific tasks. On the other hand, token embedding models focus on individual token representations without considering the entire sentence context. Additionally, sentence embedding models aim to minimize distances between embeddings of semantically similar phrases, while token embedding models disperse token representations across the embedding space. Overall, the key distinction lies in how sentence embedding models capture the overall meaning of a sentence compared to token embedding models that focus on individual token representations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'What is a sentence embedding model?',\n",
    "    'What are some known models?',\n",
    "    'How are they different than token embedding models'\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Question: {q}\\n\")\n",
    "    response = ce.chat(q).response\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111b1fa-5efc-4ee6-99f0-21c73dbd5433",
   "metadata": {},
   "source": [
    "Of course streaming works as well with Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8b24622-bc73-4bee-929e-377f06335a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8f16016-be7c-411f-aafd-b03d8816d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The creators behind SBERT are Reimers and Gurevych, as mentioned in the search results [3]."
     ]
    }
   ],
   "source": [
    "response = ce.stream_chat(\"Who is behind SBERT?\")\n",
    "for chunk in response.chat_stream:\n",
    "    print(chunk.delta or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ad93e-0692-4ad5-99a1-329788252b03",
   "metadata": {},
   "source": [
    "# Advanced RAG with Vectara and LLamaIndex\n",
    "\n",
    "## Agentic RAG\n",
    "\n",
    "LlamaIndex provides various agent implementations such as ChainOfThough or React.\n",
    "\n",
    "To use these with Vectara, you would need to use an external LLM as the driver of the agent resoning, and in this example we will be using OpenAI's GPT4o (for this to work, please make sure you have `OPENAI_API_KEY` defined in your environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92019803-7aaa-4199-a322-692d03b374c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "vectara_tool = QueryEngineTool(\n",
    "    query_engine=index.as_query_engine(\n",
    "        summary_enabled=True,\n",
    "        summary_num_results=5,\n",
    "        summary_response_lang=\"en\",\n",
    "        summary_prompt_name=\"mockingbird-1.0-2024-07-16\",\n",
    "    ),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"Vectara\",\n",
    "        description=\"Vectara tool that can answer Questions about Embedding Models, NLP, and related topics.\",\n",
    "    ),\n",
    ")\n",
    "agent = ReActAgent.from_tools(\n",
    "    tools=[vectara_tool],\n",
    "    llm=llm,\n",
    "    context=\"\"\"\n",
    "        You are a helpful chatbot that answers any user questions around embedding models in NLP using the Vectara tool.\n",
    "        You break down complex questions into simpler ones and use the vectara tool to answer every question or sub-question.\n",
    "        You use the Vectara tool to help answer the user question.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    max_iterations=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0ae3ea3-76f9-4898-a4bf-1788abb5d53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 89445c46-177a-4d07-938b-ef17201c6a81. Step input: \n",
      "    What are the sentence embedding models? \n",
      "    what are the best sentence embedding models, who created each model and in what year was the paper published?\n",
      "    Compare and contrast their architecture, training procedure and performance\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: Vectara\n",
      "Action Input: {'input': 'What are sentence embedding models?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Sentence embedding models are a type of model that represents a given input sentence using a fixed dimensional vector, independent of the length of the input sentence [4]. These models are trained to obtain state-of-the-art sentence representations for general semantic textual similarity tasks [1]. They have significantly improved performance in numerous downstream NLP tasks such as information retrieval, question answering, and machine translation [4]. Sentence embedding models can be domain-specific, and by injecting domain-specific knowledge of adapters into the model, they can be efficiently adapted for semantic textual similarity tasks in different domains [1]. Examples of sentence embedding models include Sentence-BERT, SimCSE-BERT/SimCSE-RoBERTa, Sentence-T5, and MP-Net [3].\n",
      "\u001b[0m> Running step 4bdb3ef1-d8de-462b-b3d5-defab3e57891. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I need to gather more specific information about the best sentence embedding models, their creators, and publication years.\n",
      "Action: Vectara\n",
      "Action Input: {'input': 'What are the best sentence embedding models, who created each model and in what year was the paper published?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the provided sources, the best sentence embedding models mentioned are:\n",
      "\n",
      "* ELMO + Attn (Wang et al. 2018)\n",
      "* DSE (Wang et al. 2018)\n",
      "* GenSen (Subramanian et al. 2018)\n",
      "* InferSent (Conneau et al. 2017)\n",
      "* SkipThought (Kiros et al. 2015)\n",
      "\n",
      "The creators of each model are:\n",
      "\n",
      "* ELMO + Attn: Wang et al. (2018)\n",
      "* DSE: Wang et al. (2018)\n",
      "* GenSen: Subramanian et al. (2018)\n",
      "* InferSent: Conneau et al. (2017)\n",
      "* SkipThought: Kiros et al. (2015)\n",
      "\n",
      "The papers were published in the following years:\n",
      "\n",
      "* ELMO + Attn: 2018\n",
      "* DSE: 2018\n",
      "* GenSen: 2018\n",
      "* InferSent: 2017\n",
      "* SkipThought: 2015\n",
      "\n",
      "Note that the sources do not provide a comprehensive list of all sentence embedding models, but rather mention these specific models in the context of comparisons and evaluations.\n",
      "\u001b[0m> Running step d5fb743b-15e9-4898-9007-b3e56229882c. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I need to compare and contrast the architecture, training procedure, and performance of the mentioned sentence embedding models.\n",
      "Action: Vectara\n",
      "Action Input: {'input': 'Compare and contrast the architecture, training procedure and performance of ELMO + Attn, DSE, GenSen, InferSent, and SkipThought models.'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Here is a summary of the comparison and contrast of the architecture, training procedure, and performance of ELMO + Attn, DSE, GenSen, InferSent, and SkipThought models based on the provided sources:\n",
      "\n",
      "**Architecture:**\n",
      "\n",
      "* ELMO + Attn employs CA (Compositional Attention) operations [1][4].\n",
      "* DSE is based on a larger transformer architecture and robust BERT pre-training [3].\n",
      "* GenSen is a sentence embedding model [5].\n",
      "* InferSent and SkipThought are not explicitly described in terms of architecture.\n",
      "\n",
      "**Training Procedure:**\n",
      "\n",
      "* ELMO + Attn has two variants: Single-Task (ST) and Multi-Task (MT) Training [5].\n",
      "* DSE is trained in a single-task manner [2].\n",
      "* GenSen is also a sentence embedding model, but its training procedure is not specified [5].\n",
      "* InferSent and SkipThought are not explicitly described in terms of training procedure.\n",
      "\n",
      "**Performance:**\n",
      "\n",
      "* DSE significantly outperforms ELMO + Attn (ST) across all datasets, with an average relative improvement of 15.6% [1][4].\n",
      "* DSE also outperforms GenSen across all datasets, with an average relative improvement of 8.9% [2].\n",
      "* DSE is competitive with InferSent and GenSen, outperforming them on 7 and 3 datasets, respectively [3].\n",
      "* BERT-Large embeddings are outperformed by DSE on 8 datasets [3].\n",
      "\n",
      "Overall, DSE is the best-performing model among the compared models, with significant improvements over ELMO + Attn and GenSen, and competitive performance with InferSent and SkipThought.\n",
      "\u001b[0m> Running step 78838c10-87b0-4d80-ae18-a1d33b02bb83. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Sentence embedding models are designed to represent sentences as fixed-dimensional vectors, which are useful for various NLP tasks. Among the top models, ELMO + Attn, DSE, GenSen, InferSent, and SkipThought have been highlighted.\n",
      "\n",
      "- **ELMO + Attn** and **DSE** were both created by Wang et al. in 2018. ELMO + Attn uses Compositional Attention operations and can be trained in Single-Task or Multi-Task modes. DSE, which utilizes a robust transformer architecture, is trained in a single-task manner and has shown significant performance improvements over other models.\n",
      "\n",
      "- **GenSen**, created by Subramanian et al. in 2018, and **InferSent**, by Conneau et al. in 2017, are both influential models, though specific details on their architecture and training procedures were not fully detailed in the sources.\n",
      "\n",
      "- **SkipThought**, developed by Kiros et al. in 2015, is another key model, though specific details about its architecture and training were not provided.\n",
      "\n",
      "In terms of performance, DSE stands out as the best-performing model, showing significant improvements over ELMO + Attn and GenSen, and competitive performance against InferSent and SkipThought.\n",
      "\u001b[0mSentence embedding models are designed to represent sentences as fixed-dimensional vectors, which are useful for various NLP tasks. Among the top models, ELMO + Attn, DSE, GenSen, InferSent, and SkipThought have been highlighted.\n",
      "\n",
      "- **ELMO + Attn** and **DSE** were both created by Wang et al. in 2018. ELMO + Attn uses Compositional Attention operations and can be trained in Single-Task or Multi-Task modes. DSE, which utilizes a robust transformer architecture, is trained in a single-task manner and has shown significant performance improvements over other models.\n",
      "\n",
      "- **GenSen**, created by Subramanian et al. in 2018, and **InferSent**, by Conneau et al. in 2017, are both influential models, though specific details on their architecture and training procedures were not fully detailed in the sources.\n",
      "\n",
      "- **SkipThought**, developed by Kiros et al. in 2015, is another key model, though specific details about its architecture and training were not provided.\n",
      "\n",
      "In terms of performance, DSE stands out as the best-performing model, showing significant improvements over ELMO + Attn and GenSen, and competitive performance against InferSent and SkipThought.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "    What are the sentence embedding models? \n",
    "    what are the best sentence embedding models, who created each model and in what year was the paper published?\n",
    "    Compare and contrast their architecture, training procedure and performance\n",
    "\"\"\"\n",
    "\n",
    "print(agent.chat(question).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43963c4",
   "metadata": {},
   "source": [
    "## Using Auto Retriever with Vectara\n",
    "\n",
    "LlamaIndex's auto-retriever functionality is really cool. \n",
    "It is most useful when you have metadata fields (like in our case of papers from Arxiv), and would like a query that references a metadata field to be automatically interpreted in the right way.\n",
    "\n",
    "For example, if I ask \"what is a paper about climate change risks published after 2020\", the auto-retriever would (behind the scences) interpret ths into a query \"what is a paper about climate change risks\" along with a filter condition of \"published > 2020\"\n",
    "\n",
    "Let's see how this works with the Vectara Index.\n",
    "First, we have to define a `VectorStoreInfo` structure that defines the meta data fields the auto-retriever knows about to do its job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24cc7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"information about a paper\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"published\",\n",
    "            description=\"The date the paper was published\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"author\",\n",
    "            description=\"The author of the paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"title\",\n",
    "            description=\"The title of the papers\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"url\",\n",
    "            description=\"The URL for this paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23712d46",
   "metadata": {},
   "source": [
    "Auto-retrieval is implemented before calling Vectara as a query transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f0333",
   "metadata": {},
   "source": [
    "Now we can define the `VectaraAutoRetriever`, which can perform auto-retrieval using Vectara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92de30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding?\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "\n",
    "retriever = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "res = retriever.retrieve(\"What is sentence embedding, based on papers before 2019?\")\n",
    "[(r.metadata['published'], r.text) for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7450bc",
   "metadata": {},
   "source": [
    "As you can see, the Auto Retriever was able to translate the natural language text into a shorter query and a proper condition (in this case `doc.published < 2019`).\n",
    "\n",
    "We can also of course ask a question directly: we use the `VectaraQueryEngine` which can work with the `VectaraAutoRetriever` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "327ffbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n",
      "Sentence embedding is a crucial technique in Natural Language Processing (NLP) that transforms sentences into low-dimensional vector representations to capture their semantic meanings. Various models have been developed to create these embeddings, enhancing the performance of NLP tasks like machine translation, document classification, and sentiment analysis. These models aim to preserve the original sentence meanings effectively in the embedded vectors, allowing for improved analysis and classification of text data.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara.query import VectaraQueryEngine\n",
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "\n",
    "ar = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=llm,\n",
    "    summary_enabled=True,\n",
    "    summary_num_results=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query_engine = VectaraQueryEngine(retriever=ar)\n",
    "response = query_engine.query(\"What is sentence embedding, based on papers before 2019?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5faef-9fe9-4acc-a55d-ad7c379697fd",
   "metadata": {},
   "source": [
    "## Advanced querying with QueryFusionRetriever\n",
    "\n",
    "The QueryFusion [Retriever](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion.html#reciprocal-rerank-fusion-retriever) is an advanced query mechanism whereby the original query is pre-processed to generate N variations. Each of these rephrased queries is then run against the Vectara engine and rank-fusion is used to combine the best results. \n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c662bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT employs a siamese network architecture, which involves two parallel encoders that process two inputs independently. The outputs are then compared or combined in some way, typically to compute similarity or difference. This is distinct from a dual-encoder architecture where the focus is on mapping inputs to a unified embedding space for direct comparison or retrieval tasks.\n"
     ]
    }
   ],
   "source": [
    "query = \"is SBERT a dual encoder? what type of DL architecture does it use?\"\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    summary_enabled=False,\n",
    "    llm=llm,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "896e8b7e-c028-4ec6-bb7c-0c05dfb86321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What is SBERT and how does it work as a dual encoder?\n",
      "2. Comparison of SBERT with other dual encoder models in natural language processing.\n",
      "3. Deep learning architecture used in SBERT for sentence embeddings.\n",
      "4. Advantages and limitations of using a dual encoder like SBERT in NLP tasks.\n",
      "SBERT is not a dual encoder. It uses a single embedding space instead of a separate embedding space, which means it does not employ a dual-encoder architecture.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import nest_asyncio\n",
    "\n",
    "rf_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(similarity_top_k=2)],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=5,  # this includes the origianl query; set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()     # apply nested async to run in a notebook\n",
    "query_engine = RetrieverQueryEngine.from_args(rf_retriever)\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6b3fd",
   "metadata": {},
   "source": [
    "We can see how the QueryFusionRetriever created additional query variations (they are displayed since we used `verbose=True`) and then the overall response includes the results fused together. This is very helpful in this case because the QueryFusionRetriever creates sub-questions that inquire about the specific architecture of SBERT which is relevant context to answering this question properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c98981",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we've seen various examples for using Vectara with LlamaIndex, which provides the following benefits:\n",
    "* Vectara provides a complete RAG pipeline, so you don't have to deal with a lot of the details around data ingestion: pre-processing, chunking, embedding, etc. Instead all these steps are handled automatically and efficiently in Vectara. \n",
    "* Being a platform, Vectara uses its own internal Embedding model (Boomerang), its own vector storage, and its own LLM (Mockingbird) for summarization, so you don't have to maintain separate API keys and relationships with additional vendors or install other products.\n",
    "* Vectara is built for large scale GenAI applications, and with the tools provided by LlamaIndex like Auto Retrieval and Query Fusion, you can easily build and test advanced RAG applications at enteprise scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
