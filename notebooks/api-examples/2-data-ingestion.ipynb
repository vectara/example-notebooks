{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/api-examples/2-data-ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Vectara Data Ingestion\n",
    "\n",
    "In this notebook we demonstrate how to ingest data into the two corpora we created:\n",
    "1. **File Upload API**: Upload AI research papers as PDFs from ArXiv\n",
    "2. **Core Indexing API**: Index Vectara documentation as pre-chunked text\n",
    "\n",
    "This creates a comprehensive dataset combining academic papers and practical documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the Agent Operating System for trusted enterprise AI: a unified Agentic RAG platform with built-in multi-modal retrieval, orchestration, and always-on governance. Deploy it on-prem (air-gapped), in your VPC, or as SaaS.\n",
    "\n",
    "Vectara provides a complete API-first platform for building production RAG and agentic applications:\n",
    "\n",
    "- **Simple Integration**: RESTful APIs and SDKs for Python, TypeScript, and Java make integration straightforward\n",
    "- **Flexible Deployment**: Choose SaaS, VPC, or on-premises deployment based on your security and compliance requirements\n",
    "- **Multi-Modal Support**: Index and search across text, tables, and images from various document formats\n",
    "- **Advanced Retrieval**: Hybrid search combining semantic and keyword matching with multiple reranking options\n",
    "- **Grounded Generation**: LLM responses with citations and factual consistency scores to reduce hallucinations\n",
    "- **Enterprise-Ready**: Built-in access controls, audit logging, and compliance certifications (SOC2, HIPAA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook assumes you've completed Notebook 1 (corpus creation) and have the corpus keys available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "# Get credentials and corpus keys from environment\n",
    "api_key = os.environ['VECTARA_API_KEY']\n",
    "research_corpus_key = 'tutorial-ai-research-papers'\n",
    "docs_corpus_key = 'tutorial-vectara-docs'\n",
    "\n",
    "# Base API URL\n",
    "BASE_URL = \"https://api.vectara.io/v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Part 1: Upload AI Research Papers (PDFs)\n",
    "\n",
    "We'll upload several key papers about RAG, embeddings, and retrieval from ArXiv. Vectara will automatically:\n",
    "- Extract text from PDFs\n",
    "- Chunk the content intelligently\n",
    "- Create Boomerang embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key research papers about RAG, LLMs, and retrieval\n",
    "research_papers = [\n",
    "    {\n",
    "        \"url\": \"https://arxiv.org/pdf/2005.11401.pdf\",\n",
    "        \"filename\": \"gpt3-language-models.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"source\": \"arxiv\",\n",
    "            \"year\": 2020,\n",
    "            \"topic\": \"LLMs\",\n",
    "            \"title\": \"Language Models are Few-Shot Learners\",\n",
    "            \"authors\": \"Brown et al.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://arxiv.org/pdf/2005.11401v4.pdf\",\n",
    "        \"filename\": \"rag-retrieval-augmented-generation.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"source\": \"arxiv\",\n",
    "            \"year\": 2020,\n",
    "            \"topic\": \"RAG\",\n",
    "            \"title\": \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\",\n",
    "            \"authors\": \"Lewis et al.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://arxiv.org/pdf/1706.03762.pdf\",\n",
    "        \"filename\": \"attention-is-all-you-need.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"source\": \"arxiv\",\n",
    "            \"year\": 2017,\n",
    "            \"topic\": \"embeddings\",\n",
    "            \"title\": \"Attention Is All You Need\",\n",
    "            \"authors\": \"Vaswani et al.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://arxiv.org/pdf/2104.08821.pdf\",\n",
    "        \"filename\": \"beir-retrieval-benchmark.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"source\": \"arxiv\",\n",
    "            \"year\": 2021,\n",
    "            \"topic\": \"retrieval\",\n",
    "            \"title\": \"BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models\",\n",
    "            \"authors\": \"Thakur et al.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://arxiv.org/pdf/2201.12086.pdf\",\n",
    "        \"filename\": \"dense-passage-retrieval.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"source\": \"arxiv\",\n",
    "            \"year\": 2022,\n",
    "            \"topic\": \"retrieval\",\n",
    "            \"title\": \"Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling\",\n",
    "            \"authors\": \"Hofstätter et al.\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://aclanthology.org/2025.naacl-short.38.pdf\",\n",
    "        \"filename\": \"hallucination-detection-naacl.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"source\": \"acl\",\n",
    "            \"year\": 2025,\n",
    "            \"topic\": \"RAG\",\n",
    "            \"title\": \"Hallucination Detection in RAG Systems\",\n",
    "            \"authors\": \"NAACL 2025\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"url\": \"https://arxiv.org/pdf/2210.03629\",\n",
    "        \"filename\": \"retrieval-evaluation-metrics.pdf\",\n",
    "        \"metadata\": {\n",
    "            \"source\": \"arxiv\",\n",
    "            \"year\": 2022,\n",
    "            \"topic\": \"retrieval\",\n",
    "            \"title\": \"Retrieval Evaluation Metrics and Methods\",\n",
    "            \"authors\": \"ArXiv 2022\"\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading gpt3-language-models.pdf...\n",
      "  Uploading to Vectara...\n",
      "  ✓ Successfully uploaded gpt3-language-models.pdf\n",
      "Downloading rag-retrieval-augmented-generation.pdf...\n",
      "  Uploading to Vectara...\n",
      "  ✓ Successfully uploaded rag-retrieval-augmented-generation.pdf\n",
      "Downloading attention-is-all-you-need.pdf...\n",
      "  Uploading to Vectara...\n",
      "  ✓ Successfully uploaded attention-is-all-you-need.pdf\n",
      "Downloading beir-retrieval-benchmark.pdf...\n",
      "  Uploading to Vectara...\n",
      "  ✓ Successfully uploaded beir-retrieval-benchmark.pdf\n",
      "Downloading dense-passage-retrieval.pdf...\n",
      "  Uploading to Vectara...\n",
      "  ✓ Successfully uploaded dense-passage-retrieval.pdf\n",
      "Downloading hallucination-detection-naacl.pdf...\n",
      "  Uploading to Vectara...\n",
      "  ✓ Successfully uploaded hallucination-detection-naacl.pdf\n",
      "Downloading retrieval-evaluation-metrics.pdf...\n",
      "  Uploading to Vectara...\n",
      "  ✓ Successfully uploaded retrieval-evaluation-metrics.pdf\n",
      "\n",
      "=== Upload Summary ===\n",
      "Total: 7, Successful: 7, Failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Upload each paper\n",
    "upload_url = f\"{BASE_URL}/corpora/{research_corpus_key}/upload_file\"\n",
    "upload_results = []\n",
    "\n",
    "for paper in research_papers:\n",
    "    try:\n",
    "        print(f\"Downloading {paper['filename']}...\")\n",
    "        # Download PDF\n",
    "        response = requests.get(paper['url'], timeout=30)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"  ✗ Failed to download: {response.status_code}\")\n",
    "            upload_results.append({'filename': paper['filename'], 'success': False, 'error': 'Download failed'})\n",
    "            continue\n",
    "            \n",
    "        pdf_content = response.content\n",
    "        \n",
    "        print(f\"  Uploading to Vectara...\")\n",
    "        files = {\n",
    "            'file': (paper['filename'], pdf_content, 'application/pdf'),\n",
    "            'metadata': (None, json.dumps(paper['metadata']), 'application/json'),\n",
    "            'table_extraction_config': (None, json.dumps({'extract_tables': True}), 'application/json')\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            'Accept': 'application/json',\n",
    "            'x-api-key': api_key\n",
    "        }\n",
    "        \n",
    "        upload_response = requests.post(upload_url, headers=headers, files=files, timeout=60)\n",
    "        \n",
    "        if upload_response.status_code in [200, 201]:\n",
    "            print(f\"  ✓ Successfully uploaded {paper['filename']}\")\n",
    "            upload_results.append({'filename': paper['filename'], 'success': True})\n",
    "        else:\n",
    "            print(f\"  ✗ Upload failed: {upload_response.status_code} - {upload_response.text}\")\n",
    "            upload_results.append({'filename': paper['filename'], 'success': False, 'error': upload_response.text})\n",
    "        \n",
    "        # Small delay between uploads\n",
    "        sleep(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        upload_results.append({'filename': paper['filename'], 'success': False, 'error': str(e)})\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in upload_results if r['success'])\n",
    "print(f\"\\n=== Upload Summary ===\")\n",
    "print(f\"Total: {len(upload_results)}, Successful: {successful}, Failed: {len(upload_results) - successful}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Part 2: Crawl and Index Vectara Documentation\n",
    "\n",
    "Now we'll use Scrapy to automatically crawl the entire docs.vectara.com site, extract content from all documentation pages, and index them using the Core Indexing API. This demonstrates how to build a RAG system from a complete website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries for web crawling\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except ImportError:\n",
    "    print(\"Installing beautifulsoup4...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', 'beautifulsoup4'])\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "try:\n",
    "    from urllib.parse import urljoin, urlparse\n",
    "except ImportError:\n",
    "    pass  # Built-in in Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawler class defined\n"
     ]
    }
   ],
   "source": [
    "# Global list to store scraped documents\n",
    "scraped_docs = []\n",
    "\n",
    "class VectaraDocsCrawler:\n",
    "    \"\"\"\n",
    "    Simple web crawler for docs.vectara.com that works in Jupyter notebooks\n",
    "    \"\"\"\n",
    "    def __init__(self, start_url, max_pages=100):\n",
    "        self.start_url = start_url\n",
    "        self.max_pages = max_pages\n",
    "        self.visited_urls = set()\n",
    "        self.to_visit = [start_url]\n",
    "        self.scraped_docs = []\n",
    "        \n",
    "    def is_valid_url(self, url):\n",
    "        \"\"\"Check if URL should be crawled\"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        \n",
    "        # Must be from docs.vectara.com\n",
    "        if parsed.netloc != 'docs.vectara.com':\n",
    "            return False\n",
    "        \n",
    "        # Must be under /docs/\n",
    "        if not parsed.path.startswith('/docs/'):\n",
    "            return False\n",
    "        \n",
    "        # Skip search pages and anchors\n",
    "        if '/search' in parsed.path or '#' in url:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def extract_content(self, url, html):\n",
    "        \"\"\"Extract content from a documentation page\"\"\"\n",
    "        try:\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            \n",
    "            # Extract title\n",
    "            title = None\n",
    "            for selector in ['h1', 'title', '.page-title']:\n",
    "                if selector.startswith('.'):\n",
    "                    title_elem = soup.select_one(selector)\n",
    "                else:\n",
    "                    title_elem = soup.find(selector)\n",
    "                if title_elem:\n",
    "                    title = title_elem.get_text(strip=True)\n",
    "                    break\n",
    "            \n",
    "            if not title:\n",
    "                title = url.split('/')[-1].replace('-', ' ').title()\n",
    "            \n",
    "            # Remove \"| Vectara\" or similar suffixes from title\n",
    "            title = title.split('|')[0].strip()\n",
    "            \n",
    "            # Find main content\n",
    "            content_elem = (\n",
    "                soup.find('article') or \n",
    "                soup.find('main') or \n",
    "                soup.find('div', class_='content') or\n",
    "                soup.find('div', {'role': 'main'})\n",
    "            )\n",
    "            \n",
    "            if not content_elem:\n",
    "                content_elem = soup.body\n",
    "            \n",
    "            # Remove navigation, headers, footers\n",
    "            for unwanted in content_elem.find_all(['nav', 'header', 'footer', 'aside']):\n",
    "                unwanted.decompose()\n",
    "            \n",
    "            # Extract chunks with section context\n",
    "            chunks = []\n",
    "            current_section = \"introduction\"\n",
    "            \n",
    "            for elem in content_elem.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'li', 'pre']):\n",
    "                if elem.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "                    # New section\n",
    "                    current_section = elem.get_text(strip=True).lower().replace(' ', '_')\n",
    "                elif elem.name == 'p':\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text and len(text) > 20:\n",
    "                        chunks.append({\n",
    "                            'text': text,\n",
    "                            'section': current_section\n",
    "                        })\n",
    "                elif elem.name == 'li':\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text and len(text) > 10:\n",
    "                        chunks.append({\n",
    "                            'text': text,\n",
    "                            'section': current_section\n",
    "                        })\n",
    "                elif elem.name == 'pre':\n",
    "                    # Code block\n",
    "                    text = elem.get_text(strip=True)\n",
    "                    if text and len(text) > 10:\n",
    "                        chunks.append({\n",
    "                            'text': f\"Code example: {text[:500]}\",  # Limit code length\n",
    "                            'section': current_section\n",
    "                        })\n",
    "            \n",
    "            # Skip pages with no meaningful content\n",
    "            if len(chunks) < 3:\n",
    "                return None\n",
    "            \n",
    "            # Determine doc_type and topic from URL\n",
    "            url_path = url.replace('https://docs.vectara.com/', '')\n",
    "            doc_type = 'guide'\n",
    "            topic = 'general'\n",
    "            \n",
    "            if '/api-reference/' in url_path or '/rest-api/' in url_path:\n",
    "                doc_type = 'api_reference'\n",
    "            \n",
    "            if 'query' in url_path or 'search' in url_path:\n",
    "                topic = 'query'\n",
    "            elif 'index' in url_path or 'upload' in url_path:\n",
    "                topic = 'indexing'\n",
    "            elif 'embed' in url_path or 'boomerang' in url_path:\n",
    "                topic = 'embeddings'\n",
    "            elif 'agent' in url_path:\n",
    "                topic = 'agents'\n",
    "            elif 'rerank' in url_path:\n",
    "                topic = 'reranking'\n",
    "            elif 'corpus' in url_path or 'corpora' in url_path:\n",
    "                topic = 'corpus_management'\n",
    "            elif 'grounded' in url_path or 'generation' in url_path:\n",
    "                topic = 'grounded_generation'\n",
    "            \n",
    "            # Store the scraped document\n",
    "            doc_data = {\n",
    "                'url': url,\n",
    "                'title': title,\n",
    "                'chunks': chunks[:50],  # Limit chunks per document\n",
    "                'doc_type': doc_type,\n",
    "                'topic': topic\n",
    "            }\n",
    "            \n",
    "            return doc_data\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error parsing {url}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_links(self, url, html):\n",
    "        \"\"\"Extract all links from a page\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        links = []\n",
    "        \n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            # Convert relative URLs to absolute\n",
    "            absolute_url = urljoin(url, href)\n",
    "            \n",
    "            # Remove fragments\n",
    "            absolute_url = absolute_url.split('#')[0]\n",
    "            \n",
    "            if self.is_valid_url(absolute_url) and absolute_url not in self.visited_urls:\n",
    "                links.append(absolute_url)\n",
    "        \n",
    "        return links\n",
    "    \n",
    "    def crawl(self):\n",
    "        \"\"\"Crawl the documentation site\"\"\"\n",
    "        print(\"Starting documentation crawler...\")\n",
    "        print(f\"Max pages: {self.max_pages}\\n\")\n",
    "        \n",
    "        while self.to_visit and len(self.visited_urls) < self.max_pages:\n",
    "            url = self.to_visit.pop(0)\n",
    "            \n",
    "            if url in self.visited_urls:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                print(f\"[{len(self.visited_urls) + 1}/{self.max_pages}] Crawling: {url}\")\n",
    "                \n",
    "                # Fetch the page\n",
    "                response = requests.get(url, timeout=10)\n",
    "                if response.status_code != 200:\n",
    "                    print(f\"  ✗ Failed to fetch: {response.status_code}\")\n",
    "                    self.visited_urls.add(url)\n",
    "                    continue\n",
    "                \n",
    "                html = response.text\n",
    "                self.visited_urls.add(url)\n",
    "                \n",
    "                # Extract content\n",
    "                doc_data = self.extract_content(url, html)\n",
    "                if doc_data:\n",
    "                    self.scraped_docs.append(doc_data)\n",
    "                    print(f\"  ✓ Extracted: {doc_data['title']} ({len(doc_data['chunks'])} chunks)\")\n",
    "                else:\n",
    "                    print(f\"  ⊘ Skipped (insufficient content)\")\n",
    "                \n",
    "                # Extract and queue new links\n",
    "                new_links = self.extract_links(url, html)\n",
    "                for link in new_links:\n",
    "                    if link not in self.visited_urls and link not in self.to_visit:\n",
    "                        self.to_visit.append(link)\n",
    "                \n",
    "                # Be respectful - small delay between requests\n",
    "                sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error: {str(e)}\")\n",
    "                self.visited_urls.add(url)\n",
    "        \n",
    "        print(f\"\\n=== Crawling Complete ===\")\n",
    "        print(f\"Pages visited: {len(self.visited_urls)}\")\n",
    "        print(f\"Documents scraped: {len(self.scraped_docs)}\")\n",
    "        \n",
    "        return self.scraped_docs\n",
    "\n",
    "print(\"Crawler class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "run-crawler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting documentation crawler...\n",
      "\n",
      "This will crawl docs.vectara.com/docs/ and extract all documentation pages.\n",
      "Depending on max_pages setting, this may take several minutes.\n",
      "\n",
      "Starting documentation crawler...\n",
      "Max pages: 50\n",
      "\n",
      "[1/50] Crawling: https://docs.vectara.com/docs/\n",
      "  ✓ Extracted: The Vectara Platform (47 chunks)\n",
      "[2/50] Crawling: https://docs.vectara.com/docs/rest-api\n",
      "  ⊘ Skipped (insufficient content)\n",
      "[3/50] Crawling: https://docs.vectara.com/docs/sdk/vectara-python-sdk\n",
      "  ✓ Extracted: Vectara Python SDK (26 chunks)\n",
      "[4/50] Crawling: https://docs.vectara.com/docs/release-notes\n",
      "  ✓ Extracted: Vectara Release Notes (50 chunks)\n",
      "[5/50] Crawling: https://docs.vectara.com/docs/changelog\n",
      "  ✓ Extracted: Vectara Documentation Changelog (50 chunks)\n",
      "[6/50] Crawling: https://docs.vectara.com/docs/learn/data-privacy/privacy-overview\n",
      "  ✓ Extracted: Privacy Overview (7 chunks)\n",
      "[7/50] Crawling: https://docs.vectara.com/docs/learn/authentication/authentication-authorization-vectara\n",
      "  ✓ Extracted: Authentication and Authorization in Vectara (26 chunks)\n",
      "[8/50] Crawling: https://docs.vectara.com/docs/getting-started\n",
      "  ✓ Extracted: Getting Started (13 chunks)\n",
      "[9/50] Crawling: https://docs.vectara.com/docs/deployments\n",
      "  ✓ Extracted: Private Deployment (11 chunks)\n",
      "[10/50] Crawling: https://docs.vectara.com/docs/data-management\n",
      "  ✓ Extracted: Data Management (11 chunks)\n",
      "[11/50] Crawling: https://docs.vectara.com/docs/generation\n",
      "  ✓ Extracted: Generation (7 chunks)\n",
      "[12/50] Crawling: https://docs.vectara.com/docs/agents/agent-platform-overview\n",
      "  ✓ Extracted: Agents (50 chunks)\n",
      "[13/50] Crawling: https://docs.vectara.com/docs/observability\n",
      "  ✓ Extracted: Observability and Evaluation (7 chunks)\n",
      "[14/50] Crawling: https://docs.vectara.com/docs/use-case-exploration\n",
      "  ✓ Extracted: Use Case Exploration (34 chunks)\n",
      "[15/50] Crawling: https://docs.vectara.com/docs/tutorials/build-a-chatbot\n",
      "  ✓ Extracted: Build a Chatbot with the Chat APIs (50 chunks)\n",
      "[16/50] Crawling: https://docs.vectara.com/docs/build-apps/app-building\n",
      "  ✓ Extracted: App Building Tools (8 chunks)\n",
      "[17/50] Crawling: https://docs.vectara.com/docs/integrations/community-collaborations-and-partnerships\n",
      "  ✓ Extracted: Community Collaborations and Partnerships (6 chunks)\n",
      "[18/50] Crawling: https://docs.vectara.com/docs/console-ui/vectara-console-overview\n",
      "  ✓ Extracted: Vectara Console Overview (13 chunks)\n",
      "[19/50] Crawling: https://docs.vectara.com/docs/console-ui/admin-center\n",
      "  ✓ Extracted: Vectara Admin Center (41 chunks)\n",
      "[20/50] Crawling: https://docs.vectara.com/docs/api-reference/api-overview\n",
      "  ✓ Extracted: Vectara APIs Overview (17 chunks)\n",
      "[21/50] Crawling: https://docs.vectara.com/docs/api-reference/vectara-postman-collection\n",
      "  ✓ Extracted: Vectara Postman Collection (46 chunks)\n",
      "[22/50] Crawling: https://docs.vectara.com/docs/quickstart\n",
      "  ✓ Extracted: Console Quick Start (17 chunks)\n",
      "[23/50] Crawling: https://docs.vectara.com/docs/api-recipes\n",
      "  ✓ Extracted: API Quick Start (39 chunks)\n",
      "[24/50] Crawling: https://docs.vectara.com/docs/vectara-trial\n",
      "  ✓ Extracted: Vectara Trial (5 chunks)\n",
      "[25/50] Crawling: https://docs.vectara.com/docs/deployments/onprem-evaluation-process\n",
      "  ✓ Extracted: Private Deployment Evaluation Process (29 chunks)\n",
      "[26/50] Crawling: https://docs.vectara.com/docs/api-reference/indexing-apis/indexing\n",
      "  ✓ Extracted: Indexing API Definition (50 chunks)\n",
      "[27/50] Crawling: https://docs.vectara.com/docs/api-reference/admin-apis/admin\n",
      "  ✓ Extracted: Corpus Administration (12 chunks)\n",
      "[28/50] Crawling: https://docs.vectara.com/docs/rest-api/vectara-rest-api-v-2\n",
      "  ✓ Extracted: Vectara REST API v2 (11 chunks)\n",
      "[29/50] Crawling: https://docs.vectara.com/docs/rest-api/agents\n",
      "  ✓ Extracted: Agents (7 chunks)\n",
      "[30/50] Crawling: https://docs.vectara.com/docs/rest-api/agent-sessions\n",
      "  ✓ Extracted: Agent Sessions (9 chunks)\n",
      "[31/50] Crawling: https://docs.vectara.com/docs/rest-api/authentication\n",
      "  ⊘ Skipped (insufficient content)\n",
      "[32/50] Crawling: https://docs.vectara.com/docs/rest-api/queries\n",
      "  ✓ Extracted: Queries (4 chunks)\n",
      "[33/50] Crawling: https://docs.vectara.com/docs/rest-api/upload\n",
      "  ⊘ Skipped (insufficient content)\n",
      "[34/50] Crawling: https://docs.vectara.com/docs/rest-api/index\n",
      "  ⊘ Skipped (insufficient content)\n",
      "[35/50] Crawling: https://docs.vectara.com/docs/rest-api/corpora\n",
      "  ✓ Extracted: Corpora (9 chunks)\n",
      "[36/50] Crawling: https://docs.vectara.com/docs/rest-api/documents\n",
      "  ✓ Extracted: Documents (6 chunks)\n",
      "[37/50] Crawling: https://docs.vectara.com/docs/rest-api/chats\n",
      "  ✓ Extracted: Chats (10 chunks)\n",
      "[38/50] Crawling: https://docs.vectara.com/docs/rest-api/generation-presets\n",
      "  ⊘ Skipped (insufficient content)\n",
      "[39/50] Crawling: https://docs.vectara.com/docs/rest-api/query-history\n",
      "  ✓ Extracted: Query History (3 chunks)\n",
      "[40/50] Crawling: https://docs.vectara.com/docs/rest-api/api-keys\n",
      "  ✓ Extracted: API Keys (6 chunks)\n",
      "[41/50] Crawling: https://docs.vectara.com/docs/rest-api/application-clients\n",
      "  ✓ Extracted: Application Clients (6 chunks)\n",
      "[42/50] Crawling: https://docs.vectara.com/docs/rest-api/large-language-models\n",
      "  ✓ Extracted: Large Language Models (5 chunks)\n",
      "[43/50] Crawling: https://docs.vectara.com/docs/rest-api/encoders\n",
      "  ✓ Extracted: Encoders (3 chunks)\n",
      "[44/50] Crawling: https://docs.vectara.com/docs/rest-api/rerankers\n",
      "  ⊘ Skipped (insufficient content)\n",
      "[45/50] Crawling: https://docs.vectara.com/docs/rest-api/jobs\n",
      "  ✓ Extracted: Jobs (3 chunks)\n",
      "[46/50] Crawling: https://docs.vectara.com/docs/rest-api/users\n",
      "  ✓ Extracted: Users (7 chunks)\n",
      "[47/50] Crawling: https://docs.vectara.com/docs/rest-api/table-extractors\n",
      "  ⊘ Skipped (insufficient content)\n",
      "[48/50] Crawling: https://docs.vectara.com/docs/rest-api/hallucination-correctors\n",
      "  ✓ Extracted: Hallucination Correctors (3 chunks)\n",
      "[49/50] Crawling: https://docs.vectara.com/docs/rest-api/llm-chat-completions\n",
      "  ⊘ Skipped (insufficient content)\n",
      "[50/50] Crawling: https://docs.vectara.com/docs/rest-api/factual-consistency-evaluation\n",
      "  ⊘ Skipped (insufficient content)\n",
      "\n",
      "=== Crawling Complete ===\n",
      "Pages visited: 50\n",
      "Documents scraped: 41\n",
      "\n",
      "Sample pages:\n",
      "  - The Vectara Platform (general)\n",
      "  - Vectara Python SDK (general)\n",
      "  - Vectara Release Notes (general)\n",
      "  - Vectara Documentation Changelog (general)\n",
      "  - Privacy Overview (general)\n"
     ]
    }
   ],
   "source": [
    "# Run the documentation crawler\n",
    "print(\"Starting documentation crawler...\\n\")\n",
    "print(\"This will crawl docs.vectara.com/docs/ and extract all documentation pages.\")\n",
    "print(\"Depending on max_pages setting, this may take several minutes.\\n\")\n",
    "\n",
    "# Create and run the crawler\n",
    "# Note: Set max_pages to control how many pages to crawl (default: 100)\n",
    "# For a full crawl, you can increase this number, but it will take longer\n",
    "crawler = VectaraDocsCrawler(start_url='https://docs.vectara.com/docs/', max_pages=50)\n",
    "scraped_docs = crawler.crawl()\n",
    "\n",
    "print(f\"\\nSample pages:\")\n",
    "for doc in scraped_docs[:5]:\n",
    "    print(f\"  - {doc['title']} ({doc['topic']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Index Scraped Documentation\n",
    "\n",
    "Now let's index the scraped documentation into Vectara using the Core Indexing API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing scraped documentation...\n",
      "\n",
      "Indexing: The Vectara Platform (47 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-\n",
      "Indexing: Vectara Python SDK (26 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-sdk-vectara-python-sdk\n",
      "Indexing: Vectara Release Notes (50 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-release-notes\n",
      "Indexing: Vectara Documentation Changelog (50 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-changelog\n",
      "Indexing: Privacy Overview (7 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-learn-data-privacy-privacy-overview\n",
      "Indexing: Authentication and Authorization in Vectara (26 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-learn-authentication-authentication-authorization-vectara\n",
      "Indexing: Getting Started (13 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-getting-started\n",
      "Indexing: Private Deployment (11 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-deployments\n",
      "Indexing: Data Management (11 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-data-management\n",
      "Indexing: Generation (7 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-generation\n",
      "Indexing: Agents (50 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-agents-agent-platform-overview\n",
      "Indexing: Observability and Evaluation (7 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-observability\n",
      "Indexing: Use Case Exploration (34 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-use-case-exploration\n",
      "Indexing: Build a Chatbot with the Chat APIs (50 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-tutorials-build-a-chatbot\n",
      "Indexing: App Building Tools (8 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-build-apps-app-building\n",
      "Indexing: Community Collaborations and Partnerships (6 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-integrations-community-collaborations-and-partnerships\n",
      "Indexing: Vectara Console Overview (13 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-console-ui-vectara-console-overview\n",
      "Indexing: Vectara Admin Center (41 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-console-ui-admin-center\n",
      "Indexing: Vectara APIs Overview (17 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-api-reference-api-overview\n",
      "Indexing: Vectara Postman Collection (46 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-api-reference-vectara-postman-collection\n",
      "Indexing: Console Quick Start (17 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-quickstart\n",
      "Indexing: API Quick Start (39 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-api-recipes\n",
      "Indexing: Vectara Trial (5 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-vectara-trial\n",
      "Indexing: Private Deployment Evaluation Process (29 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-deployments-onprem-evaluation-process\n",
      "Indexing: Indexing API Definition (50 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-api-reference-indexing-apis-indexing\n",
      "Indexing: Corpus Administration (12 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-api-reference-admin-apis-admin\n",
      "Indexing: Vectara REST API v2 (11 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-vectara-rest-api-v-2\n",
      "Indexing: Agents (7 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-agents\n",
      "Indexing: Agent Sessions (9 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-agent-sessions\n",
      "Indexing: Queries (4 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-queries\n",
      "Indexing: Corpora (9 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-corpora\n",
      "Indexing: Documents (6 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-documents\n",
      "Indexing: Chats (10 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-chats\n",
      "Indexing: Query History (3 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-query-history\n",
      "Indexing: API Keys (6 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-api-keys\n",
      "Indexing: Application Clients (6 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-application-clients\n",
      "Indexing: Large Language Models (5 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-large-language-models\n",
      "Indexing: Encoders (3 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-encoders\n",
      "Indexing: Jobs (3 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-jobs\n",
      "Indexing: Users (7 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-users\n",
      "Indexing: Hallucination Correctors (3 chunks)\n",
      "  ✓ Successfully indexed docs-vectara-com-docs-rest-api-hallucination-correctors\n",
      "\n",
      "=== Indexing Summary ===\n",
      "Total: 41, Successful: 41, Failed: 0\n",
      "\n",
      "Successfully indexed documentation:\n",
      "  ✓ The Vectara Platform\n",
      "  ✓ Vectara Python SDK\n",
      "  ✓ Vectara Release Notes\n",
      "  ✓ Vectara Documentation Changelog\n",
      "  ✓ Privacy Overview\n",
      "  ✓ Authentication and Authorization in Vectara\n",
      "  ✓ Getting Started\n",
      "  ✓ Private Deployment\n",
      "  ✓ Data Management\n",
      "  ✓ Generation\n",
      "  ✓ Agents\n",
      "  ✓ Observability and Evaluation\n",
      "  ✓ Use Case Exploration\n",
      "  ✓ Build a Chatbot with the Chat APIs\n",
      "  ✓ App Building Tools\n",
      "  ✓ Community Collaborations and Partnerships\n",
      "  ✓ Vectara Console Overview\n",
      "  ✓ Vectara Admin Center\n",
      "  ✓ Vectara APIs Overview\n",
      "  ✓ Vectara Postman Collection\n",
      "  ✓ Console Quick Start\n",
      "  ✓ API Quick Start\n",
      "  ✓ Vectara Trial\n",
      "  ✓ Private Deployment Evaluation Process\n",
      "  ✓ Indexing API Definition\n",
      "  ✓ Corpus Administration\n",
      "  ✓ Vectara REST API v2\n",
      "  ✓ Agents\n",
      "  ✓ Agent Sessions\n",
      "  ✓ Queries\n",
      "  ✓ Corpora\n",
      "  ✓ Documents\n",
      "  ✓ Chats\n",
      "  ✓ Query History\n",
      "  ✓ API Keys\n",
      "  ✓ Application Clients\n",
      "  ✓ Large Language Models\n",
      "  ✓ Encoders\n",
      "  ✓ Jobs\n",
      "  ✓ Users\n",
      "  ✓ Hallucination Correctors\n"
     ]
    }
   ],
   "source": [
    "# Index scraped documentation\n",
    "index_url = f\"{BASE_URL}/corpora/{docs_corpus_key}/documents\"\n",
    "index_results = []\n",
    "\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'x-api-key': api_key\n",
    "}\n",
    "\n",
    "print(\"Indexing scraped documentation...\\n\")\n",
    "\n",
    "for doc_data in scraped_docs:\n",
    "    try:\n",
    "        # Create a unique document ID from the URL\n",
    "        doc_id = doc_data['url'].replace('https://', '').replace('/', '-').replace('.', '-')\n",
    "        \n",
    "        # Prepare document for Core Indexing API\n",
    "        doc = {\n",
    "            'id': doc_id,\n",
    "            'type': 'core',\n",
    "            'document_parts': [\n",
    "                {\n",
    "                    'text': chunk['text'],\n",
    "                    'metadata': {'section': chunk['section']}\n",
    "                } for chunk in doc_data['chunks']\n",
    "            ],\n",
    "            'metadata': {\n",
    "                'source': 'vectara_docs',\n",
    "                'title': doc_data['title'],\n",
    "                'doc_type': doc_data['doc_type'],\n",
    "                'topic': doc_data['topic'],\n",
    "                'url': doc_data['url']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"Indexing: {doc_data['title']} ({len(doc_data['chunks'])} chunks)\")\n",
    "        response = requests.post(index_url, headers=headers, json=doc, timeout=30)\n",
    "        \n",
    "        if response.status_code in [200, 201]:\n",
    "            print(f\"  ✓ Successfully indexed {doc_id}\")\n",
    "            index_results.append({'id': doc_id, 'title': doc_data['title'], 'success': True})\n",
    "        else:\n",
    "            print(f\"  ✗ Indexing failed: {response.status_code} - {response.text[:200]}\")\n",
    "            index_results.append({'id': doc_id, 'title': doc_data['title'], 'success': False, 'error': response.text})\n",
    "        \n",
    "        sleep(0.5)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {str(e)}\")\n",
    "        index_results.append({'id': doc_id, 'title': doc_data.get('title', 'Unknown'), 'success': False, 'error': str(e)})\n",
    "\n",
    "# Summary\n",
    "successful = sum(1 for r in index_results if r['success'])\n",
    "print(f\"\\n=== Indexing Summary ===\")\n",
    "print(f\"Total: {len(index_results)}, Successful: {successful}, Failed: {len(index_results) - successful}\")\n",
    "\n",
    "if successful > 0:\n",
    "    print(f\"\\nSuccessfully indexed documentation:\")\n",
    "    for result in index_results:\n",
    "        if result['success']:\n",
    "            print(f\"  ✓ {result['title']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-cell",
   "metadata": {},
   "source": [
    "## Verify Indexed Content\n",
    "\n",
    "Let's verify that the expected number of documents were successfully indexed in both corpora by listing all documents in each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "verify-code-cell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AI Research Papers Corpus ===\n",
      "Expected: 7 documents\n",
      "\n",
      "Actual: 7 documents indexed\n",
      "\n",
      "Indexed papers:\n",
      "  • Language Models are Few-Shot Learners\n",
      "  • Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "  • Attention Is All You Need\n",
      "  • BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models\n",
      "  • Efficiently Teaching an Effective Dense Retriever with Balanced Topic Aware Sampling\n",
      "  • Hallucination Detection in RAG Systems\n",
      "  • Retrieval Evaluation Metrics and Methods\n",
      "\n",
      "✓ All 7 papers successfully indexed\n",
      "\n",
      "\n",
      "=== Vectara Documentation Corpus ===\n",
      "Expected: 41 documents\n",
      "\n",
      "Actual: 41 documents indexed\n",
      "\n",
      "Sample indexed documentation (showing first 10):\n",
      "  • The Vectara Platform (topic: general, type: guide)\n",
      "  • Vectara Python SDK (topic: general, type: guide)\n",
      "  • Vectara Release Notes (topic: general, type: guide)\n",
      "  • Vectara Documentation Changelog (topic: general, type: guide)\n",
      "  • Privacy Overview (topic: general, type: guide)\n",
      "  • Authentication and Authorization in Vectara (topic: general, type: guide)\n",
      "  • Getting Started (topic: general, type: guide)\n",
      "  • Private Deployment (topic: general, type: guide)\n",
      "  • Data Management (topic: general, type: guide)\n",
      "  • Generation (topic: grounded_generation, type: guide)\n",
      "\n",
      "✓ All 41 documentation pages successfully indexed\n",
      "\n",
      "\n",
      "=== Overall Indexing Summary ===\n",
      "Research Papers: 7/7 indexed\n",
      "Documentation: 41/41 indexed\n",
      "Total: 48 documents across both corpora\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'Accept': 'application/json',\n",
    "    'x-api-key': api_key\n",
    "}\n",
    "\n",
    "# Helper function to list all documents with pagination\n",
    "def list_all_documents(corpus_key, corpus_name):\n",
    "    \"\"\"List all documents in a corpus with pagination\"\"\"\n",
    "    all_documents = []\n",
    "    page_key = None\n",
    "    \n",
    "    while True:\n",
    "        # Build request with pagination\n",
    "        params = {'limit': 100}\n",
    "        if page_key:\n",
    "            params['page_key'] = page_key\n",
    "        \n",
    "        response = requests.get(\n",
    "            f\"{BASE_URL}/corpora/{corpus_key}/documents\",\n",
    "            headers=headers,\n",
    "            params=params\n",
    "        )\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            print(f\"  ✗ Error listing documents: {response.status_code}\")\n",
    "            print(f\"    {response.text}\")\n",
    "            break\n",
    "        \n",
    "        data = response.json()\n",
    "        documents = data.get('documents', [])\n",
    "        all_documents.extend(documents)\n",
    "        \n",
    "        # Check if there are more pages\n",
    "        page_key = data.get('metadata', {}).get('page_key')\n",
    "        if not page_key:\n",
    "            break\n",
    "    \n",
    "    return all_documents\n",
    "\n",
    "# Verify AI Research Papers corpus\n",
    "print(\"=== AI Research Papers Corpus ===\")\n",
    "print(f\"Expected: {len(research_papers)} documents\\n\")\n",
    "\n",
    "research_docs = list_all_documents(research_corpus_key, \"AI Research Papers\")\n",
    "print(f\"Actual: {len(research_docs)} documents indexed\\n\")\n",
    "\n",
    "if len(research_docs) > 0:\n",
    "    print(\"Indexed papers:\")\n",
    "    for doc in research_docs:\n",
    "        doc_id = doc.get('id', 'N/A')\n",
    "        metadata = doc.get('metadata', {})\n",
    "        title = metadata.get('title', 'Unknown')\n",
    "        print(f\"  • {title}\")\n",
    "else:\n",
    "    print(\"  ⚠ No documents found\")\n",
    "\n",
    "# Check if count matches expected\n",
    "if len(research_docs) == len(research_papers):\n",
    "    print(f\"\\n✓ All {len(research_papers)} papers successfully indexed\")\n",
    "elif len(research_docs) < len(research_papers):\n",
    "    print(f\"\\n⚠ Warning: Expected {len(research_papers)} papers but found {len(research_docs)}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Warning: Found more documents ({len(research_docs)}) than expected ({len(research_papers)})\")\n",
    "\n",
    "# Verify Vectara Documentation corpus\n",
    "print(\"\\n\\n=== Vectara Documentation Corpus ===\")\n",
    "print(f\"Expected: {len(scraped_docs)} documents\\n\")\n",
    "\n",
    "docs_docs = list_all_documents(docs_corpus_key, \"Vectara Documentation\")\n",
    "print(f\"Actual: {len(docs_docs)} documents indexed\\n\")\n",
    "\n",
    "if len(docs_docs) > 0:\n",
    "    print(f\"Sample indexed documentation (showing first 10):\")\n",
    "    for doc in docs_docs[:10]:\n",
    "        doc_id = doc.get('id', 'N/A')\n",
    "        metadata = doc.get('metadata', {})\n",
    "        title = metadata.get('title', 'Unknown')\n",
    "        topic = metadata.get('topic', 'N/A')\n",
    "        doc_type = metadata.get('doc_type', 'N/A')\n",
    "        print(f\"  • {title} (topic: {topic}, type: {doc_type})\")\n",
    "else:\n",
    "    print(\"  ⚠ No documents found\")\n",
    "\n",
    "# Check if count matches expected\n",
    "if len(docs_docs) == len(scraped_docs):\n",
    "    print(f\"\\n✓ All {len(scraped_docs)} documentation pages successfully indexed\")\n",
    "elif len(docs_docs) < len(scraped_docs):\n",
    "    print(f\"\\n⚠ Warning: Expected {len(scraped_docs)} pages but found {len(docs_docs)}\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Warning: Found more documents ({len(docs_docs)}) than expected ({len(scraped_docs)})\")\n",
    "\n",
    "# Overall summary\n",
    "print(\"\\n\\n=== Overall Indexing Summary ===\")\n",
    "print(f\"Research Papers: {len(research_docs)}/{len(research_papers)} indexed\")\n",
    "print(f\"Documentation: {len(docs_docs)}/{len(scraped_docs)} indexed\")\n",
    "print(f\"Total: {len(research_docs) + len(docs_docs)} documents across both corpora\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
