{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc259945",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e8e57",
   "metadata": {},
   "source": [
    "# Vectara and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7548c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain langchain_community langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a399ef-5cfb-4707-8ab1-8bbe1d68a169",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. An integrated API for processing input data, including text extraction from documents and ML-based chunking.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedding model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embeddings and retrieves the most relevant text segmentsthrough [hybrid search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and a variety of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking) strategies, including a [multilingual reranker](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker), [maximal marginal relevance (MMR) reranker](https://docs.vectara.com/docs/learn/mmr-reranker), [user-defined function reranker](https://docs.vectara.com/docs/learn/user-defined-function-reranker), and a [chain reranker](https://docs.vectara.com/docs/learn/chain-reranker) that provides a way to chain together multiple reranking methods to achieve better control over the reranking, combining the strengths of various reranking methods.\n",
    "\n",
    "4. An option to create a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview) with a wide selection of LLM summarizers (including Vectara's [Mockingbird](https://vectara.com/blog/mockingbird-is-a-rag-specific-llm-that-beats-gpt-4-gemini-1-5-pro-in-rag-output-quality/), trained specifically for RAG-based tasks), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits of using Vectara RAG-as-a-service to build your application are:\n",
    "* **Accuracy and Quality**: Vectara provides an end-to-end platform that focuses on eliminating hallucinations, reducing bias, and safeguarding copyright integrity.\n",
    "* **Security**: Vectara's platform provides acess control--protecting against prompt injection attacks--and meets SOC2 and HIPAA compliance.\n",
    "* **Explainability**: Vectara makes it easy to troubleshoot bad results by clearly explaining rephrased queries, LLM prompts, retrieved results, and agent actions.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f945f-eff4-498a-974b-cf93f3202df6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will need a Vectara account to use Vectara with LangChain. To get started, use the following steps:\n",
    "1. [Sign up](https://console.vectara.com/signup?utm_source=vectara&utm_medium=signup&utm_term=DevRel&utm_content=example-notebooks&utm_campaign=vectara-signup-DevRel-example-notebooks) for a Vectara account if you don't already have one. Once you have completed your sign up you will have a Vectara customer ID. You can find your customer ID by clicking on your name, on the top-right of the Vectara console window.\n",
    "2. Within your account you can create one or more corpora. Each corpus represents an area that stores text data upon ingest from input documents. To create a corpus, use the **\"Create Corpus\"** button. You then provide a name to your corpus as well as a description. Optionally you can define filtering attributes and apply some advanced options. If you click on your created corpus, you can see its name and corpus ID right on the top.\n",
    "3. Next you'll need to create API keys to access the corpus. Click on the **\"Access Control\"** tab in the corpus view and then the **\"Create API Key\"** button. Give your key a name, and choose whether you want query only or query+index for your key. Click \"Create\" and you now have an active API key. Keep this key confidential. Alternatively Vectara also provides a [Personal API key](https://vectara.com/blog/vectaras-new-personal-api-keys/) that is tied to your account and provides broader permissions.\n",
    "\n",
    "To use LangChain with Vectara, you'll need to provide your `customer ID`, `corpus ID` and an `api_key` to the LangChain Vectara class. You can do this in two ways:\n",
    "\n",
    "1. Include in your environment these three variables: `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY`.\n",
    "\n",
    "> For example, you can set these variables using `os.environ` as follows (these credentials point to one to a public Vectara corpus where the contents of [vectara documentation](https://docs.vectara.com/docs/) are indexed):\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "customer_id = '1366999410'\n",
    "corpus_id = '1'\n",
    "api_key = 'zqt_UXrBcnI2UXINZkrv4g1tQPhzj02vfdtqYJIDiA'\n",
    "\n",
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = customer_id\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = corpus_id\n",
    "os.environ[\"VECTARA_API_KEY\"] = api_key\n",
    "```\n",
    "\n",
    "2. Add them explicitly to the Vectara constructor:\n",
    "\n",
    "```python\n",
    "vectorstore = Vectara(\n",
    "                vectara_customer_id=customer_id,\n",
    "                vectara_corpus_id=corpus_id,\n",
    "                vectara_api_key=api_key\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b794d8e-7c1c-4300-8a6d-e7665a5d3e5e",
   "metadata": {},
   "source": [
    "## Vectara: RAG-as-a-service\n",
    "\n",
    "Vectara is not a vector DB, it's much more than that - it is a full **RAG-as-a-service** platform. \n",
    "Yes, we have our own internal implementation of a scalable and serverless vector database, but that is just one piece of a whole set of components needed to implement RAG. The other components include text extraction, chunking, the Boomerang embedding model, advanced retrieval such as hybrid search or MMR, multi-lingual reranker, and more.\n",
    "\n",
    "You can ingest data into Vectara directly using Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) API, using a tool like [vectara-ingest](https://github.com/vectara/vectara-ingest), or via the Vectara Langchain component directly. We will demonstrate data ingest via LangChain later in this notebook - for now let's assume you already ingested data into your Vectara corpus and see how querying works - the fun part!\n",
    "\n",
    "Throughout this notebook, we will utilize LangChain [LCEL](https://python.langchain.com/docs/expression_language/) which provides a nice syntax for chaining components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c4d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "customer_id = '1366999410'\n",
    "corpus_id = '1'\n",
    "api_key = 'zqt_UXrBcnI2UXINZkrv4g1tQPhzj02vfdtqYJIDiA'\n",
    "\n",
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = customer_id\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = corpus_id\n",
    "os.environ[\"VECTARA_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66910837-637f-4682-9c35-925783e70beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10 context documents\n",
      "Vectara is an end-to-end platform designed for product builders to integrate powerful generative AI capabilities into applications. It enhances traditional search methods by understanding the context and meaning of data, providing more accurate responses to user queries. Vectara combines keyword-based and semantic search in a hybrid model, allowing for flexible text retrieval. It supports secure data handling by not training on customer data and offers features like customer-managed keys and encryption. Vectara aims to transform data into insights, assisting in decision-making and improving user experiences with context-aware answers [1], [3], [4], [5].\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Vectara\n",
    "from langchain_community.vectorstores.vectara import (\n",
    "    RerankConfig,\n",
    "    SummaryConfig,\n",
    "    VectaraQueryConfig,\n",
    ")\n",
    "\n",
    "# Instantiate the Vectara object, pointing it to the corpus as specified by the environment variables\n",
    "vectara = Vectara()\n",
    "\n",
    "# Define configuration for generative summary component and create the \"retriever\" object\n",
    "summary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\"eng\", \n",
    "                               prompt_name=\"vectara-summary-ext-24-05-med-omni\")\n",
    "rerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\n",
    "config = VectaraQueryConfig(k=10, lambda_val=0.005, summary_config=summary_config, rerank_config=rerank_config)\n",
    "\n",
    "query_str = \"What is Vectara?\"\n",
    "rag = vectara.as_rag(config)\n",
    "res = rag.invoke(query_str)\n",
    "print(f\"We have {len(res['context'])} context documents\")\n",
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47738d47-b9b0-4d11-a670-aaf069685721",
   "metadata": {},
   "source": [
    "Notice how simple the RAG pipeline is here. It does not require access to an OpenAI key or any other external service for that matter, everything gets done inside the Vectara RAG platform. \n",
    "\n",
    "To set things up we have configured:\n",
    "- `SummaryConfig`: used to specify parameters for the generative summarizer, such as the language of the response, the number of top_k results to include in the summary, or the summarizer (prompt) name.\n",
    "- `RerankConfig`: used to control reranking, providing options like MMR or the multi-lingual reranker\n",
    "- `VectaraQueryConfig` providing the overall configuration structure to control the RAG pipeline.\n",
    "\n",
    "With this configuration, all you have to do is call `vectara.as_rag(config)` and you get a LangChain `Runnable` object on which you can run `invoke()` or `stream()`. \n",
    "\n",
    "To learn more about configuration parameters for summarization see [this document](https://github.com/langchain-ai/langchain/blob/1e748a6d406fc4ed5c3ca1218f4990e6a45530f3/docs/docs/integrations/providers/vectara/index.mdx#vectara-for-retrieval-augmented-generation-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de2540-3cf9-46f9-a6cb-e3a642cf08fa",
   "metadata": {},
   "source": [
    "## Hallucination detection and Factual Consistency Score\n",
    "\n",
    "Vectara created [HHEM](https://huggingface.co/vectara/hallucination_evaluation_model) - an open source model that can be used to evaluate RAG responses for factual consistency.\n",
    "\n",
    "As part of the Vectara RAG, the \"Factual Consistency Score\" (or FCS), which is an improved version of the open source HHEM is made available via the API. This is automatically included in the output of the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2acb95-21a8-46db-86e9-484c43d3199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The supported file types include PDFs, Microsoft Word, Text, HTML, and Markdown [5], [6].\n",
      "\n",
      "Vectara FCS = 0.46104664\n"
     ]
    }
   ],
   "source": [
    "resp1 = rag.invoke(\"What file types are supported?\")\n",
    "print(resp1[\"answer\"] + '\\n')\n",
    "print(f\"Vectara FCS = {resp1['fcs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a8367-cb2f-45cb-8282-cfaceaaa92db",
   "metadata": {},
   "source": [
    "## Vectara as a Retriever\n",
    "\n",
    "You can also integrate Vectara just as a powerful semantic search engine. Similar to other vector stores in Langchain, in this case you can use Vectara as a `retriever`, and take advantage of the stadnard `similarity_search` method (or `similarity_search_with_score`), which takes a query string and returns a list of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2218c2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title_level': '1', 'is_title': 'true', 'lang': 'eng', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/learn/data-privacy/encryption', 'title': 'Data Encryption | Vectara Docs'}, page_content='Data Encryption | Vectara Docs When you send documents to the index API or file upload API, Vectara indexes both the document text and metadata. If you choose the “textless” option for corpus creation, Vectara converts the document text into vectors for indexing but does not store the text anywhere in the platform.'),\n",
       " Document(metadata={'lang': 'eng', 'offset': '2629', 'len': '136', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/learn/authentication/role-based-access-control', 'title': 'Role-Based Access Control (RBAC) | Vectara Docs'}, page_content='• Encoder swapping. Whether the indexing and querying encoders be swapped to support semantic similarity matching in addition to question-answer matching. • Textless. Defines whether corpora be built without storing the indexed text. Although all textual content is encrypted with per-corpus keys, this option may appeal when an even higher level of security is desired. Enabling this can potentially reduce the quality of search. • User rate limit. Whether per-user rate limits can be defined.'),\n",
       " Document(metadata={'lang': 'eng', 'offset': '2759', 'len': '101', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/learn/data-privacy/encryption', 'title': 'Data Encryption | Vectara Docs'}, page_content='In the future, you will be able to set the ARN on the Vectara Console and these instructions will be updated. Once your AWS KMS key is configured in the platform, when encrypting your document text or metadata, Vectara connects to your KMS service to generate an encryption key. The encryption key provided by the KMS is stored in-memory and used to encrypt and decrypt your data. The in-memory key expires every hour. In turn, every hour Vectara\\n\\n asks your AWS KMS to generate that encryption key again.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.summary_config.is_enabled = False\n",
    "config.k = 3\n",
    "\n",
    "retriever = vectara.as_retriever(config=config)\n",
    "retriever.invoke(\"is data encrypted?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16f2eb-65d1-4189-9c60-203344a8920d",
   "metadata": {},
   "source": [
    "## Vectara Chat\n",
    "\n",
    "Vectara now supports Chat functionality without any additional components. all you have to do is call `as_chat()` and the resulting bot uses Vectara's native Chat functionality behind the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed80843-4bdc-40fd-989b-9dc73ae97c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\"eng\")\n",
    "rerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\n",
    "config = VectaraQueryConfig(\n",
    "    k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config\n",
    ")\n",
    "\n",
    "bot = vectara.as_chat(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22bb52f3-c987-48a6-82a1-2d51370da76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The FILE_UPLOAD API by Vectara allows extraction of text from unstructured documents like PDFs and Microsoft Word files, supporting common file types[1]. It has a 10 MB file size limit and is recommended when custom extraction logic is not in place[2][4]. The API enables attaching user-defined metadata at the document level for optimized searches[4][6]. When files are sent to the API, both the text and metadata are indexed by Vectara, with an option to convert text into vectors for indexing while not storing the text in the platform if the \"textless\" option is chosen[7].'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.invoke(\"What is the FILE_UPLOAD API?\")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7921c40-56cd-4253-a7ff-95dd061f0152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The FILE_UPLOAD API and the standard indexing API offered by Vectara serve different purposes. The FILE_UPLOAD API allows you to extract text from unstructured documents like PDFs or Microsoft Word files, with the option to include user-defined metadata at the document level [6]. It is suitable when you have not created your own extraction logic [5]. On the other hand, the standard indexing API is recommended for structured data and provides more control over data segmentation and indexing processes [3]. It transforms structured data into a searchable format quickly and supports various data formats by allowing specification of multiple document attributes and metadata [3]. Each API caters to specific needs based on the nature of the documents and the level of control required during the indexing process.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.invoke(\"how is it different than standard indexing API?\")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00784590-2e82-4059-be19-20e63a95e580",
   "metadata": {},
   "source": [
    "## Advanced LangChain query pre-processing with Vectara\n",
    "\n",
    "Vectara's \"RAG as a service\" does a lot of the heavy lifting in creating question answering or chatbot chains. The integration with LangChain provides the option to use additional capabilities such as query pre-processing like SelfQueryRetriever or [MultiQueryRetriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/MultiQueryRetriever/). Let's look at an example of using the MultiQueryRetriever.\n",
    "\n",
    "Since MQR uses an LLM we have to set that up - here we choose ChatOpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c19a5a-6f85-44a7-b73c-009f7c42de16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vectara provides a Hybrid Search that offers a powerful and flexible approach to text retrieval. We combine partial, exact, and Boolean text matching with neural models which blends traditional, keyword-based search with semantic search in what is called \"hybrid\" retrieval model. For example, Vectara enables you to do the following:\\n• Include exact keyword matches for occasions where a search term was absent from Vectara\\'s training data (e.g. product SKUs)\\n• Disable neural retrieval entirely, and instead use exact term matching\\n• Incorporate typical keyword modifiers like a function, exact phrase matching, and wildcard prefixes of terms\\n\\nThe exact and Boolean text matching (similar to a traditional, keyword-based search) is disabled by default and Vectara only uses neural retrieval. You can enable hybrid search by specifying a value, , at query time, specifically under the . This value can range from to (inclusive).'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "mqr = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "def get_summary(documents):\n",
    "    return documents[-1].page_content\n",
    "\n",
    "(mqr | get_summary).invoke(query_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12cc05-7431-4b9d-971c-2769d82a81b8",
   "metadata": {},
   "source": [
    "## Agentic RAG with Vectara\n",
    "\n",
    "Agentic RAG is a powerful methodology to provide your RAG implementation more agency with approaches like ReAct.\n",
    "The code below demonstrates how to use Vectara with LangChain to create an agent that uses Vectara for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0072afb-e9de-4bd1-b29f-62cb504c96d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ofer/miniconda3/envs/langchain/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3577: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "/Users/ofer/miniconda3/envs/langchain/lib/python3.11/site-packages/langsmith/client.py:323: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo answer your question, I will first gather information about API keys and JWT tokens using the Vectara tool.\n",
      "\n",
      "Action: vectara_tool\n",
      "Action Input: \"What is an API key?\"\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mAn API key is a unique code that grants access to specific functionalities within a system. It allows users to perform various operations such as querying, indexing, and managing data. API keys can be easily created, managed, and revoked to ensure security. They are used for controlled and anonymous access, simplifying integration with external systems like websites. API keys can have different levels of permissions, with some keys providing read-only access while others allow both read and write operations. It is essential to handle API keys with caution, similar to passwords, especially in production environments.\u001b[0m\u001b[32;1m\u001b[1;3mI have gathered information about API keys. Now, I will gather information about JWT tokens using the Vectara tool.\n",
      "\n",
      "Action: vectara_tool\n",
      "Action Input: \"What is a JWT token?\"\u001b[0m\u001b[36;1m\u001b[1;3mA JWT token is a type of token used for authentication in OAuth 2.0. It contains necessary information like the authorization URL, Client ID, and Client Secret to authenticate API requests. JWT tokens are generated using specific values and can be utilized in API requests by passing them in the header configuration, typically in the Bearer Token field. They offer advantages such as tighter security scopes compared to API keys and are easily detectable by security scanning tools for enhanced security measures [1][2][3][4][5].\u001b[0m\u001b[32;1m\u001b[1;3mI have gathered information about both API keys and JWT tokens. Now, I will summarize the information and explain when to use each.\n",
      "\n",
      "Final Answer: An API key is a unique code that grants access to specific functionalities within a system, allowing users to perform operations like querying and managing data. It is used for controlled and anonymous access and can have different permission levels. A JWT token, on the other hand, is used for authentication in OAuth 2.0, containing information like the authorization URL, Client ID, and Client Secret. It is used in API requests for authentication by passing it in the header configuration.\n",
      "\n",
      "You should use an API key when you need a simple way to authenticate and authorize access to an API, especially when the access is anonymous or when you need to control access levels. JWT tokens are more suitable when you need tighter security scopes and when you are dealing with authentication in OAuth 2.0, as they provide enhanced security measures.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "An API key is a unique code that grants access to specific functionalities within a system, allowing users to perform operations like querying and managing data. It is used for controlled and anonymous access and can have different permission levels. A JWT token, on the other hand, is used for authentication in OAuth 2.0, containing information like the authorization URL, Client ID, and Client Secret. It is used in API requests for authentication by passing it in the header configuration.\n",
      "\n",
      "You should use an API key when you need a simple way to authenticate and authorize access to an API, especially when the access is anonymous or when you need to control access levels. JWT tokens are more suitable when you need tighter security scopes and when you are dealing with authentication in OAuth 2.0, as they provide enhanced security measures.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "@tool\n",
    "def vectara_tool(\n",
    "    query: str = Field(description=\"the query string.\"), \n",
    ")-> str:\n",
    "    \"\"\"A tool for getting answers to questions about Vectara's product and API API.\"\"\"\n",
    "    summary_config = SummaryConfig(is_enabled=True, max_results=5, \n",
    "                                   response_lang=\"eng\", prompt_name=\"vectara-summary-ext-24-05-sml\")\n",
    "    rerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\n",
    "    config = VectaraQueryConfig(\n",
    "        k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config\n",
    "    )\n",
    "    \n",
    "    rag = vectara.as_rag(config)\n",
    "    return rag.invoke(query)['answer']\n",
    "    \n",
    "tools = [vectara_tool]\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt.template = '''\n",
    "Answer the following question as best you can. \n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "\n",
    "(This Thought/Action/Action Input/Observation can repeat N times)\n",
    "\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Instructions:\n",
    "- If a tool response with an incorrect response, you can rephrase your question and try again.\n",
    "- Base your response primarily on information provided by tools and not prior knowledge.\n",
    "- Tools respond better to shorter and more concise queries, so try to break down complex questions into simpler sub-questions.\n",
    "\n",
    "\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "'''\n",
    "\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(agent_executor.invoke({\"input\": \"What is a an API key? What is a JWT token? when should I use one or the other?\"})['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee237e",
   "metadata": {},
   "source": [
    "## LangChain Vectara Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac4600",
   "metadata": {},
   "source": [
    "LangChain templates offer a collection of easily deployable reference architectures, and there are two templates for using Vectara:\n",
    "* [RAG](https://github.com/langchain-ai/langchain/tree/master/templates/rag-vectara) template for basic RAG.\n",
    "* [RAG with multi-query](https://github.com/langchain-ai/langchain/tree/master/templates/rag-vectara-multiquery) for using Vectara RAG with the multi-query retriever.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c999c-930d-4a46-b397-7c32dad08d5f",
   "metadata": {},
   "source": [
    "## Data Ingestion into Vectara with LangChain\n",
    "\n",
    "Even though it is more common to use Vectara with LangChain for query purposes, it is also possible to ingest data into Vectara via LangChain. There are two main functions that are useful for this purpose: `add_texts` (or `add_documents` which is similar with a lightly different interface) and `add_files`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dfba86-8bed-4394-bbd1-667ef38830f4",
   "metadata": {},
   "source": [
    "For `add_texts` the input is simply a set of text strings:\n",
    "\n",
    "```python\n",
    "vectara.add_texts([\"to be or not to be\", \"that is the question\"])\n",
    "```\n",
    "\n",
    "A common pattern is to use one of LangChain's data upload classes, extract the text from there, and then upload the text. Note that no chunking is necessary (although it is optional) in this case since Vectara performs its own optimal chunking.\n",
    "\n",
    "Since Vectara supports [file upload](https://docs.vectara.com/docs/api-reference/indexing-apis/file-upload/file-upload) natively, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly in the LangChain class. When using this method, the file is uploaded directly to the Vectara platform, processed and chunked optimally there, so you don't have to use the LangChain document loader or chunking mechanism.\n",
    "\n",
    "As an example:\n",
    "\n",
    "```python\n",
    "vectara.add_files([\"path/to/file1.pdf\", \"path/to/file2.pdf\",...])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
