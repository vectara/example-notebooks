{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/api-examples/3-query-api.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara Query API Examples\n",
    "\n",
    "In this notebook we demonstrate how to use Vectara's Query API using direct REST API calls. We'll cover:\n",
    "- Single corpus queries with hybrid search and reranking\n",
    "- Multiple corpora queries\n",
    "- Metadata filtering\n",
    "- Streaming responses\n",
    "- Conversational chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the Agent Operating System for trusted enterprise AI: a unified Agentic RAG platform with built-in multi-modal retrieval, orchestration, and always-on governance. Deploy it on-prem (air-gapped), in your VPC, or as SaaS. Vectara agents deliver grounded answers and safe actions with source citations, step-level audit trails, fine-grained access controls, and real-time policy and factual-consistency enforcement, so teams ship faster with lower risk, and with trusted, production-grade AI agents at scale.\n",
    "\n",
    "Vectara provides a complete API-first platform for building production RAG and agentic applications:\n",
    "\n",
    "- **Simple Integration**: RESTful APIs and SDKs (Python, JavaScript) for quick integration into any stack\n",
    "- **Flexible Deployment**: Choose SaaS, VPC, or on-premises deployment based on your requirements\n",
    "- **Multi-Modal Support**: Index and search across text, tables, and images from PDFs, documents, and structured data\n",
    "- **Advanced Retrieval**: Hybrid search combining semantic and keyword matching with state-of-the-art reranking\n",
    "- **Grounded Generation**: LLM responses with citations and factual consistency scores to reduce hallucinations\n",
    "- **Enterprise-Ready**: Built-in access controls, audit logging, and compliance (SOC2, HIPAA) from day one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This notebook assumes you've completed Notebooks 1 and 2:\n",
    "- Notebook 1: Created two corpora (ai-research-papers and vectara-docs) with Boomerang embeddings\n",
    "- Notebook 2: Ingested AI research papers and Vectara documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Corpus: tutorial-ai-research-papers\n",
      "Docs Corpus: tutorial-vectara-docs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Set up authentication\n",
    "api_key = os.environ['VECTARA_API_KEY']\n",
    "\n",
    "# Get corpus keys from environment (set these from Notebook 1 output)\n",
    "research_corpus_key = 'tutorial-ai-research-papers'\n",
    "docs_corpus_key = 'tutorial-vectara-docs'\n",
    "\n",
    "# Base URL for Vectara API v2\n",
    "BASE_URL = \"https://api.vectara.io/v2\"\n",
    "\n",
    "# Common headers for all requests\n",
    "headers = {\n",
    "    'Content-Type': 'application/json',\n",
    "    'Accept': 'application/json',\n",
    "    'x-api-key': api_key\n",
    "}\n",
    "\n",
    "print(f\"Research Corpus: {research_corpus_key}\")\n",
    "print(f\"Docs Corpus: {docs_corpus_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Example 1: Basic Query with Hybrid Search and Reranking\n",
    "\n",
    "This example demonstrates a single corpus query using:\n",
    "- Hybrid search (lexical_interpolation=0.005 for best semantic search)\n",
    "- Chain reranker combining multilingual reranker with MMR (diversity_bias=0.05) for improved relevance and diversity\n",
    "- Two-stage retrieval: fetch 30 results, rerank to top 10\n",
    "- Generation with vectara-summary-ext-24-05-med-omni preset\n",
    "- Factual Consistency Score to detect potential hallucinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40947545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generated Summary ===\n",
      "Retrieval-augmented generation (RAG) is a method that combines pre-trained parametric memory models, such as sequence-to-sequence (seq2seq) transformers, with non-parametric memory, which is typically a dense vector index of external data sources like Wikipedia. This approach uses a pre-trained neural retriever to access the non-parametric memory, allowing the model to retrieve relevant information to enhance language generation tasks. RAG models can condition on the same retrieved passages for the entire generated sequence or use different passages for each token, providing flexibility in generating responses for knowledge-intensive tasks [1], [4].\n",
      "\n",
      "=== Factual Consistency Score: 0.96875 ===\n"
     ]
    }
   ],
   "source": [
    "# Construct the query request - querying research papers corpus\n",
    "query_request = {\n",
    "    \"query\": \"What is retrieval augmented generation?\",\n",
    "    \"search\": {\n",
    "        \"corpora\": [\n",
    "            {\n",
    "                \"corpus_key\": research_corpus_key,\n",
    "                \"lexical_interpolation\": 0.005\n",
    "            }\n",
    "        ],\n",
    "        \"limit\": 100,\n",
    "        \"context_configuration\": {\n",
    "            \"sentences_before\": 2,\n",
    "            \"sentences_after\": 2\n",
    "        },\n",
    "        \"reranker\": {\n",
    "            \"type\": \"chain\",\n",
    "            \"rerankers\": [\n",
    "                {\n",
    "                    \"type\": \"customer_reranker\",\n",
    "                    \"reranker_id\": \"rnk_272725719\", \n",
    "                    \"limit\": 30,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"mmr\",\n",
    "                    \"diversity_bias\": 0.05\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "    \"generation\": {\n",
    "        \"generation_preset_name\": \"vectara-summary-ext-24-05-med-omni\",\n",
    "        \"max_used_search_results\": 10,\n",
    "        \"response_language\": \"eng\",\n",
    "        \"enable_factual_consistency_score\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make the query request\n",
    "url = f\"{BASE_URL}/query\"\n",
    "response = requests.post(url, headers=headers, json=query_request)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"\\n=== Generated Summary ===\")\n",
    "    print(result['summary'])\n",
    "    print(f\"\\n=== Factual Consistency Score: {result.get('factual_consistency_score', 'N/A')} ===\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "### Examining Search Results and Citations\n",
    "\n",
    "The response includes the retrieved documents that were used to generate the summary, along with citation information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Top Search Results ===\n",
      "\n",
      "--- Result 1 ---\n",
      "Text: Additionally, providing provenance for their\n",
      "decisions and updating their world knowledge remain open research problems. Pre-\n",
      "trained models with a differentiable access mechanism to explicit non-para...\n",
      "Score: 0.9941574335098267\n",
      "Document ID: rag-retrieval-augmented-generation.pdf\n",
      "Metadata: {'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'CreationDate': 'D:20210413004838Z', 'Keywords': '', 'Producer': 'pdfTeX-1.40.21', 'Author': '', 'Title': '', 'Creator': 'LaTeX with hyperref', 'ModDate': 'D:20210413004838Z', 'Trapped': '/False', 'Subject': '', 'source': 'arxiv', 'year': 2020, 'topic': 'RAG', 'title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks', 'authors': 'Lewis et al.'}\n",
      "\n",
      "--- Result 2 ---\n",
      "Text: arXiv preprint arXiv:2203.05115, 2022. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\n",
      "    Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et...\n",
      "Score: 0.8947175145149231\n",
      "Document ID: retrieval-evaluation-metrics.pdf\n",
      "Metadata: {'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'CreationDate': 'D:20230313000911Z', 'Keywords': '', 'Producer': 'pdfTeX-1.40.21', 'Author': '', 'Title': '', 'Creator': 'LaTeX with hyperref', 'ModDate': 'D:20230313000911Z', 'Trapped': '/False', 'Subject': '', 'source': 'arxiv', 'year': 2022, 'topic': 'retrieval', 'title': 'Retrieval Evaluation Metrics and Methods', 'authors': 'ArXiv 2022'}\n",
      "\n",
      "--- Result 3 ---\n",
      "Text: Additionally, providing provenance for their\n",
      "decisions and updating their world knowledge remain open research problems. Pre-\n",
      "trained models with a differentiable access mechanism to explicit non-para...\n",
      "Score: 0.8945203423500061\n",
      "Document ID: gpt3-language-models.pdf\n",
      "Metadata: {'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'CreationDate': 'D:20210413004838Z', 'Keywords': '', 'Producer': 'pdfTeX-1.40.21', 'Author': '', 'Title': '', 'Creator': 'LaTeX with hyperref', 'ModDate': 'D:20210413004838Z', 'Trapped': '/False', 'Subject': '', 'source': 'arxiv', 'year': 2020, 'topic': 'LLMs', 'title': 'Language Models are Few-Shot Learners', 'authors': 'Brown et al.'}\n",
      "\n",
      "--- Result 4 ---\n",
      "Text: but have only explored open-domain extractive question answering. Here, we bring hybrid parametric\n",
      "and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We e...\n",
      "Score: 0.8935925364494324\n",
      "Document ID: gpt3-language-models.pdf\n",
      "Metadata: {'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'CreationDate': 'D:20210413004838Z', 'Keywords': '', 'Producer': 'pdfTeX-1.40.21', 'Author': '', 'Title': '', 'Creator': 'LaTeX with hyperref', 'ModDate': 'D:20210413004838Z', 'Trapped': '/False', 'Subject': '', 'source': 'arxiv', 'year': 2020, 'topic': 'LLMs', 'title': 'Language Models are Few-Shot Learners', 'authors': 'Brown et al.'}\n",
      "\n",
      "--- Result 5 ---\n",
      "Text: but have only explored open-domain extractive question answering. Here, we bring hybrid parametric\n",
      "and non-parametric memory to the “workhorse of NLP,” i.e. sequence-to-sequence (seq2seq) models. We e...\n",
      "Score: 0.891611635684967\n",
      "Document ID: rag-retrieval-augmented-generation.pdf\n",
      "Metadata: {'PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'CreationDate': 'D:20210413004838Z', 'Keywords': '', 'Producer': 'pdfTeX-1.40.21', 'Author': '', 'Title': '', 'Creator': 'LaTeX with hyperref', 'ModDate': 'D:20210413004838Z', 'Trapped': '/False', 'Subject': '', 'source': 'arxiv', 'year': 2020, 'topic': 'RAG', 'title': 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks', 'authors': 'Lewis et al.'}\n"
     ]
    }
   ],
   "source": [
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    \n",
    "    # Display first 5 search results\n",
    "    print(\"\\n=== Top Search Results ===\")\n",
    "    for i, search_result in enumerate(result.get('search_results', [])[:5], 1):\n",
    "        print(f\"\\n--- Result {i} ---\")\n",
    "        print(f\"Text: {search_result['text'][:200]}...\")\n",
    "        print(f\"Score: {search_result.get('score', 'N/A')}\")\n",
    "        print(f\"Document ID: {search_result.get('document_id', 'N/A')}\")\n",
    "        if 'document_metadata' in search_result:\n",
    "            print(f\"Metadata: {search_result['document_metadata']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Example 2: Querying Multiple Corpora\n",
    "\n",
    "Vectara allows you to query across multiple corpora simultaneously. This is useful when you have data organized across different collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generated Summary (Multiple Corpora) ===\n",
      "Vectara Agents work by enabling enterprises to build sophisticated, enterprise-grade intelligent applications that go beyond basic question answering. These agents interpret user input, reason through context, leverage external tools, and maintain continuity across multi-turn interactions. Unlike traditional Retrieval Augmented Generation (RAG) systems that simply retrieve documents and pass them to a language model, Vectara agents provide orchestrated workflows capable of taking action, retrieving information, invoking APIs, or maintaining user sessions. This comprehensive framework allows for the creation of AI-powered applications that can autonomously reason through problems, orchestrate multiple tools, maintain conversation context, and integrate with enterprise systems through standardized protocols [1], [6].\n",
      "\n",
      "=== Factual Consistency Score: 0.9609375 ===\n",
      "\n",
      "=== Result Sources ===\n",
      "1. Source: vectara_docs, Title: Agents\n",
      "2. Source: vectara_docs, Title: The Vectara Platform\n",
      "3. Source: vectara_docs, Title: Getting Started\n",
      "4. Source: vectara_docs, Title: Agents\n",
      "5. Source: vectara_docs, Title: Community Collaborations and Partnerships\n"
     ]
    }
   ],
   "source": [
    "# Query both corpora simultaneously\n",
    "# This combines results from research papers AND documentation\n",
    "multi_corpus_request = {\n",
    "    \"query\": \"How do Agents work with Vectara?\",\n",
    "    \"search\": {\n",
    "        \"corpora\": [\n",
    "            {\n",
    "                \"corpus_key\": research_corpus_key,\n",
    "                \"lexical_interpolation\": 0.005\n",
    "            },\n",
    "            {\n",
    "                \"corpus_key\": docs_corpus_key,\n",
    "                \"lexical_interpolation\": 0.005\n",
    "            }\n",
    "        ],\n",
    "        \"limit\": 100,\n",
    "        \"context_configuration\": {\n",
    "            \"sentences_before\": 2,\n",
    "            \"sentences_after\": 2\n",
    "        },\n",
    "        \"reranker\": {\n",
    "            \"type\": \"chain\",\n",
    "            \"rerankers\": [\n",
    "                {\n",
    "                    \"type\": \"customer_reranker\",\n",
    "                    \"reranker_id\": \"rnk_272725719\",\n",
    "                    \"limit\": 30\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"mmr\",\n",
    "                    \"diversity_bias\": 0.05\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "    \"generation\": {\n",
    "        \"generation_preset_name\": \"vectara-summary-ext-24-05-med-omni\",\n",
    "        \"max_used_search_results\": 10,\n",
    "        \"response_language\": \"eng\",\n",
    "        \"enable_factual_consistency_score\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{BASE_URL}/query\", headers=headers, json=multi_corpus_request)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"\\n=== Generated Summary (Multiple Corpora) ===\")\n",
    "    print(result['summary'])\n",
    "    print(f\"\\n=== Factual Consistency Score: {result.get('factual_consistency_score', 'N/A')} ===\")\n",
    "    \n",
    "    # Show which corpus each result came from\n",
    "    print(\"\\n=== Result Sources ===\")\n",
    "    for i, search_result in enumerate(result.get('search_results', [])[:5], 1):\n",
    "        doc_meta = search_result.get('document_metadata', {})\n",
    "        source = doc_meta.get('source', 'unknown')\n",
    "        title = doc_meta.get('title', 'N/A')\n",
    "        print(f\"{i}. Source: {source}, Title: {title}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bd62a-d6cb-4899-bfda-29f632c3b482",
   "metadata": {},
   "source": [
    "## Example 3: Metadata Filtering\n",
    "\n",
    "You can filter search results using metadata filters. This allows you to narrow down results based on document or chunk-level metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fe7a8fe-cfb8-4132-a676-5aa2b38d21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generated Summary (With Metadata Filter) ===\n",
      "The key innovations in Retrieval-Augmented Generation (RAG) include the integration of retrieval mechanisms with generation models to enhance the accuracy and reliability of generated content. This approach allows large language models (LLMs) to generate answers or summaries by leveraging external knowledge sources, thereby reducing the likelihood of hallucinations, which are unsupported or incorrect information in the generated text. Additionally, the development of benchmarks like FaithBench and RAGTruth provides a framework for evaluating and improving the trustworthiness of RAG systems by focusing on hallucination detection and mitigation strategies [2], [5], [10].\n",
      "\n",
      "=== Number of results: 30 ===\n",
      "\n",
      "=== Filtered Papers ===\n",
      "- Hallucination Detection in RAG Systems (2025) - Topic: RAG\n",
      "- Hallucination Detection in RAG Systems (2025) - Topic: RAG\n",
      "- Hallucination Detection in RAG Systems (2025) - Topic: RAG\n"
     ]
    }
   ],
   "source": [
    "# Example with metadata filtering\n",
    "# Filter to only get research papers from 2020 or later\n",
    "filtered_request = {\n",
    "    \"query\": \"What are the key innovations in retrieval augmented generation?\",\n",
    "    \"search\": {\n",
    "        \"corpora\": [\n",
    "            {\n",
    "                \"corpus_key\": research_corpus_key,\n",
    "                \"lexical_interpolation\": 0.005,\n",
    "                # Filter for recent RAG papers\n",
    "                \"metadata_filter\": \"doc.year >= 2023\"\n",
    "            }\n",
    "        ],\n",
    "        \"limit\": 100,\n",
    "        \"context_configuration\": {\n",
    "            \"sentences_before\": 2,\n",
    "            \"sentences_after\": 2\n",
    "        },\n",
    "        \"reranker\": {\n",
    "            \"type\": \"chain\",\n",
    "            \"rerankers\": [\n",
    "                {\n",
    "                    \"type\": \"customer_reranker\",\n",
    "                    \"reranker_id\": \"rnk_272725719\",\n",
    "                    \"limit\": 30\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"mmr\",\n",
    "                    \"diversity_bias\": 0.05\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "    \"generation\": {\n",
    "        \"generation_preset_name\": \"vectara-summary-ext-24-05-med-omni\",\n",
    "        \"max_used_search_results\": 10,\n",
    "        \"response_language\": \"eng\",\n",
    "        \"enable_factual_consistency_score\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(f\"{BASE_URL}/query\", headers=headers, json=filtered_request)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(\"\\n=== Generated Summary (With Metadata Filter) ===\")\n",
    "    print(result['summary'])\n",
    "    print(f\"\\n=== Number of results: {len(result.get('search_results', []))} ===\")\n",
    "    \n",
    "    # Show filtered results\n",
    "    print(\"\\n=== Filtered Papers ===\")\n",
    "    for search_result in result.get('search_results', [])[:3]:\n",
    "        doc_meta = search_result.get('document_metadata', {})\n",
    "        print(f\"- {doc_meta.get('title', 'N/A')} ({doc_meta.get('year', 'N/A')}) - Topic: {doc_meta.get('topic', 'N/A')}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9643b59-2424-4a54-89aa-5116f052a541",
   "metadata": {},
   "source": [
    "## Example 4: Streaming Responses\n",
    "\n",
    "For better user experience, you can stream the generated response in real-time using Server-Sent Events (SSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe64d30c-eead-4f40-969e-be18033bdea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Streaming Response ===\n",
      "To use chunking with Vectara, you can choose between sentence-based and character-based chunking strategies. By default, Vectara uses sentence-based chunking, where each chunk contains one complete sentence. This method can lead to higher retrieval latency due to the increased number of chunks. Alternatively, you can opt for character-based chunking to create larger chunks by setting the type to `max_chars_chunking_strategy` and defining the `max_chars_per_chunk` value. This allows you to create chunks containing 3-7 sentences (512 to 1024 characters), balancing retrieval speed and contextual integrity [1], [2], [3].\n",
      "\n",
      "=== FCS: 0.78125 ===\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Streaming query request - query the documentation corpus\n",
    "streaming_request = {\n",
    "    \"query\": \"How do I use chunking with Vectara\",\n",
    "    \"stream_response\": True,\n",
    "    \"search\": {\n",
    "        \"corpora\": [\n",
    "            {\n",
    "                \"corpus_key\": docs_corpus_key,\n",
    "                \"lexical_interpolation\": 0.005\n",
    "            }\n",
    "        ],\n",
    "        \"limit\": 100,\n",
    "        \"context_configuration\": {\n",
    "            \"sentences_before\": 2,\n",
    "            \"sentences_after\": 2\n",
    "        },\n",
    "        \"reranker\": {\n",
    "            \"type\": \"chain\",\n",
    "            \"rerankers\": [\n",
    "                {\n",
    "                    \"type\": \"customer_reranker\",\n",
    "                    \"reranker_id\": \"rnk_272725719\",\n",
    "                    \"limit\": 30\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"mmr\",\n",
    "                    \"diversity_bias\": 0.05\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    },\n",
    "    \"generation\": {\n",
    "        \"generation_preset_name\": \"vectara-summary-ext-24-05-med-omni\",\n",
    "        \"max_used_search_results\": 15,\n",
    "        \"response_language\": \"eng\",\n",
    "        \"enable_factual_consistency_score\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make streaming request\n",
    "streaming_headers = headers.copy()\n",
    "streaming_headers['Accept'] = 'text/event-stream'\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/query\",\n",
    "    headers=streaming_headers,\n",
    "    json=streaming_request,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "print(\"\\n=== Streaming Response ===\")\n",
    "if response.status_code == 200:\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode('utf-8')\n",
    "            if line_str.startswith('data:'):\n",
    "                try:\n",
    "                    data = json.loads(line_str[5:])  # Remove 'data: ' prefix\n",
    "                    # Handle different event types\n",
    "                    if data.get('type') == 'generation_chunk':\n",
    "                        # Print generation text as it arrives\n",
    "                        print(data.get('generation_chunk', ''), end='', flush=True)\n",
    "                    elif data.get('type') == 'factual_consistency_score':\n",
    "                        print(f\"\\n\\n=== FCS: {data.get('factual_consistency_score')} ===\")\n",
    "                    elif data.get('type') == 'search_results':\n",
    "                        # Search results arrive before generation starts\n",
    "                        pass\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "    print(\"\\n\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
