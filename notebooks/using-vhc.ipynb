{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85034fee-f2f9-485b-8d06-de51234a2bbe",
   "metadata": {},
   "source": [
    "# Vectara Hallucination Corrector\n",
    "\n",
    "In spite of the amazing power of LLMs, they still do hallucinate. In some cases, where creativity is required, hallucinations are okay or even necessary, but in most enterprise use-cases a trusted response is needed.\n",
    "\n",
    "HHEM (Hughes Hallucination Evaluation Model) is a model that was built specifically to help LLM practitioners measure hallucinations. It is available for use on [Huggingface Hub](https://huggingface.co/vectara/hallucination_evaluation_model), and a public [leaderboard](https://huggingface.co/spaces/vectara/leaderboard) shows the likelihood of various LLMs (both commercial and open source) to hallucinate.\n",
    "\n",
    "VHC (Vectara Hallucination Corrector) is the next step in the fight against hallucinations. It allows you to take the generated response and generate a corrected one.\n",
    "\n",
    "Let's demonstrate this via an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0193c1db-6f43-4c92-87d0-ffe4005cc176",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"what is the name of the king?\",\n",
    "        \"contexts\": [\"King Arthur took the sword out of the stone and waved it high.\"],\n",
    "        \"answer\": \"The name of the King is Johannes\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Where did the conference take place?\",\n",
    "        \"contexts\": [\n",
    "            \"The annual tech summit was held at the San Francisco Moscone Center this year.\",\n",
    "            \"Attendees flew in from across North America and Europe.\"\n",
    "        ],\n",
    "        \"answer\": \"It took place at the Berlin International Congress Center\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who painted the Mona Lisa?\",\n",
    "        \"contexts\": [\n",
    "            \"Leonardo da Vinci completed the Mona Lisa in the early 16th century.\",\n",
    "            \"The painting is housed in the Louvre Museum in Paris.\",\n",
    "            \"It is famed for the subject’s enigmatic smile.\",\n",
    "            \"Art historians credit da Vinci’s sfumato technique for its realism.\"\n",
    "        ],\n",
    "        \"answer\": \"It was painted by Michelangelo\"\n",
    "    },\n",
    "    {\n",
    "      \"query\": \"What is the capital city of Australia and how many states does it have?\",\n",
    "      \"contexts\": [\n",
    "        \"Australia is a federation comprising six states and two major mainland territories.\",\n",
    "        \"Its largest city by population is Sydney.\",\n",
    "        \"Its national parliament is seated in the Australian Capital Territory, in the city of Canberra.\",\n",
    "        \"The city was selected as a compromise between Sydney and Melbourne.\"\n",
    "      ],\n",
    "      \"answer\": \"The capital of Australia is Melbourne. Australia has 5 states.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the GDP for Spain in 2023?\",\n",
    "        \"contexts\": [\n",
    "            \"Japan's population was 125,124,989 in 2022 and 124,516,650 in 2023.\",\n",
    "            \"\"\"\n",
    "| Country | GDP 2022 (USD billion) | GDP 2023 (USD billion) |\n",
    "| ------- | ---------------------- | ---------------------- |\n",
    "| Japan   | 4,237.53               | 4,213.17               |\n",
    "| Germany | 4,085.68               | 4,500                  |\n",
    "| Spain   | 1,418.92               | 1,620.09               |\n",
    "\"\"\",\n",
    "            \"It's hotter in Spain than in Germany in the summer\",\n",
    "        ],\n",
    "        \"answer\": \"Spain's GDP in 2023 was $2.5B\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39827b80-a019-41e7-855e-1a5681c4f106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a8cd08-89cd-4198-8f16-6e4b933904a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv('VECTARA_API_KEY'):\n",
    "    raise EnvironmentError(\"VECTARA_API_KEY environment variable is not set.\")\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def call_vhc(query, answer, contexts):\n",
    "    \"\"\"Calls the Vectara Hallucination Corrector (VHC) endpoint synchronously.\"\"\"\n",
    "    payload = {\n",
    "        \"generated_text\": answer,\n",
    "        \"query\": query,\n",
    "        \"documents\": [{\"text\": c} for c in contexts],\n",
    "        \"model_name\": \"vhc-large-1.0\"\n",
    "    }\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"x-api-key\": os.getenv(\"VECTARA_API_KEY\")\n",
    "    }\n",
    "\n",
    "    # Perform the POST request\n",
    "    response = session.post(\n",
    "        \"https://api.vectara.io/v2/hallucination_correctors/correct_hallucinations\",\n",
    "        json=payload,\n",
    "        headers=headers,\n",
    "        timeout=10  # optional\n",
    "    )\n",
    "    # Raise exception for HTTP errors (4xx/5xx)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    data = response.json()\n",
    "    corrected_text = data.get(\"corrected_text\", \"\")\n",
    "    corrections = data.get(\"corrections\", [])\n",
    "\n",
    "    if not corrected_text.strip():\n",
    "        print(f\"VHC returned empty corrected_text for query={query}, retrying…\")\n",
    "        raise ValueError(\"VHC returned empty corrected_text, retrying…\")\n",
    "\n",
    "    return corrected_text, corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6612bdd-ec7e-44c7-8ade-c41803257a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The query is: what is the name of the king?\n",
      "The original response is: The name of the King is Johannes\n",
      "The corrected response is: The name of the King is Arthur.\n",
      "Corrections:\n",
      "{\n",
      "  \"original_text\": \"The name of the King is Johannes\",\n",
      "  \"corrected_text\": \"The name of the King is Arthur.\",\n",
      "  \"explanation\": \"The source states that the king is Arthur, but the response claims the king's name is Johannes, which directly contradicts the source.\"\n",
      "}\n",
      "\n",
      "\n",
      "The query is: Where did the conference take place?\n",
      "The original response is: It took place at the Berlin International Congress Center\n",
      "The corrected response is: It took place at the San Francisco Moscone Center.\n",
      "Corrections:\n",
      "{\n",
      "  \"original_text\": \"It took place at the Berlin International Congress Center\",\n",
      "  \"corrected_text\": \"It took place at the San Francisco Moscone Center.\",\n",
      "  \"explanation\": \"The response states the conference took place at the Berlin International Congress Center, but the source specifies it was held at the San Francisco Moscone Center. This is a direct contradiction.\"\n",
      "}\n",
      "\n",
      "\n",
      "The query is: Who painted the Mona Lisa?\n",
      "The original response is: It was painted by Michelangelo\n",
      "The corrected response is: It was painted by Leonardo da Vinci\n",
      "Corrections:\n",
      "{\n",
      "  \"original_text\": \"It was painted by Michelangelo\",\n",
      "  \"corrected_text\": \"It was painted by Leonardo da Vinci\",\n",
      "  \"explanation\": \"The response incorrectly attributes the painting of the Mona Lisa to Michelangelo, while the source clearly states it was completed by Leonardo da Vinci.\"\n",
      "}\n",
      "\n",
      "\n",
      "The query is: What is the capital city of Australia and how many states does it have?\n",
      "The original response is: The capital of Australia is Melbourne. Australia has 5 states.\n",
      "The corrected response is: The capital of Australia is Canberra. Australia has 6 states.\n",
      "Corrections:\n",
      "{\n",
      "  \"original_text\": \"The capital of Australia is Melbourne.\",\n",
      "  \"corrected_text\": \"The capital of Australia is Canberra.\",\n",
      "  \"explanation\": \"The source states that the national parliament is in Canberra, indicating that Canberra is the capital, not Melbourne.\"\n",
      "}\n",
      "{\n",
      "  \"original_text\": \"Australia has 5 states.\",\n",
      "  \"corrected_text\": \"Australia has 6 states.\",\n",
      "  \"explanation\": \"The source states that Australia has six states, not five.\"\n",
      "}\n",
      "\n",
      "\n",
      "The query is: What is the GDP for Spain in 2023?\n",
      "The original response is: Spain's GDP in 2023 was $2.5B\n",
      "The corrected response is: Spain's GDP in 2023 was $1,620.09 billion.\n",
      "Corrections:\n",
      "{\n",
      "  \"original_text\": \"Spain's GDP in 2023 was $2.5B\",\n",
      "  \"corrected_text\": \"Spain's GDP in 2023 was $1,620.09 billion.\",\n",
      "  \"explanation\": \"The response states Spain's GDP in 2023 was $2.5B, but the source table shows it was $1,620.09 billion. The response contradicts the source.\"\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ex in examples:\n",
    "    corrected, corrections = call_vhc(ex['query'], ex['answer'], ex['contexts'])\n",
    "    print(f\"The query is: {ex['query']}\")\n",
    "    print(f\"The original response is: {ex['answer']}\")\n",
    "    print(f\"The corrected response is: {corrected}\")\n",
    "    print(\"Corrections:\")\n",
    "\n",
    "    for c in corrections:\n",
    "        print(json.dumps(c, indent=2))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e24490d-cee6-4800-a0db-ca476e73d27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
