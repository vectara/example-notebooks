{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara and LlamaIndex\n",
    "\n",
    "In this notebook we are going to show how to use Vectara with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. An integrated API for processing input data, including text extraction from documents and ML-based chunking.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedding model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embeddings and retrieves the most relevant text segmentsthrough [hybrid search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and a variety of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking) strategies, including a [multilingual reranker](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker), [maximal marginal relevance (MMR) reranker](https://docs.vectara.com/docs/learn/mmr-reranker), [user-defined function reranker](https://docs.vectara.com/docs/learn/user-defined-function-reranker), and a [chain reranker](https://docs.vectara.com/docs/learn/chain-reranker) that provides a way to chain together multiple reranking methods to achieve better control over the reranking, combining the strengths of various reranking methods.\n",
    "\n",
    "4. An option to create a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview) with a wide selection of LLM summarizers (including Vectara's [Mockingbird](https://vectara.com/blog/mockingbird-is-a-rag-specific-llm-that-beats-gpt-4-gemini-1-5-pro-in-rag-output-quality/), trained specifically for RAG-based tasks), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits of using Vectara RAG-as-a-service to build your application are:\n",
    "* **Accuracy and Quality**: Vectara provides an end-to-end platform that focuses on eliminating hallucinations, reducing bias, and safeguarding copyright integrity.\n",
    "* **Security**: Vectara's platform provides acess control--protecting against prompt injection attacks--and meets SOC2 and HIPAA compliance.\n",
    "* **Explainability**: Vectara makes it easy to troubleshoot bad results by clearly explaining rephrased queries, LLM prompts, retrieved results, and agent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33079b",
   "metadata": {},
   "source": [
    "## About LlamaIndex\n",
    "\n",
    "LlamaIndex is a \"data framework\" to help you build LLM apps:\n",
    "\n",
    "1. It includes **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "2. It provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\n",
    "3. It provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "\n",
    "LlamaIndex's high-level API allows beginner users to use LlamaIndex to ingest and query their data in just a few lines of code, whereas its lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "Vectara is implemented in LlamaIndex as a [Managed Service](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices.html#vectara), abstracting all of Vectara's powerful API so they are easily integrated into LlamaIndex.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\n",
    "\n",
    "To get started with Vectara, [sign up](https://console.vectara.com/signup?utm_source=vectara&utm_medium=signup&utm_term=DevRel&utm_content=example-notebooks&utm_campaign=vectara-signup-DevRel-example-notebooks) (if you haven't already) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \n",
    "\n",
    "Once you have these, you can provide them as environment variables, which will be used by the LlamaIndex code later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U llama-index llama-index-indices-managed-vectara arxiv\n",
    "\n",
    "import os\n",
    "# os.environ['VECTARA_API_KEY'] = \"<YOUR_VECTARA_API_KEY>\"\n",
    "# os.environ['VECTARA_CORPUS_ID'] = \"<YOUR_VECTARA_CORPUS_ID>\"\n",
    "# os.environ['VECTARA_CUSTOMER_ID'] = \"<YOUR_VECTARA_CUSTOMER_ID>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Loading Data Into Vectara\n",
    "\n",
    "As mentioned above, Vectara is a RAG managed service, and in many cases data may be uploaded to the index ahead of time (e.g. by using [Airbyte](https://docs.airbyte.com/integrations/destinations/vectara), directly via Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) or using tools like [vectara-ingest](https://github.com/vectara/vectara-ingest)), but another easy way is via the VectaraIndex constructor: `from_documents()`.\n",
    "\n",
    "For this notebook, we will assume the Vectara corpus is empty and will load PDF documents from Arxiv, using Python's [arxiv](https://github.com/lukasschwab/arxiv.py) library. We will pull in data from the top papers related to \"climate change\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40947545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"(ti:embedding model) OR (ti:sentence embedding)\",\n",
    "  max_results = 100,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae7bd09-6569-48cc-8c54-8bf491c0e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2402.14776v2',\n",
       " 'http://arxiv.org/abs/2007.01852v2',\n",
       " 'http://arxiv.org/abs/1910.13291v1',\n",
       " 'http://arxiv.org/abs/2104.06719v1',\n",
       " 'http://arxiv.org/abs/1511.08198v3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.entry_id for p in papers][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "Next, download the Arxiv paper, and upload them into Vectara using the `add_file()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from llama_index.indices.managed.vectara import VectaraIndex\n",
    "\n",
    "data_folder = 'temp'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Create Vectara Index\n",
    "index = VectaraIndex()\n",
    "\n",
    "# Upload content for all papers\n",
    "for paper in papers:\n",
    "    try:\n",
    "        paper_fname = paper.download_pdf(data_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"File {paper_fname} failed to load with error {e}\")\n",
    "        continue\n",
    "    metadata = {\n",
    "        'url': paper.pdf_url,\n",
    "        'title': paper.title,\n",
    "        'author': str(paper.authors[0]),\n",
    "        'published': str(paper.published.date())\n",
    "    }\n",
    "    index.insert_file(file_path=paper_fname, metadata=metadata)\n",
    "\n",
    "shutil.rmtree(data_folder)\n",
    "del papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea571",
   "metadata": {},
   "source": [
    "Two important things to note here:\n",
    "1. Vectara processes each file uploaded on the backend, and performs appropriate chunking. So you don't need to apply any local processing, or choose a chunking strategy. \n",
    "2. We have used the fields `url`, `title`, `author`, and `published` as metadata fields (for simplicity, author is the first author if there are multiple). You will need to make sure those fields are defined in your Vectara corpus as [filterable metadata fields](https://docs.vectara.com/docs/learn/metadata-search-filtering/filter-overview) to ensure we can filter by them in query time.\n",
    "\n",
    "So that's it for upload. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Querying with the VectaraIndex\n",
    "We can now ask questions using the `VectaraIndex` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb174ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is sentence embedding?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, here is a summary that answers the query \"What is sentence embedding?\":\n",
      "\n",
      "Sentence embedding is a form of word or sentence representation that maps text data into vectors, which can be a set of real numbers (a vector) [1]. It is a way to represent words or sentences in a text that encodes the meaning of the word or the sentence in n-dimensional space [1]. The goal of sentence embedding is to make the embeddings of two sentences that are similar to get closer in this vector space [2]. This is achieved by training models such as SimCSE, BERT, or RoBERTa, which input a sentence and output a vector representation of it [3]. The vector representation captures the meaning of the sentence, and sentences that are similar in meaning will have vectors that are closer together in the vector space [1][2].\n",
      "\n",
      "Sources: [1], [2], [3]\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True, summary_num_results=5,\n",
    "    summary_response_lang=\"eng\",\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\"\n",
    ")\n",
    "res = query_engine.query(query)\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52878fd2",
   "metadata": {},
   "source": [
    "Note that the response here is fully generated by Vectara. There is no additional LLM involved (or API key you need to setup). The response also includes citations (marked in square brackets), which provide links to references used to generate this response by Vectara. \n",
    "<br>\n",
    "The `res` object includes the actual response to the user query, but also has the citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b110a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (1, 'http://arxiv.org/pdf/1910.13291v1'),\n",
       " (2, 'http://arxiv.org/pdf/2305.03010v1'),\n",
       " (3, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (5, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (6, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (7, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (8, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (9, 'http://arxiv.org/pdf/1904.05542v1')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(res.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cce7c7-c664-4001-8c84-f80c5d499796",
   "metadata": {},
   "source": [
    "## Using Streaming\n",
    "\n",
    "You can also stream the Vectara response simply by specifying `streaming=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd30027-d975-4440-a1c2-66d1bd30bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, here is a summary that answers the query \"What is sentence embedding?\":\n",
      "\n",
      "Sentence embedding is a form of word or sentence representation that maps text data into vectors, which can be a set of real numbers (a vector) [1]. It is a way to represent words or sentences in a text that encodes the meaning of the word or the sentence in n-dimensional space [1]. The goal of sentence embedding is to make the embeddings of two sentences that are similar to get closer in this vector space [2]. This is achieved by training models such as SimCSE, BERT, or RoBERTa, which input sentence representations and output sentence embeddings [3].\n",
      "\n",
      "In the context of natural language processing, sentence embedding is used to capture the meaning of a sentence and is used in tasks such as multiple choice question answering, next sentence prediction, and paraphrase identification [2]. It is also used in models such as LSTM, SDAE, and NMT, which use sentence embeddings as input representations [4].\n",
      "\n",
      "Overall, sentence embedding is a way to represent the meaning of a sentence in a machine-understandable format, which is essential for various natural language processing tasks.\n",
      "\n",
      "Sources: [1], [2], [3], [4]"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True,\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\",\n",
    "    streaming=True)\n",
    "\n",
    "res = query_engine.query(query)\n",
    "\n",
    "# print streamed output chunk by chunk\n",
    "for chunk in res.response_gen:\n",
    "    print(chunk.delta or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cd598-ce73-4c9e-9aa6-5d8604fd6c14",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "Vectara supports three types of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking):\n",
    "1. [Maximal Marginal Relevance](https://docs.vectara.com/docs/learn/mmr-reranker), or MMR, provides a reranking that can promote diversity in results at the cost of relevance.\n",
    "2. [Slingshot](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker) is a mulitilingual reranker that increases the accuracy of retrieved results across 100+ languages and is available to Vectara Scale customers.\n",
    "3. [User Defined Functions](https://docs.vectara.com/docs/learn/user-defined-function-reranker) allow you to create your own functions for reranking search results, unlocking better retrieval in a wide variety of use cases, such as sorting by recency or price of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49914a",
   "metadata": {},
   "source": [
    " Let's see an example of how to use MMR: We will run the same query but this time we will use MMR where `mmr_diversity_bias=0.3` provides a tradeoff between relevance and diversity (0.0 is full relevance, 1.0 is only diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72832e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a method of representing words or sentences in a text by encoding their meaning in n-dimensional space. It involves mapping text data into vectors of real numbers, where words or sentences closer in the vector space are more similar. Different types of embeddings exist, such as traditional, static, and contextualized word embeddings, as well as non-parameterized and parameterized models for sentence embeddings. This technique is crucial in preparing texts for machine understanding and various natural language processing tasks.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    reranker=\"mmr\",\n",
    "    rerank_k=50,\n",
    "    mmr_diversity_bias=0.3,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17020d34-5176-4910-bb88-5c5685513804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (1, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (2, 'http://arxiv.org/pdf/2305.15077v2'),\n",
       " (3, 'http://arxiv.org/pdf/2404.17606v1'),\n",
       " (4, 'http://arxiv.org/pdf/1910.03375v1')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e76fd1",
   "metadata": {},
   "source": [
    "As you can see, the results are now reranked in a way that provides more diversity instead of maximizing pure relevance. This in turn results in a different set of chunks used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562956f5-b9b7-4288-8100-f443a5dc3e4d",
   "metadata": {},
   "source": [
    "Now let's see an example with a user defined function. We may be interested in getting results that are the most semantically similar to our question, but we also want the most up-to-date information. Thus, we can bias our search results so that the papers that are not only semantically similar but also published more recently are used to answer our query. We can do this by using the available time functions (to see other built-in functions, see the UDF Reranker [documentation](https://docs.vectara.com/docs/learn/user-defined-function-reranker)).\n",
    "\n",
    "Vectara also supports chain-reranking, which provides a way to chain together multiple reranking methods to achieve better control over the reranking, and combining the strengths of various reranking methods. A great way to use the UDF reranker is in a chain: first the multilingual reranker, followed by the maximal marginal relevance (MMR) reranker, and then a user-defined function, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32ff2f1-ccd2-4fd0-99df-fd977182df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innovations in sentence embedding models include the development of Espresso Sentence Embeddings (ESE) supporting model depth and embedding size scaling [1]. Additionally, there is a shift towards generative models like PromptEOL, which enhance embeddings by introducing an Explicit One word Limitation (EOL) into prompts [2]. Furthermore, research is focusing on computationally efficient direct inference methods for sentence representation, bridging the gap in fine-tuning scenarios [3]. Lastly, advancements like Backward Dependency Enhanced Large Language Models (BeLLM) are exploring the effects of backward dependencies in LLMs for semantic similarity measurements [5].\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k = 50,\n",
    "    reranker=\"chain\",\n",
    "    rerank_chain=[\n",
    "        {\n",
    "            \"type\": \"slingshot\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"mmr\",\n",
    "            \"diversity_bias\": 0.3\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"udf\",\n",
    "            \"user_function\": \"max(0, 5 * get('$.score') - hours(seconds((to_unix_timestamp(now()) - to_unix_timestamp(datetime_parse(get('$.document_metadata.published'), 'yyyy-MM-dd'))))) / 24 / 365)\",\n",
    "            \"limit\": 5\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What innovations have been made to sentence embedding models?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01b96eb2-9e88-4c6a-90c4-676dddcc5ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '2024-02-22'),\n",
       " (1, '2024-04-05'),\n",
       " (2, '2024-04-05'),\n",
       " (3, '2023-11-16'),\n",
       " (4, '2023-11-09')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['published']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb7457-50e2-4b24-8416-daf65e9a4d5c",
   "metadata": {},
   "source": [
    "Notice how many of the papers used to generate the final summary were published recently, and they still give us information to generate a relevant response that answers our question.\n",
    "\n",
    "Also notice how we use a max() function with 0 in our user-defined expression. This is to ensure that all of our reranking scores are non-negative. Additionally, since we multiplied the original score by 10 and its value ranges from 0 to 1, we throw away any search results that are older than 10 years old for generating our final response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863b914",
   "metadata": {},
   "source": [
    "So far we've used Vectara's internal summarization capability, which is the best way for most users.\n",
    "\n",
    "You can still use Llama-Index's standard VectorStore `as_query_engine()` method, in which case Vectara's summarization won't be used, and you would be using an external LLM (e.g. OpenAI's GPT-4) and a custom prompt from LlamaIndex to generate the summary. For this option just set `summary_enabled=False`\n",
    "\n",
    "For this functionality, you will need to specify your own OpenAI API key in the environment:\n",
    "\n",
    "> `os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b0a49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a method used to represent sentences in a text by encoding their meaning into n-dimensional space. This technique involves mapping sentences into vectors of real numbers, where each vector represents the semantic content of a sentence. The proximity of these vectors in the vector space indicates the similarity between the meanings of the sentences. Sentence embeddings are utilized in various machine learning applications to handle and process text data effectively.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    summary_enabled=False,\n",
    "    llm=llm\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebad3cf-b09e-4af5-be3d-ba2a3e3e8d73",
   "metadata": {},
   "source": [
    "## Using Vectara Chat\n",
    "\n",
    "Vectara now fully supports Chat in its platform, where the chat history is maintained by Vectara and so you don't have to worry about keeping history and integrating it with your RAG pipeline. \n",
    "\n",
    "To use it, simply call `as_chat_engine()`.\n",
    "\n",
    "(Chat mode always uses Vectara's summarization so you don't have to explicitly specify `summary_enabled=True` like before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42af08f5-dc13-4991-96f4-fdd294b0ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e3368be-46a3-4b2f-9589-de3383c65201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a sentence embedding model?\n",
      "\n",
      "Response: A sentence embedding model is a framework that converts sentences into numerical vectors, capturing their semantic meanings. These models, such as Sentence-BERT, SimCSE-BERT, SimCSE-RoBERTa, Sentence-T5, and MP-Net, are used for tasks like information retrieval and semantic textual similarity benchmarks. They play a crucial role in various applications by representing sentences in a continuous vector space for efficient processing and analysis.\n",
      "\n",
      "Question: What are some known models?\n",
      "\n",
      "Response: Some known sentence embedding models include Sentence-BERT, SimCSE-BERT, SimCSE-RoBERTa, Sentence-T5, MP-Net, GenSen, and DSE. These models have been used for various tasks such as embedding inversion attacks and semantic textual similarity benchmarks. Additionally, there is ongoing research and development in this area, with a focus on both supervised and unsupervised models for constructing sentence embeddings.\n",
      "\n",
      "Question: How are they different than token embedding models\n",
      "\n",
      "Response: Sentence embedding models differ from token embedding models in that sentence embedding models focus on capturing the overall meaning and context of a sentence as a whole, while token embedding models concentrate on representing individual words or tokens within a sentence. Sentence embedding models excel at detecting meaning equivalence but may struggle with distinguishing fine-grained degrees of meaning overlap. On the other hand, token embedding models, such as those using token-embedding pooling techniques, show minimal correlation with models fine-tuned for sentence similarity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'What is a sentence embedding model?',\n",
    "    'What are some known models?',\n",
    "    'How are they different than token embedding models'\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Question: {q}\\n\")\n",
    "    response = ce.chat(q).response\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111b1fa-5efc-4ee6-99f0-21c73dbd5433",
   "metadata": {},
   "source": [
    "Of course streaming works as well with Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8b24622-bc73-4bee-929e-377f06335a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f16016-be7c-411f-aafd-b03d8816d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The individuals behind SBERT are Reimers and Gurevych, as they proposed the SBERT network structure in 2019. SBERT is based on transformer models like BERT and fine-tunes them using a siamese network structure [1]."
     ]
    }
   ],
   "source": [
    "response = ce.stream_chat(\"Who is behind SBERT?\")\n",
    "for chunk in response.chat_stream:\n",
    "    print(chunk.delta or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ad93e-0692-4ad5-99a1-329788252b03",
   "metadata": {},
   "source": [
    "# Advanced RAG with Vectara and LLamaIndex\n",
    "\n",
    "## Agentic RAG\n",
    "\n",
    "Vectara also has its own package, [vectara-agentic](https://github.com/vectara/py-vectara-agentic), built on top of many features from LlamaIndex to easily implement agentic RAG applications. It allows you to create your own AI assistant with RAG query tools and other custom tools, such as making API calls to retrieve information from financial websites. You can find the full documentation for vectara-agentic [here](https://vectara.github.io/vectara-agentic-docs/).\n",
    "\n",
    "Let's create a ReAct Agent with a single RAG tool using vectara-agentic (to create a ReAct agent, specify `VECTARA_AGENTIC_AGENT_TYPE` as `\"REACT\"` in your environment).\n",
    "\n",
    "Vectara does not yet have an LLM capable of acting as an agent for planning and tool use, so we will need to use another LLM as the driver of the agent resoning.\n",
    "\n",
    "In this demo, we are using OpenAI's GPT4o. Please make sure you have `OPENAI_API_KEY` defined in your environment or specify another LLM with the corresponding key (for the full list of supported LLMs, check out our [documentation](https://vectara.github.io/vectara-agentic-docs/introduction.html#try-it-yourself) for setting up your environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "648b415f-303a-4d0a-aef1-9d58aa6380ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U vectara-agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c77fd093-522d-435e-954e-f571a16a64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing vectara-agentic version 0.1.17...\n",
      "No observer set.\n",
      "> Running step 36031c69-ab3e-4952-8e6b-d07b4310a628. Step input: Tell me about the latest innovations in sentence embedding models.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: ask_embeddings\n",
      "Action Input: {'query': 'latest innovations in sentence embedding models'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: \n",
      "                    Response: '''The latest innovations in sentence embedding models include Sentence-BERT, SimCSE-BERT/SimCSE-RoBERTa, Sentence-T5, and MP-Net [1]. These models have been found to have isotropy issues, which can lead to alignment problems in areas such as domain adaptation, word embedding evaluation, and machine translation [2]. Additionally, there are concerns about the trustworthiness of reported results in the field of NLP, with some studies suggesting that neural language models have been misleadingly evaluated [5]. A novel sentence embedding model called Espresso Sentence Embeddings (ESE) has been proposed, which supports both model depth and embedding size scaling [4]. This model aims to address the limitations of existing sentence embedding models and provide a more effective and trustworthy approach.'''\n",
      "                    References:\n",
      "                    [1]: page='5'; section='75'; PTEX.Fullbanner='This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2'; CreationDate='D:20230505003815Z'; Keywords=''; Producer='pdfTeX-1.40.21'; Author=''; Title=''; Creator='LaTeX with hyperref'; ModDate='D:20230505003815Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2305.03010v1'; title='arXiv:2305.03010v1  [cs.CL]  4 May 2023'; author='Haoran Li'; published='2023-05-04'; framework='llama_index'.\n",
      "[2]: page='11'; title='Extracting Sentence Embeddings from Pretrained Transformer Models'; breadcrumb='[\"Related work\",\"Reshaping representation spaces\"]'; section='4'; PTEX.Fullbanner='This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'; CreationDate='D:20240816002943Z'; Keywords='BERT; embeddings; large language models; natural language processing; text embeddings; sentence vector representation; semantic similarity; transformer models; prompt engineering; unsupervised learning'; Producer='pdfTeX-1.40.25'; Author='Lukas Stankevicius and Mantas Lukosevicius'; Title='Extracting Sentence Embeddings from Pretrained Transformer Models'; Creator='LaTeX with hyperref'; ModDate='D:20240816002943Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2408.08073v1'; author='Lukas StankeviÄius'; published='2024-08-15'; framework='llama_index'.\n",
      "[4]: page='10'; title='Introduction'; section='14'; PTEX.Fullbanner='This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'; CreationDate='D:20240522002303Z'; Keywords=''; Producer='pdfTeX-1.40.25'; Author=''; Title=''; Creator='LaTeX with hyperref'; ModDate='D:20240522002303Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2402.14776v2'; author='Xianming Li'; published='2024-02-22'; framework='llama_index'.\n",
      "[5]: page='1'; section='6'; PTEX.Fullbanner='This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2'; CreationDate='D:20190605010437Z'; Keywords=''; Producer='pdfTeX-1.40.17'; Author=''; Title=''; Creator='LaTeX with hyperref package'; ModDate='D:20190605010437Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/1906.01575v1'; title='arXiv:1906.01575v1  [cs.CL]  4 Jun 2019'; author='Steffen Eger'; published='2019-06-04'; framework='llama_index'.\n",
      "\n",
      "                \n",
      "\u001b[0m> Running step 93455a6a-c4ea-46c0-9098-a1f5a6d4831d. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: Recent innovations in sentence embedding models include advancements such as Sentence-BERT, SimCSE-BERT/SimCSE-RoBERTa, Sentence-T5, and MP-Net. These models have been noted for having isotropy issues, which can cause alignment problems in various applications like domain adaptation and machine translation. Additionally, there are concerns about the reliability of reported results in NLP, with some studies indicating that neural language models might have been misleadingly evaluated. A new model called Espresso Sentence Embeddings (ESE) has been introduced, which supports scaling in both model depth and embedding size, aiming to address the limitations of existing models and provide a more effective and trustworthy approach. \n",
      "\n",
      "For more detailed information, you can refer to the following sources: [arXiv:2305.03010v1](http://arxiv.org/pdf/2305.03010v1), [arXiv:2408.08073v1](http://arxiv.org/pdf/2408.08073v1), and [arXiv:2402.14776v2](http://arxiv.org/pdf/2402.14776v2).\n",
      "\u001b[0mTime taken: 12.116064071655273\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Recent innovations in sentence embedding models include advancements such as Sentence-BERT, SimCSE-BERT/SimCSE-RoBERTa, Sentence-T5, and MP-Net. These models have been noted for having isotropy issues, which can cause alignment problems in various applications like domain adaptation and machine translation. Additionally, there are concerns about the reliability of reported results in NLP, with some studies indicating that neural language models might have been misleadingly evaluated. A new model called Espresso Sentence Embeddings (ESE) has been introduced, which supports scaling in both model depth and embedding size, aiming to address the limitations of existing models and provide a more effective and trustworthy approach. \n",
       "\n",
       "For more detailed information, you can refer to the following sources: [arXiv:2305.03010v1](http://arxiv.org/pdf/2305.03010v1), [arXiv:2408.08073v1](http://arxiv.org/pdf/2408.08073v1), and [arXiv:2402.14776v2](http://arxiv.org/pdf/2402.14776v2)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vectara_agentic.agent import Agent\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "agent = Agent.from_corpus(\n",
    "    data_description=\"sentence embeddings\",\n",
    "    assistant_specialty=\"sentence embeddings research\",\n",
    "    tool_name=\"ask_embeddings\",\n",
    "    vectara_summary_num_results=5,\n",
    "    vectara_summarizer=\"mockingbird-1.0-2024-07-16\",\n",
    "    vectara_reranker=\"mmr\",\n",
    "    vectara_rerank_k=50,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "response = agent.chat(\n",
    "    \"Tell me about the latest innovations in sentence embedding models.\"\n",
    ")\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c98981",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we've seen various examples for using Vectara with LlamaIndex, which provides the following benefits:\n",
    "* Vectara provides a complete RAG pipeline, so you don't have to deal with a lot of the details around data ingestion: pre-processing, chunking, embedding, etc. Instead all these steps are handled automatically and efficiently in Vectara. \n",
    "* Being a platform, Vectara uses its own internal Embedding model (Boomerang), its own vector storage, and its own LLM (Mockingbird) for summarization, so you don't have to maintain separate API keys and relationships with additional vendors or install other products.\n",
    "* Vectara is built for large scale GenAI applications, and with the vectara-agentic package, you can easily build and test advanced RAG applications at an enteprise scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
