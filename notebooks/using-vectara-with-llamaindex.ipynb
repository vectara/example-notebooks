{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara and LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted GenAI and semantic search platform that provides an easy-to-use API for document indexing and querying. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. A way to extract text from document files and chunk them into sentences.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedder model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embedding, and retrieves the most relevant text segments (including support for [Hybrid Search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and [MMR](https://vectara.com/get-diverse-results-and-comprehensive-summaries-with-vectaras-mmr-reranker/))\n",
    "\n",
    "4. An option to create [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits for using Vectara for a RAG application are:\n",
    "* **Easy to use**: Vectara takes care of much detail required for a fully functional, highly scalable and robust RAG application, so as a user you don't have to code up these pieces and maintain them over time\n",
    "* **Scalable and Secure**: building GenAI applications may seem easy at first, and Vectara provides instant scalablility to millions of documents, while maintaing data security and privacy, as well as latency SLAs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33079b",
   "metadata": {},
   "source": [
    "## About Llama Index\n",
    "\n",
    "LlamaIndex is a \"data framework\" to help you build LLM apps:\n",
    "\n",
    "1. It includes **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "2. It provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\n",
    "3. It provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "\n",
    "LlamaIndex's high-level API allows beginner users to use LlamaIndex to ingest and query their data in just a few lines of code, whereas its lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "Vectara is implemented in LlamaIndex as a [Managed Service](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices.html#vectara), abstracting all of Vectara's powerful API so they are easily integrated into LlamaIndex.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\n",
    "\n",
    "To get started with Vectara, [sign up](https://vectara.com/integrations/llamaindex) (if you haven't already) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \n",
    "\n",
    "Once you have these, you can provide them as environment variables, which will be used by the LlamaIndex code later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U llama-index llama-index-indices-managed-vectara arxiv\n",
    "\n",
    "import os\n",
    "os.environ['VECTARA_API_KEY'] = \"<YOUR_VECTARA_API_KEY>\"\n",
    "os.environ['VECTARA_CORPUS_ID'] = \"<YOUR_VECTARA_CORPUS_ID>\"\n",
    "os.environ['VECTARA_CUSTOMER_ID'] = \"<YOUR_VECTARA_CUSTOMER_ID>\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Loading Data Into Vectara\n",
    "\n",
    "As mentioned above, Vectara is a RAG managed service, and in many cases data may be uploaded to the index ahead of time (e.g. by using [Airbyte](https://docs.airbyte.com/integrations/destinations/vectara), directly via Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) or using tools like [vectara-ingest](https://github.com/vectara/vectara-ingest)), but another easy way is via the VectaraIndex constructor: `from_documents()`.\n",
    "\n",
    "For this notebook we will assume the Vectara corpus is empty and will load PDF documents from Arxiv, using Python's [arxiv](https://github.com/lukasschwab/arxiv.py) library. We would pull in data from the top papers related to \"climate change\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40947545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"(ti:embedding model) OR (ti:sentence embedding)\",\n",
    "  max_results = 100,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae7bd09-6569-48cc-8c54-8bf491c0e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2402.14776v1',\n",
       " 'http://arxiv.org/abs/2007.01852v2',\n",
       " 'http://arxiv.org/abs/1910.13291v1',\n",
       " 'http://arxiv.org/abs/2104.06719v1',\n",
       " 'http://arxiv.org/abs/1511.08198v3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.entry_id for p in papers][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "Next, download the Arxiv paper, and upload them into Vectara using the `add_file()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from llama_index.indices.managed.vectara import VectaraIndex\n",
    "\n",
    "data_folder = 'temp'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Create Vectara Index\n",
    "index = VectaraIndex()\n",
    "\n",
    "# Upload content ofr all papers\n",
    "for paper in papers:\n",
    "    try:\n",
    "        paper_fname = paper.download_pdf(data_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"File {paper_fname} failed to load with error {e}\")\n",
    "        continue\n",
    "    metadata = {\n",
    "        'url': paper.entry_id,\n",
    "        'title': paper.title,\n",
    "        'author': str(paper.authors[0]),\n",
    "        'published': str(paper.published.date())\n",
    "    }        \n",
    "    index.insert_file(file_path=paper_fname, metadata=metadata)\n",
    "\n",
    "shutil.rmtree(data_folder)\n",
    "del papers, index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea571",
   "metadata": {},
   "source": [
    "Two important things to note here:\n",
    "1. Vectara processes each file uploaded on the backend, and performs appropriate chunking. So you don't need to apply any local processing, or choose a chunking strategy. \n",
    "2. We have used the fields `url`, `title`, `author`, and `published` as metadata fields (where author is the first author if there are many, just to simplify). You will need to make sure those fields are defined in your Vectara corpus as [filterable metadata fields](https://docs.vectara.com/docs/learn/metadata-search-filtering/filter-overview) to ensure we can filter by them in query time.\n",
    "\n",
    "So that's it for upload. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Querying with the VectaraIndex\n",
    "We can now ask questions using the VectaraIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb174ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n",
      "Embeddings have been explicitly disabled. Using MockEmbedding.\n"
     ]
    }
   ],
   "source": [
    "index = VectaraIndex()\n",
    "query = \"What is sentence embedding?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a technique used in Natural Language Processing (NLP) that involves encoding sentences into fixed-length numerical vectors for various downstream tasks. This process typically includes an encoder, such as an LSTM encoder, to convert the input sentence into a meaningful representation [1]. The encoded sentence vectors can then be used for tasks like classification and inference, where a softmax classifier may be employed to make predictions based on the sentence embeddings [2]. Additionally, sentence embedding models may utilize both German and English encoders to generate embeddings for multilingual applications [3]. The goal of sentence embedding is to capture the semantic meaning of sentences in a continuous vector space, enabling efficient processing and analysis of textual data for a wide range of NLP tasks.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=5)\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52878fd2",
   "metadata": {},
   "source": [
    "Note that the response here is fully generated by Vectara. There is no additional LLM involved (or API key you need to setup). The response also includes citations (marked in square brackets), which provide links to references used to generate this response by Vectara. \n",
    "<br>\n",
    "When we use `print(response)` it simply prints the response text. But the `response` object also has the citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b110a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/abs/2305.03010v1'),\n",
       " (1, 'http://arxiv.org/abs/1904.05542v1'),\n",
       " (2, 'http://arxiv.org/abs/1904.05542v1'),\n",
       " (3, 'http://arxiv.org/abs/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/abs/2305.15077v2')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx,n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49914a",
   "metadata": {},
   "source": [
    "Vectara supports [max-marginal-relevance](https://docs.vectara.com/docs/api-reference/search-apis/reranking#maximal-marginal-relevance-mmr-reranker) natively in the backend, and this is available as a query mode. \n",
    "\n",
    "Let's see an example of how to use MMR: We will run the same query but this time we will use MMR where mmr_diversity_bias=0.3 provides a tradeoff between relevance and diversity (0.0 is full relevance, 1.0 is only diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72832e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a crucial text processing technique in NLP, with various models proposed for tasks like author profiling, sentiment classification, and textual entailment [1]. Different methods like ELMo, BERT, and SBERT-WK have been used to compute sentence embeddings by leveraging deep contextualized word models and fusion techniques [3, 5]. These models aim to preserve the original sentence meanings effectively in the embedded vectors, enhancing performance in document classification and sentiment analysis tasks [3]. Additionally, exploring analogical relationships in sentence embedding spaces can reveal regularities and unique properties for semantic matching capabilities [6, 7]. The Relational Sentence Embedding (RSE) method introduces relation modeling to leverage multi-source relational data for improved generalizability in supervised sentence embedding learning [7].\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    vectara_query_mode=\"mmr\",\n",
    "    mmr_k=50,\n",
    "    mmr_diversity_bias=0.3,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17020d34-5176-4910-bb88-5c5685513804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/abs/1703.03130v1'),\n",
       " (1, 'http://arxiv.org/abs/2305.15077v2'),\n",
       " (2, 'http://arxiv.org/abs/1808.05505v3'),\n",
       " (3, 'http://arxiv.org/abs/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/abs/2002.09620v2')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx,n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e76fd1",
   "metadata": {},
   "source": [
    "As you can see, the results are now reranked in a way that provides more diversity instead of maximizing pure relevance. This in turn results in a different set of chunks used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863b914",
   "metadata": {},
   "source": [
    "So far we've used Vectara's internal summarization capability, which is the best way for most users.\n",
    "\n",
    "You can still use Llama-Index's standard VectorStore `as_query_engine()` method, in which case Vectara's summarization won't be used, and you would be using an external LLM (like OpenAI's GPT-4 or similar) and a custom prompt from LlamaIndex to generate the summary. For this option just set `summary_enabled=False`\n",
    "\n",
    "For this you would need to specify your own OpenAI API key in the environment:\n",
    "\n",
    "> `os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa28d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm=OpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b0a49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding refers to the process of converting a sentence into a fixed-length numerical vector representation that captures the meaning and context of the sentence.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    summary_enabled=False,\n",
    "    llm=llm\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43963c4",
   "metadata": {},
   "source": [
    "## Using Auto Retriever with Vectara\n",
    "\n",
    "LlamaIndex's auto-retriever functionality is really cool. \n",
    "It is most useful when you have metadata fields (like in our case of papers from Arxiv), and would like a query that references a metadata field to be automatically interpreted in the right way.\n",
    "\n",
    "For example, if I ask \"what is a paper about climate change risks published after 2020\", the auto-retriever would (behind the scences) interpret ths into a query \"what is a paper about climate change risks\" along with a filter condition of \"published > 2020\"\n",
    "\n",
    "Let's see how this works with the Vectara Index.\n",
    "First, we have to define a `VectorStoreInfo` structure that defines the meta data fields the auto-retriever knows about to do its job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24cc7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"information about a paper\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"published\",\n",
    "            description=\"The date the paper was published\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"author\",\n",
    "            description=\"The author of the paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"title\",\n",
    "            description=\"The title of the papers\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"url\",\n",
    "            description=\"The URL for this paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23712d46",
   "metadata": {},
   "source": [
    "Auto-retrieval is implemented before calling Vectara as a query transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f0333",
   "metadata": {},
   "source": [
    "Now we can define the `VectaraAutoRetriever`, which can perform auto-retrieval using Vectara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92de30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding?\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('2017-03-09',\n",
       "  'Instead of using a vector, we use a 2-D matrix\\nto represent the embedding, with each row of the matrix attending on a different\\npart of the sentence. We also propose a self-attention mechanism and a special\\nregularization term for the model. As a side effect, the embedding comes with an\\neasy way of visualizing what speciï¬c parts of the sentence are encoded into the\\nembedding. We evaluate our model on 3 different tasks: author proï¬ling, senti-\\nment classiï¬cation and textual entailment. Results show that our model yields a\\nsigniï¬cant performance gain compared to other sentence embedding methods in\\nall of the 3 tasks.'),\n",
       " ('2018-08-16',\n",
       "  'This\\nproblem can be alleviated by obtaining more of para-\\nphrase sentence pairs. Conclusion Sentence embedding is one of the most important text\\nprocessing techniques in NLP. To date,  various sen-\\ntence embedding models have been proposed and have\\nyielded good performances in document classification\\nand sentiment analysis tasks. However, the fundamen-\\ntal ability of sentence embedding methods, i.e., how\\neffectively the meanings of the original sentences are\\npreserved  in  the  embedded  vectors,  cannot  be  fully\\nevaluated through such indirect methods.'),\n",
       " ('2018-08-16',\n",
       "  'Paraphrase Thought:  Sentence Embedding Module Imitating\\n                        Human Language Recognition Myeongjun Jang 1 Abstract\\nSentence embedding is an important research\\ntopic in natural language processing. It is es-\\nsential to generate a good embedding vector\\nthat  fully  reflects  the  semantic  meaning  of\\na sentence in order to achieve an enhanced\\nperformance  for  various  natural  language\\nprocessing  tasks,   such  as  machine  trans-\\nlation  and  document  classification. Thus\\nfar, various sentence embedding models have\\nbeen proposed, and their feasibility has been\\ndemonstrated through good performances on\\ntasks following embedding, such as sentiment\\nanalysis  and  sentence  classification.'),\n",
       " ('2018-08-16',\n",
       "  'Pilsung Kang 1 Introduction Sentence embedding, which transforms sentences into\\nlow-dimensional  vector  values  reflecting  their  mean-\\nings,  is a highly important task in natural language\\nprocessing  (NLP). By  mapping  unstructured  text\\ndata into a certain form of structured representation,\\nthe embedding vector can enhance the performances\\nof  various  NLP  tasks,  such  as  machine  translation\\n(Artetxe et al., 2017; Lee et al., 2016; Zhao & Zhang,\\n2016), document classification (Conneau et al., 2017b;\\nZhou et al., 2016), and sentence matching (Wan et al.,\\n2016). As sentence embedding plays an import role\\nin NLP, various methods (Kiros et al., 2015; Pagliar-\\ndini et al., 2017; Hill et al., 2016; Arora et al., 2017;\\nConneau  et  al.,  2017a;  Chen,  2017)  have  been  pro-\\nposed since the advent of the Doc2vec method (Le &\\nMikolov, 2014).'),\n",
       " ('2016-06-15',\n",
       "  '2    Siamese CBOW\\n\\nWe present the Siamese Continuous Bag of Words\\n(CBOW)  model,  a  neural  network  for  efï¬cient\\nestimation of high-quality sentence embeddings. Quality should manifest itself in embeddings of\\nsemantically close sentences being similar to one\\nanother, and embeddings of semantically different\\nsentences being dissimilar. An efï¬cient and sur-\\nprisingly successful way of computing a sentence\\nembedding is to average the embeddings of its\\nconstituent words. Recent work uses pre-trained\\nword embeddings (such as word2vec and GloVe)\\nfor this task, which are not optimized for sentence\\nrepresentations. Following these approaches, we\\ncompute sentence embeddings by averaging word\\nembeddings, but we optimize word embeddings\\ndirectly for the purpose of being averaged.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "retriever = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "res = retriever.retrieve(\"What is sentence embedding, based on papers before 2019?\")\n",
    "[(r.metadata['published'], r.text) for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7450bc",
   "metadata": {},
   "source": [
    "As you can see, the Auto Retriever was able to translate the natural language text into a shorter query and a proper condition (in this case `doc.published < 2019`).\n",
    "\n",
    "We can also of course ask a question directly: we use the `VectaraQueryEngine` which can work with the `VectaraAutoRetriever` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "327ffbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding?\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n",
      "Sentence embedding is a crucial technique in natural language processing, aiming to represent the semantic meaning of sentences in a low-dimensional vector form. Various models have been proposed to generate embedding vectors that capture the essence of sentences, enhancing performance in tasks like machine translation, document classification, sentiment analysis, and more [4]. These models focus on preserving the original sentence meanings effectively in the embedded vectors, leading to significant performance gains compared to other methods [2]. By transforming sentences into structured representations, sentence embedding can improve the outcomes of NLP tasks, such as document classification and machine translation [4]. The Siamese Continuous Bag of Words (CBOW) model is one approach that efficiently estimates high-quality sentence embeddings by averaging the embeddings of constituent words, optimizing word embeddings directly for this purpose [5].\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara.query import VectaraQueryEngine\n",
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "\n",
    "ar = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=llm,\n",
    "    summary_enabled=True,\n",
    "    summary_num_results=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query_engine = VectaraQueryEngine(retriever=ar)\n",
    "response = query_engine.query(\"What is sentence embedding, based on papers before 2019?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5faef-9fe9-4acc-a55d-ad7c379697fd",
   "metadata": {},
   "source": [
    "## Advanced querying with QueryFusionRetriever\n",
    "\n",
    "The QueryFusion [Retriever](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion.html#reciprocal-rerank-fusion-retriever) is an advanced query mechanism whereby the original query is pre-processed to generate N variations. Each of these rephrased queries is then run against the Vectara engine and rank-fusion is used to combine the best results. \n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c662bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT is a dual encoder. It uses a dual-encoder architecture.\n"
     ]
    }
   ],
   "source": [
    "query = \"is SBERT a dual encoder? what type of DL architecture does it use?\"\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    summary_enabled=False,\n",
    "    llm=llm,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896e8b7e-c028-4ec6-bb7c-0c05dfb86321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What is SBERT and how does it differ from other encoder models?\n",
      "2. Comparison of SBERT with other dual encoder models in natural language processing.\n",
      "3. Deep dive into the architecture of SBERT and its implementation in deep learning.\n",
      "4. Exploring the benefits and drawbacks of using SBERT as a dual encoder in machine learning tasks.\n",
      "SBERT is a dual encoder model. It uses a siamese network structure, which is a type of deep learning architecture.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import nest_asyncio\n",
    "\n",
    "rf_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(similarity_top_k=2)],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=5,  # this includes the origianl query; set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()     # apply nested async to run in a notebook\n",
    "query_engine = RetrieverQueryEngine.from_args(rf_retriever)\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6b3fd",
   "metadata": {},
   "source": [
    "We can see how the QueryFusionRetriever created additional query variations (they are displayed since we used `verbose=True`) and then the overall response includes the results fused together. This is very helpful in this case because the QueryFusionRetriever creates sub-questions that inquire about the specific architecture of SBERT which is relevant context to answering this question properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e185e43c",
   "metadata": {},
   "source": [
    "## The Vectara-RAG LlamaPack\n",
    "\n",
    "[Llama Packs](https://docs.llamaindex.ai/en/stable/community/llama_packs/root.html) are a community-driven hub of prepackaged modules for LlamaIndex.\n",
    "\n",
    "Vectara's integration with LlamaIndex provides Vectara RAG, a Llama Pack with ready-to-go RAG.\n",
    "\n",
    "Try it out for yourself by following these [instructions](https://github.com/run-llama/llama-hub/tree/main/llama_hub/llama_packs/vectara_rag)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c98981",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we've seen various examples for using Vectara with LlamaIndex, which provides the following benefits:\n",
    "* Vectara provides a complete RAG pipeline, so you don't have to deal with a lot of the details around data ingestion: pre-processing, chunking, embedding, etc. Instead all these steps are handled automatically and efficiently in Vectara. \n",
    "* Being a platform, Vectara uses its own internal Embedding model (Boomerang), its own vector storage, and calls the LLM for summarization, so you don't have to maintain separate API keys and relationships with additional vendors or install other products.\n",
    "* Vectara is built for large scale GenAI applications, and with the tools provided by LlamaIndex like Auto Retrieval and Query Fusion, you can easily build and test advanced RAG applications at enteprise scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c950620-9618-4d2c-9597-58ec5adae393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
