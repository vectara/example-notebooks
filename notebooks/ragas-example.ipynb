{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a26c52ce-2cf8-409e-9ac6-096e97a30473",
   "metadata": {},
   "source": [
    "# Evaluating Vectara With RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7253c225-89c3-4eae-bbf2-47bccb0c6cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# On Mac:\n",
    "# brew install libmagic\n",
    "#\n",
    "# Additional dependecies\n",
    "# pip install ragas pandas python-magic datasets langchain langchainhub langchain-experimental\n",
    "# pip install -U ragas (version 0.1.5 or higher)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ef5c516-76f8-4274-aa0f-745072cbac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import requests\n",
    "import magic\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    answer_similarity\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "from vectaraClient import VectaraClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b314fa2-282b-4ef1-a27d-b281161c914c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = \"<YOUR-VECTARA-CUSTOMER-ID>\"\n",
    "corpus_id = \"<YOUR-VECTARA-CORPUS-ID>\"\n",
    "api_key = \"<YOUR-VECTARA-API-KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1540476b-6302-44dd-96f1-d9082fa73683",
   "metadata": {},
   "source": [
    "## Part 1: Generate Synthetic Evaluation Dataset from a Vectara Corpus\n",
    "\n",
    "For RAGAs to generate synthetic data, we will utilize OpenAI embedding and Chat. Make sure to have your OPENAI_API_KEY available in the environment. First we use the VectaraClient to download URLs of each document in our corpus (`url` in this case is a metadata field) and use those to download the actual content to a local `temp` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57979bda-bff0-4a5a-a033-db55c978b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = VectaraClient(api_key, customer_id, corpus_id)\n",
    "\n",
    "urls = vc.get_all_doc_urls()\n",
    "print(f\"Found {len(urls)} documents in corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ea44f-04ce-43f8-b0b0-e85430b62e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_correct_ext(file_path):\n",
    "    mime = magic.Magic(mime=True)\n",
    "    mime_type = mime.from_file(file_path)\n",
    "    \n",
    "    # Dictionary mapping some MIME types to file extensions\n",
    "    extension_mapping = {\n",
    "        'application/pdf': '.pdf',\n",
    "        'text/html': '.html',\n",
    "        'text/plain': '.txt',\n",
    "        'text/markdown': '.md',\n",
    "        'application/vnd.ms-powerpoint': '.ppt',\n",
    "        'application/msword': '.doc',\n",
    "    }\n",
    "    \n",
    "    extension = extension_mapping.get(mime_type, '')    \n",
    "    if extension:\n",
    "        new_file_path = f\"{file_path}{extension}\"\n",
    "        os.rename(file_path, new_file_path)\n",
    "        return new_file_path\n",
    "    else:\n",
    "        print(f\"Unsupported file type or no extension mapping found: mime_type={mime_type}.\")\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d3653-dabd-48a5-8d5a-54f72b0b96ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './temp'\n",
    "if os.path.exists(data_dir):\n",
    "    shutil.rmtree(data_dir)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "for url in urls:\n",
    "    file_name = url.split('/')[-1]\n",
    "    if len(file_name)==0:\n",
    "        file_name = 'root'\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "        \n",
    "    # Check if the download was successful\n",
    "    if response.status_code == 200:\n",
    "        # Write the content to a file in the specified directory\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        file_path = add_correct_ext(file_path)\n",
    "    else:\n",
    "        print(f\"Failed to download the file (url={url}). Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ecb174-2e9a-4400-9138-06fbc9d5a662",
   "metadata": {},
   "source": [
    "Now that all files are available locally, we load them as LangChain documents and use the RAGAs `TestsetGenerator` functionality to generate 50 synthetic question/answer pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a1b18-6573-4d40-a2ce-5d2b15de72e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_questions = 50\n",
    "\n",
    "loader = DirectoryLoader(data_dir, use_multithreading=True, silent_errors=True)\n",
    "documents = loader.load()\n",
    "for document in documents:\n",
    "    document.metadata['file_name'] = document.metadata['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5146858-3050-428e-abd3-595f4779d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "critic_llm = ChatOpenAI(model=\"gpt-4-turbo-preview\")\n",
    "emb = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(generator_llm=gen_llm, \n",
    "                                            critic_llm=critic_llm,\n",
    "                                            embeddings=emb)\n",
    "testset = generator.generate_with_langchain_docs(documents, test_size=n_questions, \n",
    "                                                 raise_exceptions=False, with_debugging_logs=False, is_async=False,\n",
    "                                                 distributions={simple: 0.5, reasoning: 0.2, multi_context: 0.3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d29c61-b959-4366-8db8-dca55c4331e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_df = testset.to_pandas()\n",
    "testset_df = testset_df[pd.isnull(testset_df.ground_truth)==False]\n",
    "testset_df = testset_df[['question', 'ground_truth', 'contexts']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a0ce877-cd12-4d91-b380-f0a69a1ecd44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are some examples of use cases for the Vectara platform?</td>\n",
       "      <td>The Vectara platform has a unique ability to understand and process information, using hybrid search to find the most relevant products, support cases, and documents that answer user's questions first. It can power chatbots, Q&amp;A systems, conversational applications, and websites based on relevant information. Vectara also provides result recommendations and enables global collaboration through cross-language search.</td>\n",
       "      <td>['\\n\\nUse Case Exploration\\n\\nExplore the Vectara Use Cases\\n\\nThe AI era has changed interactions between people and information\\ndramatically. Users expect relevant answers to questions in a natural\\nlanguage, and they expect the best results with the right context.\\nUsing Vectara gives you relevant results no matter how you ask. Our\\nconversational search platform generates summarized responses that speak your\\nlanguage. Better results enable better outcomes that reduce support costs and\\nimprove the customer experience.\\n\\nWhy Vectara? Get Answers and Better Outcomes\\u200b\\n\\nThe Vectara platform has a unique ability to understand and process\\ninformation. Our platform uses hybrid search to find the most relevant\\nproducts, support cases, and documents that answer your userâ€™s questions first.\\nPower chatbots, Q&amp;A systems, conversational applications and websites that base\\ntheir information on what you and your users care about â€“ information grounded\\nin facts. Vectara also provides result recommendations and enables global\\ncollaboration through its cross-language search.\\n\\nVectara GenAI Use Cases\\u200b\\n\\nThis versatile Vectara GenAI platform caters to a wide range of use\\ncases to drive better outcomes and unlock new possibilities in search\\napplications. Vectara provides an easy entry point to generative AI\\ncapabilities while protecting company IP and customer data. The data\\nis secure. Vectara does not train on user data and respects data\\nsovereignty and provides you with peace of mind.\\n\\nChoose the Data for Ingestion\\u200b\\n\\nYou might be wondering what kind of data to select for ingestion. Our Vectara Quick Start Tutorial\\nprovides an example that gets you set up and searching for answers quickly!\\n\\nHere are some other ideas to let you see Vectara in action:\\n\\nEmployee handbook\\n\\nProduct manuals\\n\\nLegal contracts\\n\\nResearch papers\\n\\nTraining materials\\n\\nFinancial reports\\n\\nGovernment regulations.\\n\\nThese types of documents contain very nuanced information where semantic search\\nreally shines! Think about what information takes a long time for a user to\\nlocate manually in a large volume. Unless you know exact keywords and section\\ntitles, you might struggle to find the exact information you need for understanding\\na real-estate contract, complex machine repair, conducting scientific\\nresearch, and so on.\\n\\nðŸ’¡ Check out example Vectara applications, demos, and tutorials to explore the many capabilities of our platform.\\n\\nConversational AI\\u200b\\n\\nUse Vectara to leverage the power of intelligent chatbots that provide\\nan interactive user experience. Enable your users with self-service as they engage\\nin human-like interactions, providing context in queries, and receiving\\nintelligent answers because the system understands them. This technology\\nsits behind virtual assistants, chatbots, and messaging applications to\\nhelp businesses automate customer service and streamline operations.\\n\\nVectara enables you to empower users with real-time feedback to avoid\\nescalations and build a digital chat agent that can deflect support\\ncalls. Making conversational AI easier to use increases both customer\\nsatisfaction and engagement.\\n\\nQuestion and Answering\\u200b\\n\\nVectara understands the context of a question and provides accurate, relevant\\nresponses. The Vectara advantage lets users ask complex questions to get\\nprecise answers that save your team valuable time and resources.\\n\\nEnable your users to ask a question and get the precise answers quickly. Embed\\nyour FAQs, customer support interactions, product manuals, inform knowledge\\nworkers on data, and enhance your website search. Vectara empowers your\\norganization to create a dynamic, responsive, and continuous improving Question\\nand Answer system that enhances the user experience and provides context-aware answers.\\n\\nResearch and Analysis\\u200b\\n\\nVectara sifts through volumes of publications, news articles, financial reports,\\nscientific and medical research, corporate documents and more and provides\\nsummarized answers to guide decision-making in your domain. Collaborate with\\nresearchers to streamline the peer review process by investigating topics and\\nquestions in these vast volumes of data to identify key insights.\\n\\nUsing Vectara is like having a global research assistant that\\ncan read and understand large volumes of documents in an instant. Let the\\nplatform speed up your research process, find the most relevant information,\\nand become a recommendation system for your domain.\\n\\nVectara can help transform data into insights which help make decision-making\\neasier. This platform can provide hidden insights and patterns from your data,\\nhelping you make informed decisions. Not only can it answer your questions,\\nbut also provides citations grounded in facts from the raw data.\\n\\nSemantic App Search\\u200b\\n\\nVectara lets you embed powerful hybrid search into your applications without\\nbeing an LLM expert. You provide data and queries through']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can developers customize prompts with metadata using Vectara's Custom Retrieval Augmented Generation (RAG) Prompt Engine?</td>\n",
       "      <td>Vectara empowers developers with a flexible way of customizing prompts with metadata through the Custom Retrieval Augmented Generation (RAG) Prompt Engine. Developers can use available prompt variables and functions to customize prompts based on their needs.</td>\n",
       "      <td>['\\n\\nGenerative Prompts\\n\\nCustom Prompts with Metadata\\n\\nCustom Prompts with Metadata\\n\\nVectara handles the system and user prompts automatically, but if you want to\\ndo it yourself, Vectara now empowers developers with a flexible way of\\ncustomizing prompts with metadata. Our Custom Retrieval Augmented Generation\\n(RAG) Prompt Engine provides several available prompt variables and functions\\nfor Scale users to customize prompts.\\n\\nAvailable Prompt Variables\\u200b\\n\\nThe following table shows the available custom prompt variables:\\n\\n$vectaraOutChars Number of characters See below $vectaraLangCode ISO639 v3 code for the passed language code See below $vectaraQuery The query provided by the user Generate a summary in $vectaraOutChars characters in language \\'${vectaraLangCode}\\' for the query \\\\\"$esc.java(${vectaraQuery})\\\\\" solely based on the search results in this chat. Generate a summary in 512 characters in language \\'ara\\' for the query \\\\\"Give me \\\\\"some\\\\\" search results.\\\\\" solely based on the search results in this chat. $vectaraIdxWord A utility array to convert the index to words i.e \"first\", \"second\", \"third\", \"forth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\" $vectaraIdxWord[0] first $vectaraLangName Set to the requested language name. The language can either be requested explicitly or detected from the language of the query. You are a helpful assistant. Answer in ${vectaraLangName}. You are a helpful assistant. Answer in Arabic. $vectaraQueryResults An array of query results is found in the response, sorted by relevance score. #foreach ($qResult in $vectaraQueryResults)    {\"role\": \"user\", \"content\": \"Give me the $vectaraIdxWord[$foreach.index] search result.\"},    {\"role\": \"assistant\", \"content\": \"$esc.java(${qResult.text()})\" },#end {\"role\": \"user\", \"content\": \"Give me the second search result.\"},{\"role\": \"assistant\", \"content\": \"2nd result\" },\\n\\nAvailable Prompt Functions\\u200b\\n\\nThe following table shows the available custom prompt functions:\\n\\n$esc.java(...) A utility method to escape special charts, has methods such as \"esc.java\", \"esc.url\", \"esc.xml\", \"esc.html\" See below #foreach ($qResult in $vectaraQueryResults) $qResult.getText() or $qResult.text() Returns text of the query result $qResult.text() Result text $qResult.docMetadata() Returns the metadata of the document this result belongs to $qResult.docMetadata() {\"title\": \"documentTitle\", ...} $qResult.docMetadata().present() Returns true/false if there are any values present in the metadata #if ($qResult.docMetadata().present())...#end $qResult.docMetadata().get(\"title\") Returns the specified field value from doc metadata, an incorrect key would result in an empty value $qResult.docMetadata().get(\"title\") documentTitle $qResult.partMetadata().present() Returns true/false if there are any values present in the metadata #if ($qResult.partMetadata().present())...#end $qResult.partMetadata() Returns the metadata of the part of the document this result belongs to $qResult.partMetadata() {\"page\": \"1\", ...} $qResult.partMetadata().get(\"page\") Returns the specified field value from part metadata, incorrect key would result in empty value $qResult.docMetadata().get(\"page\") \"1\"\\n\\nInclude Metadata in Prompt\\u200b\\n\\nThis snippet shows how to get metadata associated with a single result qResult\\nby retrieving metadata docMetadata from the date that information was\\nanswered answerDate. It then extracts the text content of qResult.\\n\\n\"role\"\\n\\n\"assistant\"\\n\\n\"content\"\\n\\n\"qResult.docMetadata().get(\\'answerDate\\') $esc.java(${qResult.getText()})\"\\n\\nLet\\'s dive into a full custom prompt example that shows more details about a\\ncustom prompt with\\nmetadata.\\n\\nExample Custom Prompt for an RFI Answering Bot\\u200b\\n\\nThe following example prompt creates a Request for information (RFI)\\nanswering bot that includes metadata. First, we ask the generative LLM to\\nanswer an RFI question and tell it how the results will come back from the\\nquery.\\n']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                        question  \\\n",
       "0                                                                  What are some examples of use cases for the Vectara platform?   \n",
       "1  How can developers customize prompts with metadata using Vectara's Custom Retrieval Augmented Generation (RAG) Prompt Engine?   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                          ground_truth  \\\n",
       "0  The Vectara platform has a unique ability to understand and process information, using hybrid search to find the most relevant products, support cases, and documents that answer user's questions first. It can power chatbots, Q&A systems, conversational applications, and websites based on relevant information. Vectara also provides result recommendations and enables global collaboration through cross-language search.   \n",
       "1                                                                                                                                                                   Vectara empowers developers with a flexible way of customizing prompts with metadata through the Custom Retrieval Augmented Generation (RAG) Prompt Engine. Developers can use available prompt variables and functions to customize prompts based on their needs.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     contexts  \n",
       "0  ['\\n\\nUse Case Exploration\\n\\nExplore the Vectara Use Cases\\n\\nThe AI era has changed interactions between people and information\\ndramatically. Users expect relevant answers to questions in a natural\\nlanguage, and they expect the best results with the right context.\\nUsing Vectara gives you relevant results no matter how you ask. Our\\nconversational search platform generates summarized responses that speak your\\nlanguage. Better results enable better outcomes that reduce support costs and\\nimprove the customer experience.\\n\\nWhy Vectara? Get Answers and Better Outcomes\\u200b\\n\\nThe Vectara platform has a unique ability to understand and process\\ninformation. Our platform uses hybrid search to find the most relevant\\nproducts, support cases, and documents that answer your userâ€™s questions first.\\nPower chatbots, Q&A systems, conversational applications and websites that base\\ntheir information on what you and your users care about â€“ information grounded\\nin facts. Vectara also provides result recommendations and enables global\\ncollaboration through its cross-language search.\\n\\nVectara GenAI Use Cases\\u200b\\n\\nThis versatile Vectara GenAI platform caters to a wide range of use\\ncases to drive better outcomes and unlock new possibilities in search\\napplications. Vectara provides an easy entry point to generative AI\\ncapabilities while protecting company IP and customer data. The data\\nis secure. Vectara does not train on user data and respects data\\nsovereignty and provides you with peace of mind.\\n\\nChoose the Data for Ingestion\\u200b\\n\\nYou might be wondering what kind of data to select for ingestion. Our Vectara Quick Start Tutorial\\nprovides an example that gets you set up and searching for answers quickly!\\n\\nHere are some other ideas to let you see Vectara in action:\\n\\nEmployee handbook\\n\\nProduct manuals\\n\\nLegal contracts\\n\\nResearch papers\\n\\nTraining materials\\n\\nFinancial reports\\n\\nGovernment regulations.\\n\\nThese types of documents contain very nuanced information where semantic search\\nreally shines! Think about what information takes a long time for a user to\\nlocate manually in a large volume. Unless you know exact keywords and section\\ntitles, you might struggle to find the exact information you need for understanding\\na real-estate contract, complex machine repair, conducting scientific\\nresearch, and so on.\\n\\nðŸ’¡ Check out example Vectara applications, demos, and tutorials to explore the many capabilities of our platform.\\n\\nConversational AI\\u200b\\n\\nUse Vectara to leverage the power of intelligent chatbots that provide\\nan interactive user experience. Enable your users with self-service as they engage\\nin human-like interactions, providing context in queries, and receiving\\nintelligent answers because the system understands them. This technology\\nsits behind virtual assistants, chatbots, and messaging applications to\\nhelp businesses automate customer service and streamline operations.\\n\\nVectara enables you to empower users with real-time feedback to avoid\\nescalations and build a digital chat agent that can deflect support\\ncalls. Making conversational AI easier to use increases both customer\\nsatisfaction and engagement.\\n\\nQuestion and Answering\\u200b\\n\\nVectara understands the context of a question and provides accurate, relevant\\nresponses. The Vectara advantage lets users ask complex questions to get\\nprecise answers that save your team valuable time and resources.\\n\\nEnable your users to ask a question and get the precise answers quickly. Embed\\nyour FAQs, customer support interactions, product manuals, inform knowledge\\nworkers on data, and enhance your website search. Vectara empowers your\\norganization to create a dynamic, responsive, and continuous improving Question\\nand Answer system that enhances the user experience and provides context-aware answers.\\n\\nResearch and Analysis\\u200b\\n\\nVectara sifts through volumes of publications, news articles, financial reports,\\nscientific and medical research, corporate documents and more and provides\\nsummarized answers to guide decision-making in your domain. Collaborate with\\nresearchers to streamline the peer review process by investigating topics and\\nquestions in these vast volumes of data to identify key insights.\\n\\nUsing Vectara is like having a global research assistant that\\ncan read and understand large volumes of documents in an instant. Let the\\nplatform speed up your research process, find the most relevant information,\\nand become a recommendation system for your domain.\\n\\nVectara can help transform data into insights which help make decision-making\\neasier. This platform can provide hidden insights and patterns from your data,\\nhelping you make informed decisions. Not only can it answer your questions,\\nbut also provides citations grounded in facts from the raw data.\\n\\nSemantic App Search\\u200b\\n\\nVectara lets you embed powerful hybrid search into your applications without\\nbeing an LLM expert. You provide data and queries through']  \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            ['\\n\\nGenerative Prompts\\n\\nCustom Prompts with Metadata\\n\\nCustom Prompts with Metadata\\n\\nVectara handles the system and user prompts automatically, but if you want to\\ndo it yourself, Vectara now empowers developers with a flexible way of\\ncustomizing prompts with metadata. Our Custom Retrieval Augmented Generation\\n(RAG) Prompt Engine provides several available prompt variables and functions\\nfor Scale users to customize prompts.\\n\\nAvailable Prompt Variables\\u200b\\n\\nThe following table shows the available custom prompt variables:\\n\\n$vectaraOutChars Number of characters See below $vectaraLangCode ISO639 v3 code for the passed language code See below $vectaraQuery The query provided by the user Generate a summary in $vectaraOutChars characters in language \\'${vectaraLangCode}\\' for the query \\\\\"$esc.java(${vectaraQuery})\\\\\" solely based on the search results in this chat. Generate a summary in 512 characters in language \\'ara\\' for the query \\\\\"Give me \\\\\"some\\\\\" search results.\\\\\" solely based on the search results in this chat. $vectaraIdxWord A utility array to convert the index to words i.e \"first\", \"second\", \"third\", \"forth\", \"fifth\", \"sixth\", \"seventh\", \"eighth\", \"ninth\", \"tenth\" $vectaraIdxWord[0] first $vectaraLangName Set to the requested language name. The language can either be requested explicitly or detected from the language of the query. You are a helpful assistant. Answer in ${vectaraLangName}. You are a helpful assistant. Answer in Arabic. $vectaraQueryResults An array of query results is found in the response, sorted by relevance score. #foreach ($qResult in $vectaraQueryResults)    {\"role\": \"user\", \"content\": \"Give me the $vectaraIdxWord[$foreach.index] search result.\"},    {\"role\": \"assistant\", \"content\": \"$esc.java(${qResult.text()})\" },#end {\"role\": \"user\", \"content\": \"Give me the second search result.\"},{\"role\": \"assistant\", \"content\": \"2nd result\" },\\n\\nAvailable Prompt Functions\\u200b\\n\\nThe following table shows the available custom prompt functions:\\n\\n$esc.java(...) A utility method to escape special charts, has methods such as \"esc.java\", \"esc.url\", \"esc.xml\", \"esc.html\" See below #foreach ($qResult in $vectaraQueryResults) $qResult.getText() or $qResult.text() Returns text of the query result $qResult.text() Result text $qResult.docMetadata() Returns the metadata of the document this result belongs to $qResult.docMetadata() {\"title\": \"documentTitle\", ...} $qResult.docMetadata().present() Returns true/false if there are any values present in the metadata #if ($qResult.docMetadata().present())...#end $qResult.docMetadata().get(\"title\") Returns the specified field value from doc metadata, an incorrect key would result in an empty value $qResult.docMetadata().get(\"title\") documentTitle $qResult.partMetadata().present() Returns true/false if there are any values present in the metadata #if ($qResult.partMetadata().present())...#end $qResult.partMetadata() Returns the metadata of the part of the document this result belongs to $qResult.partMetadata() {\"page\": \"1\", ...} $qResult.partMetadata().get(\"page\") Returns the specified field value from part metadata, incorrect key would result in empty value $qResult.docMetadata().get(\"page\") \"1\"\\n\\nInclude Metadata in Prompt\\u200b\\n\\nThis snippet shows how to get metadata associated with a single result qResult\\nby retrieving metadata docMetadata from the date that information was\\nanswered answerDate. It then extracts the text content of qResult.\\n\\n\"role\"\\n\\n\"assistant\"\\n\\n\"content\"\\n\\n\"qResult.docMetadata().get(\\'answerDate\\') $esc.java(${qResult.getText()})\"\\n\\nLet\\'s dive into a full custom prompt example that shows more details about a\\ncustom prompt with\\nmetadata.\\n\\nExample Custom Prompt for an RFI Answering Bot\\u200b\\n\\nThe following example prompt creates a Request for information (RFI)\\nanswering bot that includes metadata. First, we ask the generative LLM to\\nanswer an RFI question and tell it how the results will come back from the\\nquery.\\n']  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9a479f-d096-4ec1-88f5-b9fd467f7c21",
   "metadata": {},
   "source": [
    "## Evaluate your Vectara RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bb6c73-cb80-45a6-8881-4f4f9d21d869",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_top_k = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5fe7975-a821-406e-972c-8a96af9e43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(row):\n",
    "    query = row['question']\n",
    "    response, contexts = vc.query(query, cfg)\n",
    "    response = re.sub(\"\\[\\d+(,\\s*\\d+)*\\]\", \"\", str(response)).replace(' .', '.')\n",
    "    return pd.Series([response, contexts], index=['answer', 'contexts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29db7773-e25a-4c17-9b47-720294a7e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we also add a calculation of HHEM - Vectara's Hallucination Detection Model\n",
    "# HHEM is a factual consistency score, similar to faithfulness, \n",
    "# ...but computed directly as a model instead of using LLM-as-a-judge as in RAGAs\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "def calc_hhem(contexts, answer):\n",
    "    model = CrossEncoder('vectara/hallucination_evaluation_model')\n",
    "    scores = model.predict([[context, answer] for context in contexts])\n",
    "    return np.mean(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a624c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eval_rag(df):\n",
    "    df2 = df.copy()\n",
    "    df2[['answer', 'contexts']] = df2.apply(get_response, axis=1)\n",
    "    result = evaluate(\n",
    "        Dataset.from_pandas(df2),\n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            answer_similarity,\n",
    "            answer_correctness\n",
    "        ],\n",
    "        llm = ChatOpenAI(model_name=\"gpt-4-turbo-preview\", temperature=0),\n",
    "        raise_exceptions=False\n",
    "    )    \n",
    "\n",
    "    result['hhem_score'] = np.mean(np.vectorize(calc_hhem)(df2['contexts'], df2['answer']))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f799d719-64c6-4da3-b4f7-609f1c1dcb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b80b097bdee4072b039bc7f1a4e8def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9507, 'answer_relevancy': 0.9453, 'answer_similarity': 0.9456, 'answer_correctness': 0.5154, 'hhem_score': 0.7285}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "        'lambda': 0.0,\n",
    "        'max_summary_result': sum_top_k,\n",
    "        'mmr': True,\n",
    "        'prompt_name': 'vectara-experimental-summary-ext-2023-10-23-small'\n",
    "    }\n",
    "\n",
    "latency = []\n",
    "res = eval_rag(testset_df)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0163e31e-c98e-4d91-b460-140deba64e56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ac122a24f9406599c1f7e89a399072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9561, 'answer_relevancy': 0.9395, 'answer_similarity': 0.9407, 'answer_correctness': 0.5260, 'hhem_score': 0.7426}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "        'lambda': 0.0,\n",
    "        'max_summary_result': sum_top_k,\n",
    "        'mmr': False,\n",
    "        'prompt_name': 'vectara-experimental-summary-ext-2023-10-23-small'\n",
    "    }\n",
    "\n",
    "latency = []\n",
    "res = eval_rag(testset_df)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "643c3cb4-c6db-472d-92d1-eff3acc16aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2101b486dc4e463dbefd4dbee61679f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9409, 'answer_relevancy': 0.9490, 'answer_similarity': 0.9490, 'answer_correctness': 0.5580, 'hhem_score': 0.7113}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "        'lambda': 0.025,\n",
    "        'max_summary_result': sum_top_k,\n",
    "        'mmr': False,\n",
    "        'prompt_name': 'vectara-experimental-summary-ext-2023-10-23-small'\n",
    "    }\n",
    "\n",
    "latency = []\n",
    "res = eval_rag(testset_df)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baba6d83-dba7-4f88-8b11-d804aa7155f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9548d9c708d643cf979493ec7e8116c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'faithfulness': 0.9896, 'answer_relevancy': 0.9391, 'answer_similarity': 0.9530, 'answer_correctness': 0.5953, 'hhem_score': 0.6177}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = {\n",
    "        'lambda': 0.025,\n",
    "        'max_summary_result': sum_top_k,\n",
    "        'mmr': False,\n",
    "        'prompt_name': 'vectara-experimental-summary-ext-2023-12-11-large'\n",
    "    }\n",
    "\n",
    "latency = []\n",
    "res = eval_rag(testset_df)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4608f8f-dd49-4b1c-bfed-8920fdf04098",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
