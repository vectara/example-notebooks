{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/api-examples/5-sub-agents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Vectara Sub-Agents: Building Modular AI Workflows\n",
    "\n",
    "This notebook demonstrates how to use Vectara's **sub-agents** capability to build modular, specialized AI workflows. Sub-agents allow a parent agent to delegate tasks to specialized child agents, enabling:\n",
    "\n",
    "- **Context isolation**: Each sub-agent maintains its own conversation history\n",
    "- **Specialized configuration**: Each sub-agent can have distinct instructions and tools\n",
    "- **Reusability**: Build once, invoke from any parent agent\n",
    "- **Parallel execution**: Run multiple sub-agents simultaneously\n",
    "- **Better performance**: Smaller, focused agents make fewer mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the Agent Operating System for trusted enterprise AI: a unified Agentic RAG platform with built-in multi-modal retrieval, orchestration, and always-on governance. Deploy it on-prem (air-gapped), in your VPC, or as SaaS.\n",
    "\n",
    "Vectara provides a complete API-first platform for building production RAG and agentic applications:\n",
    "\n",
    "- **Simple Integration**: RESTful APIs and SDKs (Python, JavaScript) for quick integration into any stack\n",
    "- **Flexible Deployment**: Choose SaaS, VPC, or on-premises deployment based on your security requirements\n",
    "- **Multi-Modal Support**: Index and search across text, tables, and images from PDFs, documents, and structured data\n",
    "- **Advanced Retrieval**: Hybrid search combining semantic and keyword matching with state-of-the-art reranking\n",
    "- **Grounded Generation**: LLM responses with citations and factual consistency scores to reduce hallucinations\n",
    "- **Enterprise-Ready**: Built-in access controls, audit logging, and compliance (SOC2, HIPAA) from day one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "This notebook assumes you've completed Notebooks 1-4:\n",
    "- Notebook 1: Created two corpora (ai-research-papers and vectara-docs)\n",
    "- Notebook 2: Ingested AI research papers and Vectara documentation\n",
    "- Notebook 3: Queried the data with various techniques\n",
    "- Notebook 4: Created agents that can search and reason across data\n",
    "\n",
    "Now we'll create a multi-agent system where specialized sub-agents handle domain-specific tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Why Sub-Agents?\n",
    "\n",
    "When agents face complex, multi-step tasks, they often run into context window limits or need specialized capabilities. Consider a comprehensive research assistant that needs to:\n",
    "\n",
    "1. Analyze academic papers for theoretical foundations\n",
    "2. Search product documentation for implementation details\n",
    "3. Synthesize findings into actionable recommendations\n",
    "\n",
    "A single monolithic agent trying to handle all of this might:\n",
    "- Become confused between different instruction sets\n",
    "- Consume excessive context with domain-specific guidelines\n",
    "- Produce lower quality results due to competing priorities\n",
    "\n",
    "**Sub-agents solve this by delegation**: the parent agent orchestrates, while specialized sub-agents focus on their domains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Corpus: tutorial-ai-research-papers\n",
      "Docs Corpus: tutorial-vectara-docs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Get credentials from environment variables\n",
    "api_key = os.environ['VECTARA_API_KEY']\n",
    "\n",
    "# Corpus keys from previous notebooks\n",
    "research_corpus_key = 'tutorial-ai-research-papers'\n",
    "docs_corpus_key = 'tutorial-vectara-docs'\n",
    "\n",
    "# Base API URL\n",
    "BASE_URL = \"https://api.vectara.io/v2\"\n",
    "\n",
    "# Common headers\n",
    "headers = {\n",
    "    \"x-api-key\": api_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "print(f\"Research Corpus: {research_corpus_key}\")\n",
    "print(f\"Docs Corpus: {docs_corpus_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## Step 1: Create Specialized Sub-Agents\n",
    "\n",
    "We'll create two specialized agents that will serve as sub-agents:\n",
    "\n",
    "1. **Research Paper Analyst**: Expert at analyzing academic papers on RAG, embeddings, and retrieval\n",
    "2. **Documentation Expert**: Expert at finding implementation guidance from Vectara docs\n",
    "\n",
    "Each agent has focused instructions and tools optimized for its domain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "### Sub-Agent 1: Research Paper Analyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to delete and create agent\n",
    "def delete_and_create_agent(agent_config, agent_name):\n",
    "    \"\"\"Delete agent if it exists, then create a new one.\"\"\"\n",
    "    # Check if agent already exists and delete it\n",
    "    list_response = requests.get(f\"{BASE_URL}/agents\", headers=headers)\n",
    "\n",
    "    if list_response.status_code == 200:\n",
    "        agents = list_response.json().get('agents', [])\n",
    "        for agent in agents:\n",
    "            if agent.get('name') == agent_name:\n",
    "                existing_key = agent['key']\n",
    "                print(f\"Deleting existing agent '{agent_name}' ({existing_key})\")\n",
    "                delete_response = requests.delete(f\"{BASE_URL}/agents/{existing_key}\", headers=headers)\n",
    "                if delete_response.status_code == 204:\n",
    "                    print(f\"Deleted agent: {existing_key}\")\n",
    "                else:\n",
    "                    print(f\"Error deleting {existing_key}: {delete_response.text}\")\n",
    "                break\n",
    "\n",
    "    # Create new agent\n",
    "    response = requests.post(f\"{BASE_URL}/agents\", headers=headers, json=agent_config)\n",
    "\n",
    "    if response.status_code == 201:\n",
    "        agent_data = response.json()\n",
    "        print(f\"Created agent '{agent_name}'\")\n",
    "        print(f\"Agent Key: {agent_data['key']}\")\n",
    "        return agent_data['key']\n",
    "    else:\n",
    "        print(f\"Error creating agent: {response.status_code}\")\n",
    "        print(f\"{response.text}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent 'Research Paper Analyst'\n",
      "Agent Key: agt_research_paper_analyst_7916\n"
     ]
    }
   ],
   "source": [
    "# Create Research Paper Analyst sub-agent\n",
    "reranker_config = {\n",
    "    \"type\": \"chain\",\n",
    "    \"rerankers\": [\n",
    "        {\n",
    "            \"type\": \"customer_reranker\",\n",
    "            \"reranker_id\": \"rnk_272725719\", \n",
    "            \"limit\": 25,\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"mmr\",\n",
    "            \"diversity_bias\": 0.05\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "generation_config = {\n",
    "    \"generation_preset_name\": \"vectara-summary-table-md-query-ext-jan-2025-gpt-4o\",\n",
    "    \"max_used_search_results\": 10,\n",
    "    \"model_parameters\": {\n",
    "        \"llm_name\": \"gpt-4o\",\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "research_analyst_config = {\n",
    "    \"name\": \"Research Paper Analyst\",\n",
    "    \"description\": \"Specialized agent for analyzing academic research papers on RAG, embeddings, and retrieval techniques\",\n",
    "    \"model\": {\"name\": \"gpt-4o\"},\n",
    "    \"first_step\": {\n",
    "        \"type\": \"conversational\",\n",
    "        \"instructions\": [\n",
    "            {\n",
    "                \"type\": \"inline\",\n",
    "                \"name\": \"research_analyst_instructions\",\n",
    "                \"template\": \"\"\"You are an expert academic research analyst specializing in AI, machine learning, and natural language processing.\n",
    "\n",
    "Your expertise includes:\n",
    "- Retrieval Augmented Generation (RAG) architectures\n",
    "- Dense and sparse retrieval methods\n",
    "- Embedding models and vector representations\n",
    "- Transformer architectures and attention mechanisms\n",
    "- Information retrieval benchmarks and evaluation metrics\n",
    "\n",
    "When analyzing research papers:\n",
    "1. Identify the key contributions and novel techniques\n",
    "2. Explain technical concepts clearly with examples\n",
    "3. Highlight practical implications and limitations\n",
    "4. Compare with related work when relevant\n",
    "5. Provide citations to the source papers\n",
    "\n",
    "Always use tools to retrieve relevant content to answer user queries.\n",
    "Always ground your response in the retrieved content. \n",
    "If you cannot answer the user question from the retrieved content from tools, just say \"I don't know\"\n",
    "\n",
    "IMPORTANT: When responding, provide a complete, self-contained summary that includes all relevant findings.\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        \"output_parser\": {\"type\": \"default\"}\n",
    "    },\n",
    "    \"tool_configurations\": {\n",
    "        \"research_search\": {\n",
    "            \"type\": \"corpora_search\",\n",
    "            \"query_configuration\": {\n",
    "                \"search\": {\n",
    "                    \"corpora\": [{\"corpus_key\": research_corpus_key}],\n",
    "                    \"limit\": 100,\n",
    "                    \"context_configuration\": {\n",
    "                        \"sentences_before\": 2,\n",
    "                        \"sentences_after\": 2\n",
    "                    },\n",
    "                    \"reranker\": reranker_config,                   \n",
    "                },\n",
    "#                \"generation\": generation_config,\n",
    "                \"save_history\": True,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "research_analyst_key = delete_and_create_agent(research_analyst_config, \"Research Paper Analyst\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "### Sub-Agent 2: Documentation Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent 'Documentation Expert'\n",
      "Agent Key: agt_documentation_expert_2b20\n"
     ]
    }
   ],
   "source": [
    "# Create Documentation Expert sub-agent\n",
    "\n",
    "docs_expert_config = {\n",
    "    \"name\": \"Documentation Expert\",\n",
    "    \"description\": \"Specialized agent for finding implementation guidance and best practices from Vectara documentation\",\n",
    "    \"model\": {\"name\": \"gpt-4o\"},\n",
    "    \"first_step\": {\n",
    "        \"type\": \"conversational\",\n",
    "        \"instructions\": [\n",
    "            {\n",
    "                \"type\": \"inline\",\n",
    "                \"name\": \"docs_expert_instructions\",\n",
    "                \"template\": \"\"\"You are a Vectara platform expert who helps developers implement AI solutions.\n",
    "\n",
    "Your expertise includes:\n",
    "- Vectara API integration (indexing, querying, agents)\n",
    "- Corpus management and configuration\n",
    "- Search optimization (hybrid search, reranking, filters)\n",
    "- RAG implementation best practices\n",
    "- SDK usage and code examples\n",
    "\n",
    "When providing guidance:\n",
    "1. Give specific, actionable implementation steps using the API.\n",
    "2. Include relevant API endpoints and parameters\n",
    "3. Your examples should show how to use the API, not using Vectara SDK.\n",
    "4. Highlight configuration options and trade-offs\n",
    "5. Point to relevant documentation sections\n",
    "\n",
    "Always use tools to retrieve relevant content to answer user queries.\n",
    "Always ground your response in the retrieved content. \n",
    "If you cannot answer the user question from the retrieved content from tools, just say \"I don't know\"\n",
    "\n",
    "When responding, provide a complete, self-contained answer with all implementation details.\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        \"output_parser\": {\"type\": \"default\"}\n",
    "    },\n",
    "    \"tool_configurations\": {\n",
    "        \"docs_search\": {\n",
    "            \"type\": \"corpora_search\",\n",
    "            \"query_configuration\": {\n",
    "                \"search\": {\n",
    "                    \"corpora\": [{\"corpus_key\": docs_corpus_key}],\n",
    "                    \"limit\": 100,\n",
    "                    \"context_configuration\": {\n",
    "                        \"sentences_before\": 2,\n",
    "                        \"sentences_after\": 2\n",
    "                    },\n",
    "                    \"reranker\": reranker_config,                   \n",
    "                },\n",
    "#                \"generation\": generation_config,\n",
    "                \"save_history\": True,\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "docs_expert_key = delete_and_create_agent(docs_expert_config, \"Documentation Expert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 2: Create the Parent Orchestrator Agent\n",
    "\n",
    "Now we'll create a parent agent that can delegate to both sub-agents. The parent agent:\n",
    "- Analyzes user requests to determine which sub-agent(s) to invoke\n",
    "- Delegates domain-specific tasks to the appropriate sub-agent\n",
    "- Synthesizes responses from multiple sub-agents into a cohesive answer\n",
    "\n",
    "### Sub-Agent Tool Configuration\n",
    "\n",
    "Sub-agents are configured as tools using the `sub_agent` type:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"type\": \"sub_agent\",\n",
    "  \"description_template\": \"Description the LLM sees when deciding to use this tool\",\n",
    "  \"sub_agent_configuration\": {\n",
    "    \"agent_key\": \"the_sub_agent_key\",\n",
    "    \"session_mode\": \"llm_controlled\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### Session Modes\n",
    "\n",
    "When specifying in the parent agent how sub-agents are to be used, you need to define the \"session mode\":\n",
    "- **`llm_controlled`** (default): The LLM decides whether to resume an existing session or create a new one\n",
    "- **`persistent`**: Always reuse the same session, accumulating knowledge across invocations\n",
    "- **`ephemeral`**: Create a fresh session every time, ensuring no state leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created agent 'AI Research Orchestrator'\n",
      "Agent Key: agt_ai_research_orchestrator_b41b\n"
     ]
    }
   ],
   "source": [
    "# Create the Orchestrator Agent with sub-agent tools\n",
    "orchestrator_config = {\n",
    "    \"name\": \"AI Research Orchestrator\",\n",
    "    \"description\": \"Orchestrator agent that delegates to specialized sub-agents for comprehensive AI research assistance\",\n",
    "    \"model\": {\"name\": \"gpt-4o\"},\n",
    "    \"first_step\": {\n",
    "        \"type\": \"conversational\",\n",
    "        \"instructions\": [\n",
    "            {\n",
    "                \"type\": \"inline\",\n",
    "                \"name\": \"orchestrator_instructions\",\n",
    "                \"template\": \"\"\"You are an AI research orchestrator that helps users understand and implement AI technologies.\n",
    "\n",
    "Your role is to:\n",
    "1. Analyze the user's question to determine what expertise is needed.\n",
    "2. Use appropriate sub-agent(s) to get information needed to answer the user query.\n",
    "3. Synthesize sub-agent responses into a comprehensive answer.\n",
    "4. Bridge theory and practice when both are relevant.\n",
    "\n",
    "When synthesizing, clearly indicate which insights come from research vs documentation.\"\"\"\n",
    "            }\n",
    "        ],\n",
    "        \"output_parser\": {\"type\": \"default\"}\n",
    "    },\n",
    "    \"tool_configurations\": {\n",
    "        \"research_analyst\": {\n",
    "            \"type\": \"sub_agent\",\n",
    "            \"description_template\": \"Delegate academic research analysis tasks to a specialized research paper analyst. Use for: theoretical foundations, algorithm explanations, research paper analysis, academic citations, and comparisons between research approaches.\",\n",
    "            \"sub_agent_configuration\": {\n",
    "                \"agent_key\": research_analyst_key,\n",
    "                \"session_mode\": \"ephemeral\"       # Fresh context for each analysis\n",
    "            }\n",
    "        },\n",
    "        \"docs_expert\": {\n",
    "            \"type\": \"sub_agent\",\n",
    "            \"description_template\": \"Delegate implementation and documentation questions to a Vectara documentation expert. Use for: API usage, code examples, configuration guidance, best practices, and troubleshooting.\",\n",
    "            \"sub_agent_configuration\": {\n",
    "                \"agent_key\": docs_expert_key,\n",
    "                \"session_mode\": \"ephemeral\"     #\"llm_controlled\"  # LLM decides based on context\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "orchestrator_key = delete_and_create_agent(orchestrator_config, \"AI Research Orchestrator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## Step 3: Test the Multi-Agent Workflow\n",
    "\n",
    "Now let's test the orchestrator with different types of questions to see how it delegates to sub-agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to send messages and display responses\n",
    "def chat_with_agent(agent_key, session_key, message, show_events=False):\n",
    "    \"\"\"Send a message to an agent and return the response.\"\"\"\n",
    "    message_data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"content\": message\n",
    "            }\n",
    "        ],\n",
    "        \"stream_response\": False\n",
    "    }\n",
    "    \n",
    "    url = f\"{BASE_URL}/agents/{agent_key}/sessions/{session_key}/events\"\n",
    "    response = requests.post(url, headers=headers, json=message_data)\n",
    "    \n",
    "    if response.status_code == 201:\n",
    "        event_data = response.json()\n",
    "        \n",
    "        if show_events:\n",
    "            print(\"\\n------ All Events ------\")\n",
    "            for event in event_data.get('events', []):\n",
    "                event_type = event.get('type', 'INVALID')\n",
    "                print(f\"  Event type: {event_type}\")\n",
    "                if event_type == 'tool_input':\n",
    "                    print(f\"    Tool: {event.get('tool_configuration_name', 'N/A')}\")\n",
    "                    if event.get(\"tool_input\", None):\n",
    "                        print(f\"    Tool input: {event[\"tool_input\"][\"message\"]}\")\n",
    "                if event_type == 'tool_output':\n",
    "                    print(f\"    Tool: {event.get('tool_configuration_name', 'N/A')}\")\n",
    "                    if event.get(\"tool_output\", None):\n",
    "                        print(f\"    Tool output: {event[\"tool_output\"][\"sub_agent_response\"]}...\")\n",
    "            print(\"-\"*20)\n",
    "        \n",
    "        # Extract agent output\n",
    "        for event in event_data.get('events', []):\n",
    "            if event.get('type') == 'agent_output':\n",
    "                return event.get('content', 'No content')\n",
    "        \n",
    "        return \"No agent output found\"\n",
    "    else:\n",
    "        return f\"Error: {response.status_code} - {response.text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session Created: ase_orchestrator_session_20251203-214720_a60c\n"
     ]
    }
   ],
   "source": [
    "# Create a session for the orchestrator\n",
    "session_name = f\"Orchestrator Session {datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "session_config = {\n",
    "    \"name\": session_name,\n",
    "    \"metadata\": {\n",
    "        \"purpose\": \"sub_agent_demo\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{BASE_URL}/agents/{orchestrator_key}/sessions\",\n",
    "    headers=headers,\n",
    "    json=session_config\n",
    ")\n",
    "\n",
    "if response.status_code == 201:\n",
    "    session_data = response.json()\n",
    "    orchestrator_session_key = session_data[\"key\"]\n",
    "    print(f\"Session Created: {orchestrator_session_key}\")\n",
    "else:\n",
    "    print(f\"Error: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "### Test 1: Research-Focused Question\n",
    "\n",
    "This question should primarily use the research_analyst sub-agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cell-19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What are the key innovations in the original RAG paper?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "------ All Events ------\n",
      "  Event type: input_message\n",
      "  Event type: tool_input\n",
      "    Tool: research_analyst\n",
      "    Tool input: Analyze the original RAG (Retrieval-Augmented Generation) research paper and summarize its key innovations. Focus on the development and applications of the RAG model including any novel methodologies or approaches introduced.\n",
      "  Event type: tool_output\n",
      "    Tool: research_analyst\n",
      "    Tool output: The original research paper on Retrieval-Augmented Generation (RAG) introduces a novel approach for knowledge-intensive natural language processing (NLP) tasks. RAG combines the strengths of parametric memory in pre-trained generative models with non-parametric memory using retrieval mechanisms, creating a hybrid system for more effective language generation.\n",
      "\n",
      "### Key Innovations:\n",
      "1. **Hybrid Memory Model**: RAG leverages both parametric memory, in the form of a pre-trained sequence-to-sequence (seq2seq) transformer, and non-parametric memory, through a dense vector index of external data sources like Wikipedia. This combination allows RAG to perform better in open-domain question answering tasks [Lewis et al., 2020].\n",
      "\n",
      "2. **End-to-End Training**: The RAG model is trained in an end-to-end manner, incorporating both retrieval and generation processes into a single probabilistic framework. This approach enhances the model's ability to generate accurate and specific responses by conditioning generation on retrieved information [Lewis et al., 2020].\n",
      "\n",
      "3. **Two RAG Formulations**: The research explores two formulations of RAG: RAG-Token and RAG-Sequence. RAG-Token allows different retrieved passages for each token in the output sequence, while RAG-Sequence uses the same retrieved passages for the entire sequence [Lewis et al., 2020].\n",
      "\n",
      "4. **Retrieval Component**: The retrieval mechanism is a significant part of the model, initialized using the Dense Passage Retriever (DPR), and proven to enhance the system's performance significantly. Unlike other models like REALM, RAG achieves its results without the need for specialized pre-training such as salient span masking [Lewis et al., 2020].\n",
      "\n",
      "5. **Application and Performance**: RAG models set new state-of-the-art performances in various open-domain QA tasks. They outperform existing systems by combining the flexibility of closed-book methods and the accuracy of open-book approaches without requiring complex retrieval and reranking strategies [Lewis et al., 2020].\n",
      "\n",
      "6. **Modular and Updatable**: RAG's design allows its retrieval index to be updated or replaced without retraining the entire model, providing a flexible and extensible system for incorporating new information [Lewis et al., 2020].\n",
      "\n",
      "### Practical Implications and Limits:\n",
      "- **Applications**: RAG is particularly well-suited for tasks requiring robust knowledge retrieval, such as question answering, summarization, and information retrieval tasks, where it can independently reason and retrieve necessary information to generate informed responses.\n",
      "  \n",
      "- **Limitations**: While RAG performs well in many QA tasks, the model's performance relies heavily on the quality and comprehensiveness of the non-parametric memory source (like Wikipedia). Furthermore, integrating retrieval and parametric components might induce additional computational complexity [Lewis et al., 2020].\n",
      "\n",
      "This research paper by Lewis et al. marks a significant advancement in the integration of retrieval mechanisms with generative language models, offering a pathway to more informed AI systems capable of leveraging extensive datasets efficiently ([Source: Lewis et al., 2020](https://arxiv.org/pdf/2005.11401.pdf))....\n",
      "  Event type: agent_output\n",
      "--------------------\n",
      "Agent Response:\n",
      "The original Retrieval-Augmented Generation (RAG) paper introduces several key innovations in the field of knowledge-intensive natural language processing (NLP):\n",
      "\n",
      "1. **Hybrid Memory Model**: RAG combines parametric memory (a pre-trained seq2seq transformer) with non-parametric memory (a dense vector index of external data sources like Wikipedia). This hybrid approach allows it to effectively handle open-domain question answering tasks.\n",
      "\n",
      "2. **End-to-End Training**: The model incorporates both retrieval and generation processes into a single probabilistic framework, making it capable of generating accurate and specific responses from retrieved information.\n",
      "\n",
      "3. **Two RAG Formulations**: The paper introduces RAG-Token and RAG-Sequence, which differ based on how they use retrieved passages—RAG-Token retrieves for each token, while RAG-Sequence retrieves for the entire sequence.\n",
      "\n",
      "4. **Retrieval Component**: RAG uses the Dense Passage Retriever (DPR) for its retrieval mechanism, which significantly enhances performance without needing specialized pre-training methods.\n",
      "\n",
      "5. **Application and Performance**: RAG achieves state-of-the-art performance in various open-domain QA tasks, combining the strengths of closed-book and open-book approaches without complex retrieval strategies.\n",
      "\n",
      "6. **Modular and Updatable**: The retrieval index in RAG can be updated or replaced without retraining the model, offering flexibility in integrating new information.\n",
      "\n",
      "### Practical Implications and Limitations:\n",
      "- **Applications**: It's particularly effective for tasks like question answering, summarization, and any task that benefits from knowledge retrieval.\n",
      "- **Limitations**: RAG's performance depends on the quality of its non-parametric memory. This setup can also introduce computational complexity.\n",
      "\n",
      "These innovations collectively offer a significant advancement by integrating retrieval mechanisms with generative models, providing a pathway to more informed AI systems ([Source: Lewis et al., 2020](https://arxiv.org/pdf/2005.11401.pdf)).\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key innovations in the original RAG paper?\"\n",
    "print(f\"User: {query}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "response = chat_with_agent(\n",
    "    orchestrator_key,\n",
    "    orchestrator_session_key,\n",
    "    query,\n",
    "    show_events=True\n",
    ")\n",
    "\n",
    "print(f\"Agent Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "### Test 2: Implementation-Focused Question\n",
    "\n",
    "This question should primarily use the docs_expert sub-agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: How do I configure hybrid search with Vectara's API?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "------ All Events ------\n",
      "  Event type: input_message\n",
      "  Event type: tool_input\n",
      "    Tool: docs_expert\n",
      "    Tool input: Provide a step-by-step guide on how to configure hybrid search using Vectara's API. Include details on any setup, configuration of parameters, and any required code snippets. Also, highlight best practices to ensure efficient implementation.\n",
      "  Event type: tool_output\n",
      "    Tool: docs_expert\n",
      "    Tool output: To configure hybrid search using Vectara's API, you'll need to combine semantic and lexical search capabilities by adjusting specific API parameters. Here's a step-by-step guide to implementing hybrid search:\n",
      "\n",
      "### Step-by-Step Guide\n",
      "\n",
      "1. **Authentication Setup**:\n",
      "   - First, ensure you have the necessary API key or OAuth 2.0 credentials. You can configure these in Postman or your application where you're testing the API.\n",
      "\n",
      "2. **Create a Corpus**:\n",
      "   - If you haven't already, you need to create a corpus where your documents will be indexed and queried. You can do this via the Vectara Console or by using the API endpoints directly (e.g., `https://api.vectara.io/v2/corpora`).\n",
      "\n",
      "3. **Index Documents**:\n",
      "   - Use the Vectara indexing API to add documents to your corpus. This involves sending POST requests to the indexing endpoint with your documents in the payload. Example:\n",
      "\n",
      "     ```http\n",
      "     POST https://api.vectara.io/v2/corpora/<your_corpus_key>/documents\n",
      "     \n",
      "     {\n",
      "       \"document\": {\n",
      "         \"title\": \"Sample Document Title\",\n",
      "         \"body\": \"This is the content to be indexed for semantic and lexical search.\",\n",
      "         \"metadata_json\": \"{\\\"category\\\": \\\"documentation\\\"}\"\n",
      "       }\n",
      "     }\n",
      "     ```\n",
      "\n",
      "4. **Configure Hybrid Search Parameters**:\n",
      "   - Use the search API to perform queries with a mix of semantic and lexical search. You accomplish this by adjusting the `lexical_interpolation` parameter, which determines the balance between semantic and lexical results.\n",
      "\n",
      "     ```http\n",
      "     POST https://api.vectara.io/v2/search\n",
      "     \n",
      "     {\n",
      "       \"query\": \"What is Vectara hybrid search?\",\n",
      "       \"search\": {\n",
      "         \"corpora\": [\n",
      "           {\n",
      "             \"corpus_key\": \"<your_corpus_key>\",\n",
      "             \"lexical_interpolation\": 0.5  // Adjust this value between 0 and 1\n",
      "           }\n",
      "         ],\n",
      "         \"limit\": 10,\n",
      "         \"context_configuration\": {\n",
      "           \"sentences_before\": 1,\n",
      "           \"sentences_after\": 1\n",
      "         }\n",
      "       }\n",
      "     }\n",
      "     ```\n",
      "\n",
      "5. **Tuning Hybrid Search**:\n",
      "   - **Lexical Interpolation**: The value for this parameter can range from 0 (purely semantic) to 1 (purely lexical). A mid-range value like `0.5` provides an equal mix of both.\n",
      "   - Experiment with different values and evaluate the quality of the returned results to find the optimal setting for your use case.\n",
      "\n",
      "6. **Leverage Metadata Filters**:\n",
      "   - Use metadata filters to refine your search results further. This ensures that only relevant documents are considered based on metadata criteria specified in your query.\n",
      "\n",
      "7. **Testing and Iteration**:\n",
      "   - Test the setup using various queries to ensure it meets your expectations for search relevance and accuracy. Adjust the parameters as needed based on your testing.\n",
      "\n",
      "### Best Practices\n",
      "\n",
      "- **Start with a Balanced Interpolation**: Begin with a `0.5` value for `lexical_interpolation` and adjust based on your retrieval needs.\n",
      "- **Use Filters Wisely**: Leverage metadata filters to limit the scope of your searches to only the most relevant documents.\n",
      "- **Continuous Iteration**: Regularly evaluate search results and adjust configurations to meet evolving data and business requirements.\n",
      "\n",
      "### Documentation References\n",
      "\n",
      "- **API Quick Start Guide**: [Vectara API Recipes](https://docs.vectara.com/docs/api-recipes)\n",
      "- **Indexing API Reference**: [Indexing Documentation](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing)\n",
      "\n",
      "By following these steps and best practices, you can effectively implement and optimize hybrid search functionality using Vectara's powerful search capabilities....\n",
      "  Event type: agent_output\n",
      "--------------------\n",
      "Agent Response:\n",
      "To configure hybrid search with Vectara's API, you need to combine semantic and lexical search capabilities by adjusting specific parameters. Here’s how you can achieve this:\n",
      "\n",
      "### Step-by-Step Guide\n",
      "\n",
      "1. **Authentication Setup**:\n",
      "   - Ensure you have the necessary API key or OAuth 2.0 credentials. These are required to authenticate your requests.\n",
      "\n",
      "2. **Create a Corpus**:\n",
      "   - Create a corpus for indexing and querying documents. This can be done via the Vectara Console or using the API (e.g., `https://api.vectara.io/v2/corpora`).\n",
      "\n",
      "3. **Index Documents**:\n",
      "   - Use the Vectara indexing API to add documents to your corpus. Send POST requests with your document details.\n",
      "\n",
      "     ```http\n",
      "     POST https://api.vectara.io/v2/corpora/<your_corpus_key>/documents\n",
      "\n",
      "     {\n",
      "       \"document\": {\n",
      "         \"title\": \"Sample Document Title\",\n",
      "         \"body\": \"This is the content to be indexed for semantic and lexical search.\",\n",
      "         \"metadata_json\": \"{\\\"category\\\": \\\"documentation\\\"}\"\n",
      "       }\n",
      "     }\n",
      "     ```\n",
      "\n",
      "4. **Configure Hybrid Search Parameters**:\n",
      "   - Perform queries with a mix of semantic and lexical search by adjusting the `lexical_interpolation` parameter.\n",
      "\n",
      "     ```http\n",
      "     POST https://api.vectara.io/v2/search\n",
      "\n",
      "     {\n",
      "       \"query\": \"What is Vectara hybrid search?\",\n",
      "       \"search\": {\n",
      "         \"corpora\": [\n",
      "           {\n",
      "             \"corpus_key\": \"<your_corpus_key>\",\n",
      "             \"lexical_interpolation\": 0.5  // Adjust this value between 0 and 1\n",
      "           }\n",
      "         ],\n",
      "         \"limit\": 10,\n",
      "         \"context_configuration\": {\n",
      "           \"sentences_before\": 1,\n",
      "           \"sentences_after\": 1\n",
      "         }\n",
      "       }\n",
      "     }\n",
      "     ```\n",
      "\n",
      "5. **Tuning Hybrid Search**:\n",
      "   - **Lexical Interpolation**: A value of 0 results in purely semantic search, and 1 yields purely lexical search. An initial value of `0.5` offers a balanced search approach.\n",
      "   - Evaluate and adjust this parameter based on the quality of search results.\n",
      "\n",
      "6. **Leverage Metadata Filters**:\n",
      "   - Use metadata filters to refine results based on specific document criteria you define.\n",
      "\n",
      "7. **Testing and Iteration**:\n",
      "   - Test with various queries to ensure relevance and accuracy. Adjust configurations based on results.\n",
      "\n",
      "### Best Practices\n",
      "\n",
      "- **Start with Balanced Interpolation**: Begin with a `0.5` setting for a mix of semantic and lexical searches.\n",
      "- **Use Filters Wisely**: Refine search results with appropriate metadata filters.\n",
      "- **Continuous Iteration**: Regularly review and tweak configurations as data and business needs change.\n",
      "\n",
      "### Documentation References\n",
      "\n",
      "- [API Quick Start Guide](https://docs.vectara.com/docs/api-recipes)\n",
      "- [Indexing API Reference](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing)\n",
      "\n",
      "By following these guidelines and best practices, you can effectively configure and optimize hybrid search functionality using Vectara's API.\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I configure hybrid search with Vectara's API?\"\n",
    "print(f\"User: {query}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "response = chat_with_agent(\n",
    "    orchestrator_key,\n",
    "    orchestrator_session_key,\n",
    "    query,\n",
    "    show_events=True,\n",
    ")\n",
    "\n",
    "print(f\"Agent Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "### Test 3: Comprehensive Question (Both Sub-Agents)\n",
    "\n",
    "This question should use both sub-agents and synthesize their responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cell-23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Explain how dense retrieval works theoretically, and show me how to implement it with Vectara.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "------ All Events ------\n",
      "  Event type: input_message\n",
      "  Event type: tool_input\n",
      "    Tool: research_analyst\n",
      "    Tool input: Explain the theoretical foundations of dense retrieval in information retrieval systems. Focus on the concepts, algorithms used (e.g., BERT, Transformer models), and how it compares to traditional lexical retrieval methods.\n",
      "  Event type: tool_input\n",
      "    Tool: docs_expert\n",
      "    Tool input: Provide a guide on how to implement dense retrieval using Vectara’s API. Include any necessary setup, relevant API endpoints, and example code for indexing and querying documents.\n",
      "  Event type: tool_output\n",
      "    Tool: research_analyst\n",
      "    Tool output: Dense retrieval is an advanced approach in information retrieval systems that aims to improve the effectiveness of finding relevant documents or passages in response to a query. Unlike traditional lexical retrieval methods that rely on keyword matching (e.g., BM25), dense retrieval leverages dense vector representations to capture semantic meaning, allowing for more effective retrieval based on context and related meanings rather than exact word matches.\n",
      "\n",
      "### Theoretical Foundations and Components:\n",
      "\n",
      "1. **Dense Vector Representations:**\n",
      "   - Dense retrieval employs vector representations (embeddings) of both queries and documents, typically learned through neural network models, such as Transformers. These embeddings capture semantic meanings, enabling retrieval based on content similarity rather than lexical overlap.\n",
      "   - BERT (Bidirectional Encoder Representations from Transformers) is commonly used to generate such embeddings, facilitating the creation of a vector space where semantically similar queries and documents are closer together.\n",
      "\n",
      "2. **Bi-Encoder Architecture:**\n",
      "   - Systems like DPR (Dense Passage Retrieval) utilize a bi-encoder setup, where separate neural encoders are used for queries and documents. Both encoders, often based on pre-trained models like BERT, generate dense vectors, which are then compared using techniques such as Maximum Inner Product Search (MIPS) to find the most relevant documents by measuring the angle (or dot product) between vectors.\n",
      "\n",
      "3. **Retrieval Models:**\n",
      "   - **DPR (Dense Passage Retriever):** Encodes queries and documents into dense vectors, using BERT as the backbone, and retrieves documents based on the similarity of these vectors, effectively handling open-domain question answering tasks.\n",
      "\n",
      "4. **Retrieval Augmented Generation (RAG):**\n",
      "   - Concepts such as RAG combine dense retrieval with generative models. Here, a generative model like a seq2seq transformer relies on retrieved documents (from dense retrieval) as context, enhancing tasks like language generation with additional knowledge.\n",
      "\n",
      "### Comparison with Traditional Lexical Retrieval:\n",
      "\n",
      "- **Semantic vs. Lexical Matching:**\n",
      "  - Dense retrieval focuses on semantic similarity by comparing vectorized representations, enabling better handling of synonyms and context variations, whereas lexical methods depend on keyword matches, often missing nuanced meanings.\n",
      "  \n",
      "- **Handling Rich Contexts:**\n",
      "  - By leveraging neural network models, dense retrieval systems can understand and encode rich contextual information, achieving better performance in complex natural language processing tasks.\n",
      "\n",
      "### Practical Implications:\n",
      "\n",
      "- Dense retrieval can significantly enhance applications like open-domain question answering, where traditional approaches may fail due to their reliance on exact keyword matches.\n",
      "- It integrates seamlessly with modern NLP approaches, including retrieval-augmented language models, enabling systems to incorporate external knowledge dynamically during tasks like content generation.\n",
      "\n",
      "### Limitations:\n",
      "\n",
      "- Dense retrieval systems are often computationally intensive due to the need to compute and store dense vector representations.\n",
      "- Ensuring the scalability and efficiency of such systems for large-scale applications remains challenging.\n",
      "\n",
      "**References:**\n",
      "- Lewis et al.'s work on retrieval-augmented generation (RAG) models explores the integration of dense retrieval in seq2seq models, capturing sophisticated NLP tasks ([source](https://arxiv.org/abs/2005.11401)).\n",
      "- Karpukhin et al.'s work on Dense Passage Retrieval (DPR) outlines effective retrieval strategies for open-domain question answering ([source](https://arxiv.org/abs/2004.04906)).\n",
      "\n",
      "By adopting dense retrieval, modern information retrieval systems become robust against the intricate nuances of human language, outperforming conventional methods especially in context-rich environments....\n",
      "  Event type: tool_output\n",
      "    Tool: docs_expert\n",
      "    Tool output: To implement dense retrieval using Vectara's API, you'll want to handle both indexing and querying of your documents. Here's a complete guide with setup, relevant API endpoints, and example code for integrating dense retrieval into your application using Vectara's API.\n",
      "\n",
      "### Setup and Authenticating with the API\n",
      "Before indexing and querying documents, ensure you have:\n",
      "- A Vectara API key.\n",
      "- Customer ID and corpus ID.\n",
      "\n",
      "Vectara supports API Key authentication for most endpoints and OAuth 2.0 for secure or automated workflows. Choose the method that fits your use case. Refer to the [Vectara documentation](https://docs.vectara.com/docs/api-reference/vectara-postman-collection) for more details on obtaining credentials.\n",
      "\n",
      "### Indexing Documents\n",
      "\n",
      "1. **Create a Corpus**: \n",
      "   - Ensure you have created a corpus in which to store your documents. A corpus is a container for documents relevant to a specific domain or topic.\n",
      "\n",
      "2. **Use the Indexing API to Add Documents**:\n",
      "   - Endpoint: `POST https://api.vectara.io/v2/corpora/:corpus_key/documents`\n",
      "   - Here, `:corpus_key` is the identifier for the corpus where documents will be indexed.\n",
      "\n",
      "   **Example Request** (Python with requests library):\n",
      "   ```python\n",
      "   import requests\n",
      "\n",
      "   url = \"https://api.vectara.io/v2/corpora/your_corpus_key/documents\"\n",
      "   headers = {\n",
      "       \"Authorization\": \"Bearer your_api_key\",\n",
      "       \"Content-Type\": \"application/json\"\n",
      "   }\n",
      "   data = {\n",
      "       \"document_parts\": [\n",
      "           {\n",
      "               \"text\": \"Your document text here...\",\n",
      "               \"metadata_json\": \"{\\\"category\\\": \\\"example\\\"}\"\n",
      "           }\n",
      "       ],\n",
      "       \"metadata\": {\n",
      "           \"core\": True\n",
      "       }\n",
      "   }\n",
      "\n",
      "   response = requests.post(url, headers=headers, json=data)\n",
      "   print(response.json())\n",
      "   ```\n",
      "\n",
      "3. **Document Formats**:\n",
      "   - You can index documents in structured or core formats, specifying document parts that become individual search results.\n",
      "\n",
      "4. **Custom Dimensions and Metadata**:\n",
      "   - Optionally, define custom dimensions that might be used during query time to influence the ranking of the search results.\n",
      "\n",
      "### Querying Documents\n",
      "\n",
      "1. **Perform a Search Query Using the API**:\n",
      "   - After successfully indexing your documents, you can perform queries to retrieve relevant documents based on user input.\n",
      "   - Endpoint: Use `POST` requests against Vectara's search API for querying.\n",
      "\n",
      "2. **Example Query Request**:\n",
      "   ```python\n",
      "   import requests\n",
      "\n",
      "   url = \"https://api.vectara.io/v2/query\"\n",
      "   headers = {\n",
      "       \"Authorization\": \"Bearer your_api_key\",\n",
      "       \"Content-Type\": \"application/json\"\n",
      "   }\n",
      "   query_data = {\n",
      "       \"query\": \"What you want to search for\",\n",
      "       \"search\": {\n",
      "           \"corpora\": [\n",
      "               {\n",
      "                   \"corpus_key\": \"your_corpus_key\"\n",
      "               }\n",
      "           ],\n",
      "           \"limit\": 10\n",
      "       }\n",
      "   }\n",
      "\n",
      "   response = requests.post(url, headers=headers, json=query_data)\n",
      "   print(response.json())\n",
      "   ```\n",
      "\n",
      "3. **Handling Results**:\n",
      "   - The response will include the relevant parts of your indexed documents that match the query. These results can be further customized and processed based on your application needs.\n",
      "\n",
      "By following these steps, you can effectively implement dense retrieval using Vectara's API to enhance your applications with advanced document search capabilities. For more detailed setup and customization options, refer to the [Vectara Postman Collection documentation](https://docs.vectara.com/docs/api-reference/vectara-postman-collection) for exploring ready-to-use API calls and authentication setups....\n",
      "  Event type: agent_output\n",
      "--------------------\n",
      "Agent Response:\n",
      "### Theoretical Foundations of Dense Retrieval\n",
      "\n",
      "Dense retrieval improves traditional information retrieval by utilizing dense vector representations to capture semantic relationships. Here are the key theoretical aspects:\n",
      "\n",
      "1. **Dense Vector Representations**:\n",
      "   - Queries and documents are transformed into dense vectors, learned via neural networks, typically using transformer models like BERT. This allows for capturing semantic meanings beyond mere lexical overlap.\n",
      "\n",
      "2. **Bi-Encoder Architecture**:\n",
      "   - Utilizes separate encoders for queries and documents, each generating dense vectors. The similarity between these vectors is used for retrieval, often computed using techniques like Maximum Inner Product Search (MIPS).\n",
      "\n",
      "3. **Dense Passage Retrieval (DPR)**:\n",
      "   - DPR uses BERT to encode queries and documents into dense vectors, selecting documents based on vector similarities. It's effective in open-domain question answering by understanding context and semantic nuances.\n",
      "\n",
      "4. **Retrieval-Augmented Generation (RAG)**:\n",
      "   - Combines dense retrieval with generative models, where a transformer model uses retrieved documents as context, enriching tasks like content generation with additional knowledge.\n",
      "\n",
      "**Comparison with Traditional Methods**:\n",
      "- Dense retrieval focuses on semantic meaning while traditional methods rely on exact keyword matches, often missing nuances in language.\n",
      "\n",
      "**Applications and Limitations**:\n",
      "- It enhances tasks where understanding context is crucial, although it can be computationally intensive in terms of vector computation and storage.\n",
      "\n",
      "### Implementing Dense Retrieval with Vectara\n",
      "\n",
      "#### Setup and Authentication\n",
      "- **Obtain**: API key and relevant IDs (Customer ID, Corpus ID).\n",
      "- **Choose Authentication**: API Key or OAuth 2.0 depending on security needs.\n",
      "\n",
      "#### Indexing Documents\n",
      "1. **Create a Corpus**: Container for document storage.\n",
      "2. **Index Documents**:\n",
      "   ```python\n",
      "   import requests\n",
      "\n",
      "   url = \"https://api.vectara.io/v2/corpora/your_corpus_key/documents\"\n",
      "   headers = {\"Authorization\": \"Bearer your_api_key\", \"Content-Type\": \"application/json\"}\n",
      "   data = {\n",
      "       \"document_parts\": [{\"text\": \"Your document text here...\", \"metadata_json\": \"{\\\"category\\\": \\\"example\\\"}\"}],\n",
      "       \"metadata\": {\"core\": True}\n",
      "   }\n",
      "\n",
      "   response = requests.post(url, headers=headers, json=data)\n",
      "   print(response.json())\n",
      "   ```\n",
      "\n",
      "3. **Document Formats**: Index structured formats specifying document parts for individual search results.\n",
      "4. **Custom Dimensions and Metadata**: Define during query time for better ranking influences.\n",
      "\n",
      "#### Querying Documents\n",
      "1. **Perform a Search Query**:\n",
      "   ```python\n",
      "   import requests\n",
      "\n",
      "   url = \"https://api.vectara.io/v2/query\"\n",
      "   headers = {\"Authorization\": \"Bearer your_api_key\", \"Content-Type\": \"application/json\"}\n",
      "   query_data = {\n",
      "       \"query\": \"What you want to search for\",\n",
      "       \"search\": {\n",
      "           \"corpora\": [{\"corpus_key\": \"your_corpus_key\"}],\n",
      "           \"limit\": 10\n",
      "       }\n",
      "   }\n",
      "\n",
      "   response = requests.post(url, headers=headers, json=query_data)\n",
      "   print(response.json())\n",
      "   ```\n",
      "\n",
      "2. **Handle Results**: The response will include relevant document parts matching the query.\n",
      "\n",
      "For detailed customization and implementation, Vectara’s [API documentation](https://docs.vectara.com/docs/api-reference/vectara-postman-collection) can guide you further on setups and API call structures. This integration enables your applications to leverage advanced dense retrieval capabilities effectively.\n"
     ]
    }
   ],
   "source": [
    "query = \"Explain how dense retrieval works theoretically, and show me how to implement it with Vectara.\"\n",
    "print(f\"User: {query}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "response = chat_with_agent(\n",
    "    orchestrator_key,\n",
    "    orchestrator_session_key,\n",
    "    query,\n",
    "    show_events=True\n",
    ")\n",
    "\n",
    "print(f\"Agent Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "## Cleanup (Optional)\n",
    "\n",
    "If you want to delete the agents created in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cell-30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted agent: agt_ai_research_orchestrator_b41b\n",
      "Deleted agent: agt_research_paper_analyst_7916\n",
      "Deleted agent: agt_documentation_expert_2b20\n"
     ]
    }
   ],
   "source": [
    "# Delete parent first since it depends on sub-agents\n",
    "\n",
    "agents_to_delete = [\n",
    "    orchestrator_key,\n",
    "    research_analyst_key,\n",
    "    docs_expert_key\n",
    "]\n",
    "\n",
    "for agent_key in agents_to_delete:\n",
    "    if agent_key:\n",
    "        response = requests.delete(f\"{BASE_URL}/agents/{agent_key}\", headers=headers)\n",
    "        if response.status_code == 204:\n",
    "            print(f\"Deleted agent: {agent_key}\")\n",
    "        else:\n",
    "            print(f\"Error deleting {agent_key}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f10017f-bc24-4360-814d-5c3838f30afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
