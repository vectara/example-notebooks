{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc259945",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e8e57",
   "metadata": {},
   "source": [
    "# Vectara and LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7548c0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain langchain_community langchain_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a399ef-5cfb-4707-8ab1-8bbe1d68a169",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted GenAI and semantic search platform that provides an easy-to-use API for document indexing and querying. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. A way to extract text from document files and chunk them into sentences.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LangChain you do not need to call a separate embedder model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embedding, and retrieves the most relevant text segments (including support for [Hybrid Search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching), [MMR](https://vectara.com/get-diverse-results-and-comprehensive-summaries-with-vectaras-mmr-reranker/), and a state of the art [multi-lingualreranker](https://vectara.com/blog/deep-dive-into-vectara-multilingual-reranker-v1-state-of-the-art-reranker-across-100-languages/))\n",
    "\n",
    "4. An option to create [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits for using Vectara for a RAG application are:\n",
    "* **Easy to use**: Vectara provides an end-to-end, fully functional, highly scalable and robust RAG pipeline, so as a user you don't have to code up these pieces and maintain them over time.\n",
    "* **Scalable and Secure**: building GenAI applications may seem easy at first, but the DIY approach can become overwhelming beyond simple examples. Vectara provides instant scalablility to millions of documents, while maintaing data security and privacy, as well as latency SLAs.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f945f-eff4-498a-974b-cf93f3202df6",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You will need a Vectara account to use Vectara with LangChain. To get started, use the following steps:\n",
    "1. [Sign up](https://www.vectara.com/integrations/langchain) for a Vectara account if you don't already have one. Once you have completed your sign up you will have a Vectara customer ID. You can find your customer ID by clicking on your name, on the top-right of the Vectara console window.\n",
    "2. Within your account you can create one or more corpora. Each corpus represents an area that stores text data upon ingest from input documents. To create a corpus, use the **\"Create Corpus\"** button. You then provide a name to your corpus as well as a description. Optionally you can define filtering attributes and apply some advanced options. If you click on your created corpus, you can see its name and corpus ID right on the top.\n",
    "3. Next you'll need to create API keys to access the corpus. Click on the **\"Access Control\"** tab in the corpus view and then the **\"Create API Key\"** button. Give your key a name, and choose whether you want query only or query+index for your key. Click \"Create\" and you now have an active API key. Keep this key confidential. Alternatively Vectara also provides a [Personal API key](https://vectara.com/blog/vectaras-new-personal-api-keys/) that is tied to your account and provides broader permissions.\n",
    "\n",
    "To use LangChain with Vectara, you'll need to provide your `customer ID`, `corpus ID` and an `api_key` to the LangChain Vectara class. You can do this in two ways:\n",
    "\n",
    "1. Include in your environment these three variables: `VECTARA_CUSTOMER_ID`, `VECTARA_CORPUS_ID` and `VECTARA_API_KEY`.\n",
    "\n",
    "> For example, you can set these variables using `os.environ` as follows (these credentials point to one to a public Vectara corpus where the contents of [vectara documentation](https://docs.vectara.com/docs/) are indexed):\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "customer_id = '1366999410'\n",
    "corpus_id = '1'\n",
    "api_key = 'zqt_UXrBcnI2UXINZkrv4g1tQPhzj02vfdtqYJIDiA'\n",
    "\n",
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = customer_id\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = corpus_id\n",
    "os.environ[\"VECTARA_API_KEY\"] = api_key\n",
    "```\n",
    "\n",
    "2. Add them explicitly to the Vectara constructor:\n",
    "\n",
    "```python\n",
    "vectorstore = Vectara(\n",
    "                vectara_customer_id=customer_id,\n",
    "                vectara_corpus_id=corpus_id,\n",
    "                vectara_api_key=api_key\n",
    "            )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b794d8e-7c1c-4300-8a6d-e7665a5d3e5e",
   "metadata": {},
   "source": [
    "## Vectara: RAG-as-a-service\n",
    "\n",
    "Vectara is not a vector DB, it's much more than that - it is a full **RAG-as-a-service** platform. \n",
    "Yes, we have our own internal implementation of a scalable and serverless vector database, but that is just one piece of a whole set of components needed to implement RAG. The other components include text extraction, chunking, the Boomerang embedding model, advanced retrieval such as hybrid search or MMR, multi-lingual reranker, and more.\n",
    "\n",
    "You can ingest data into Vectara directly using Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) API, using a tool like [vectara-ingest](https://github.com/vectara/vectara-ingest), or via the Vectara Langchain component directly. We will demonstrate data ingest via LangChain later in this notebook - for now let's assume you already ingested data into your Vectara corpus and see how querying works - the fun part!\n",
    "\n",
    "Throughout this notebook, we will utilize LangChain [LCEL](https://python.langchain.com/docs/expression_language/) which provides a nice syntax for chaining components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0c4d437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "customer_id = '1366999410'\n",
    "corpus_id = '1'\n",
    "api_key = 'zqt_UXrBcnI2UXINZkrv4g1tQPhzj02vfdtqYJIDiA'\n",
    "\n",
    "os.environ[\"VECTARA_CUSTOMER_ID\"] = customer_id\n",
    "os.environ[\"VECTARA_CORPUS_ID\"] = corpus_id\n",
    "os.environ[\"VECTARA_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66910837-637f-4682-9c35-925783e70beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Vectara is an end-to-end platform that integrates powerful generative AI capabilities into applications, offering significant improvements over traditional searches by understanding data context and meaning. It provides a comprehensive authorization system and a Hybrid Search combining keyword and semantic approaches. Vectara ensures data security by not training on customer data and supports encryption and client-configurable data retention. This platform acts as a research assistant, accelerating the research process, providing relevant information, and aiding decision-making by transforming data into insights. Vectara enables precise answers, enhances user experiences, and offers context-aware responses through a dynamic Question and Answer system.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Vectara\n",
    "from langchain_community.vectorstores.vectara import (\n",
    "    RerankConfig,\n",
    "    SummaryConfig,\n",
    "    VectaraQueryConfig,\n",
    ")\n",
    "\n",
    "# Instantiate the Vectara object, pointing it to the corpus as specified by the environment variables\n",
    "vectara = Vectara()\n",
    "\n",
    "# Define configuration for generative summary component and create the \"retriever\" object\n",
    "summary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\"eng\", \n",
    "                               prompt_name=\"vectara-summary-ext-24-05-sml\")\n",
    "rerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\n",
    "config = VectaraQueryConfig(k=10, lambda_val=0.005, summary_config=summary_config, rerank_config=rerank_config)\n",
    "\n",
    "query_str = \"What is Vectara?\"\n",
    "rag = vectara.as_rag(config)\n",
    "rag.invoke(query_str)[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47738d47-b9b0-4d11-a670-aaf069685721",
   "metadata": {},
   "source": [
    "Notice how simple the RAG pipeline is here. It does not require access to an OpenAI key or any other external service for that matter, everything gets done inside the Vectara RAG platform. \n",
    "\n",
    "To set things up we have configured:\n",
    "- `SummaryConfig`: used to specify parameters for the generative summarizer, such as the language of the response, the number of top_k results to include in the summary, or the summarizer (prompt) name.\n",
    "- `RerankConfig`: used to control reranking, providing options like MMR or the multi-lingual reranker\n",
    "- `VectaraQueryConfig` providing the overall configuration structure to control the RAG pipeline.\n",
    "\n",
    "With this configuration, all you have to do is call `vectara.as_rag(config)` and you get a LangChain `Runnable` object on which you can run `invoke()` or `stream()`. \n",
    "\n",
    "To learn more about configuration parameters for summarization see [this document](https://github.com/langchain-ai/langchain/blob/1e748a6d406fc4ed5c3ca1218f4990e6a45530f3/docs/docs/integrations/providers/vectara/index.mdx#vectara-for-retrieval-augmented-generation-rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29de2540-3cf9-46f9-a6cb-e3a642cf08fa",
   "metadata": {},
   "source": [
    "## Hallucination detection and Factual Consistency Score\n",
    "\n",
    "Vectara created [HHEM](https://huggingface.co/vectara/hallucination_evaluation_model) - an open source model that can be used to evaluate RAG responses for factual consistency.\n",
    "\n",
    "As part of the Vectara RAG, the \"Factual Consistency Score\" (or FCS), which is an improved version of the open source HHEM is made available via the API. This is automatically included in the output of the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2acb95-21a8-46db-86e9-484c43d3199c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectara supports various file types for text extraction, including PDFs, Microsoft Word, Text, HTML, and Markdown files. Additionally, semi-structured documents reflecting a proto message can be sent in specific formats. The supported raw document types include those commonly used for unstructured documents. The system also indexes extracted values to enhance filter expression performance. Furthermore, Vectara supports specific attribute types like integer, real, text, and boolean for indexing documents. The maximum file size allowed for upload is 10 MB.\n",
      "\n",
      "Vectara FCS = 0.81162083\n"
     ]
    }
   ],
   "source": [
    "resp1 = rag.invoke(\"What file types are supported?\")\n",
    "print(resp1[\"answer\"] + '\\n')\n",
    "print(f\"Vectara FCS = {resp1['fcs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a8367-cb2f-45cb-8282-cfaceaaa92db",
   "metadata": {},
   "source": [
    "## Vectara as a Retriever\n",
    "\n",
    "You can also integrate Vectara just as a powerful semantic search engine. Similar to other vector stores in Langchain, in this case you can use Vectara as a `retriever`, and take advantage of the stadnard `similarity_search` method (or `similarity_search_with_score`), which takes a query string and returns a list of results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2218c2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Data Encryption | Vectara Docs When you send documents to the index API or file upload API, Vectara indexes both the document text and metadata. If you choose the “textless” option for corpus creation, Vectara converts the document text into vectors for indexing but does not store the text anywhere in the platform.', metadata={'title_level': '1', 'is_title': 'true', 'lang': 'eng', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/learn/data-privacy/encryption', 'title': 'Data Encryption | Vectara Docs'}),\n",
       " Document(page_content='• Encoder swapping. Whether the indexing and querying encoders be swapped to support semantic similarity matching in addition to question-answer matching. • Textless. Defines whether corpora be built without storing the indexed text. Although all textual content is encrypted with per-corpus keys, this option may appeal when an even higher level of security is desired. Enabling this can potentially reduce the quality of search. • User rate limit. Whether per-user rate limits can be defined.', metadata={'lang': 'eng', 'offset': '2629', 'len': '136', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/learn/authentication/role-based-access-control', 'title': 'Role-Based Access Control (RBAC) | Vectara Docs'}),\n",
       " Document(page_content='In the future, you will be able to set the ARN on the Vectara Console and these instructions will be updated. Once your AWS KMS key is configured in the platform, when encrypting your document text or metadata, Vectara connects to your KMS service to generate an encryption key. The encryption key provided by the KMS is stored in-memory and used to encrypt and decrypt your data. The in-memory key expires every hour. In turn, every hour Vectara\\n\\n asks your AWS KMS to generate that encryption key again.', metadata={'lang': 'eng', 'offset': '2759', 'len': '101', 'source': 'docusaurus', 'url': 'https://docs.vectara.com/docs/learn/data-privacy/encryption', 'title': 'Data Encryption | Vectara Docs'})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.summary_config.is_enabled = False\n",
    "config.k = 3\n",
    "\n",
    "retriever = vectara.as_retriever(config=config)\n",
    "retriever.invoke(\"is data encrypted?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b16f2eb-65d1-4189-9c60-203344a8920d",
   "metadata": {},
   "source": [
    "## Vectara Chat\n",
    "\n",
    "Vectara now supports Chat functionality without any additional components. all you have to do is call `as_chat()` and the resulting bot uses Vectara's native Chat functionality behind the scenes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed80843-4bdc-40fd-989b-9dc73ae97c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_config = SummaryConfig(is_enabled=True, max_results=7, response_lang=\"eng\")\n",
    "rerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\n",
    "config = VectaraQueryConfig(\n",
    "    k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config\n",
    ")\n",
    "\n",
    "bot = vectara.as_chat(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22bb52f3-c987-48a6-82a1-2d51370da76f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The FILE_UPLOAD API offered by Vectara allows users to extract text from various file types like PDFs, Microsoft Word, Text, HTML, and Markdown. This API supports a maximum file size of 10 MB and is recommended when users have not created their extraction logic [1]. Users can attach user-defined metadata at the document level to optimize searches by formatting data as JSON. The API exposes an HTTP endpoint for uploading and indexing documents into a corpus, providing a solution for extracting text from unstructured documents with minimal manual intervention [4]. Additionally, Vectara indexes both the document text and metadata when documents are sent to the index API or file upload API. However, if the \"textless\" option for corpus creation is chosen, the document text is converted into vectors for indexing without being stored on the platform [7].'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.invoke(\"What is the FILE_UPLOAD API?\")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7921c40-56cd-4253-a7ff-95dd061f0152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The File Upload API allows extraction of text from various unstructured document types like PDFs and Microsoft Word, supporting files up to 10 MB in size. It is recommended when lacking extraction logic and enables addition of user-defined metadata for optimized searches. On the other hand, the Standard Indexing API transforms structured data into searchable format quickly, ideal for applications with clear and consistent document structure. Vectara offers different indexing APIs to suit varying needs, with the File Upload API being suitable for extracting text from unstructured documents efficiently, while the Standard Indexing API is suitable for structured data indexing.[6][7]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot.invoke(\"how is it different than standard indexing API?\")[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00784590-2e82-4059-be19-20e63a95e580",
   "metadata": {},
   "source": [
    "## Advanced LangChain query pre-processing with Vectara\n",
    "\n",
    "Vectara's \"RAG as a service\" does a lot of the heavy lifting in creating question answering or chatbot chains. The integration with LangChain provides the option to use additional capabilities such as query pre-processing like SelfQueryRetriever or [MultiQueryRetriever](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/MultiQueryRetriever/). Let's look at an example of using the MultiQueryRetriever.\n",
    "\n",
    "Since MQR uses an LLM we have to set that up - here we choose ChatOpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23c19a5a-6f85-44a7-b73c-009f7c42de16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Vectara platform is more than just an AI product. It is a pioneer in the realm of neural search, leading the way to harness the power of your data. Vectara wants to transform the way developers interact with data and unlock a world of insights at their fingertips. Welcome to the future of information interaction! If you don't have a Vectara account yet, register for one here.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "mqr = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "\n",
    "def get_summary(documents):\n",
    "    return documents[-1].page_content\n",
    "\n",
    "(mqr | get_summary).invoke(query_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d12cc05-7431-4b9d-971c-2769d82a81b8",
   "metadata": {},
   "source": [
    "## Agentic RAG with Vectara\n",
    "\n",
    "Agentic RAG is a powerful methodology to provide your RAG implementation more agency with approaches like ReAct.\n",
    "The code below demonstrates how to use Vectara with LangChain to create an agent that uses Vectara for RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0072afb-e9de-4bd1-b29f-62cb504c96d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to break down the question into simpler sub-questions to get precise answers. I'll start by asking about an API key.\n",
      "\n",
      "Action: vectara_tool\n",
      "Action Input: What is an API key?\u001b[0m\u001b[36;1m\u001b[1;3mAn API key is a unique code that grants access to specific functionalities like querying and indexing operations within a system. It allows controlled and anonymous access for running semantic searches on corpora. API keys can be managed, revoked, and replaced quickly if compromised. They are used by passing them in the header request and can be disabled temporarily if needed. API keys come in different types, such as Query API Keys for read-only operations and Index API Keys for both read and write access, with the latter being more powerful and requiring caution in production environments.\u001b[0m\u001b[32;1m\u001b[1;3mI have obtained information about what an API key is. Now, I need to find out what a JWT token is.\n",
      "\n",
      "Action: vectara_tool\n",
      "Action Input: What is a JWT token?\u001b[0m\u001b[36;1m\u001b[1;3mA JWT token is a type of token used for authentication in applications like Vectara. It is generated using specific pieces of information such as the authorization URL, Client ID, and Client Secret. This token is crucial for OAuth 2.0 authentication and is passed in API requests through the header configuration. JWT tokens offer enhanced security features and are easily detectable by security scanning tools, making accidental exposure easier to identify [1][2][3][4].\u001b[0m\u001b[32;1m\u001b[1;3mI have obtained information about both API keys and JWT tokens. Now, I need to determine when to use one over the other.\n",
      "\n",
      "Action: vectara_tool\n",
      "Action Input: When should I use an API key versus a JWT token?\u001b[0m\u001b[36;1m\u001b[1;3mYou should use an API key when you want a less tightly scoped authorization method. On the other hand, you should use a JWT token when you need a more secure authentication method that can be easily detected by security scanning tools. JWT tokens are recommended for API requests and can be generated for Vectara API requests in various ways, such as through JavaScript or cURL commands.\u001b[0m\u001b[32;1m\u001b[1;3mI have gathered the necessary information to answer the original question.\n",
      "\n",
      "Final Answer: \n",
      "\n",
      "An API key is a unique code that grants access to specific functionalities within a system, such as querying and indexing operations. It allows controlled and anonymous access and can be managed, revoked, and replaced quickly if compromised. API keys are passed in the header request and come in different types, such as Query API Keys for read-only operations and Index API Keys for both read and write access.\n",
      "\n",
      "A JWT (JSON Web Token) token is used for authentication and is generated using specific pieces of information like the authorization URL, Client ID, and Client Secret. It is crucial for OAuth 2.0 authentication and is passed in API requests through the header configuration. JWT tokens offer enhanced security features and are easily detectable by security scanning tools.\n",
      "\n",
      "You should use an API key when you want a less tightly scoped authorization method. In contrast, you should use a JWT token when you need a more secure authentication method that can be easily detected by security scanning tools. JWT tokens are recommended for API requests due to their enhanced security features.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "An API key is a unique code that grants access to specific functionalities within a system, such as querying and indexing operations. It allows controlled and anonymous access and can be managed, revoked, and replaced quickly if compromised. API keys are passed in the header request and come in different types, such as Query API Keys for read-only operations and Index API Keys for both read and write access.\n",
      "\n",
      "A JWT (JSON Web Token) token is used for authentication and is generated using specific pieces of information like the authorization URL, Client ID, and Client Secret. It is crucial for OAuth 2.0 authentication and is passed in API requests through the header configuration. JWT tokens offer enhanced security features and are easily detectable by security scanning tools.\n",
      "\n",
      "You should use an API key when you want a less tightly scoped authorization method. In contrast, you should use a JWT token when you need a more secure authentication method that can be easily detected by security scanning tools. JWT tokens are recommended for API requests due to their enhanced security features.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "\n",
    "@tool\n",
    "def vectara_tool(\n",
    "    query: str = Field(description=\"the query string.\"), \n",
    ")-> str:\n",
    "    \"\"\"A tool for getting answers to questions about Vectara's product and API API.\"\"\"\n",
    "    summary_config = SummaryConfig(is_enabled=True, max_results=5, \n",
    "                                   response_lang=\"eng\", prompt_name=\"vectara-summary-ext-24-05-sml\")\n",
    "    rerank_config = RerankConfig(reranker=\"mmr\", rerank_k=50, mmr_diversity_bias=0.2)\n",
    "    config = VectaraQueryConfig(\n",
    "        k=10, lambda_val=0.005, rerank_config=rerank_config, summary_config=summary_config\n",
    "    )\n",
    "    \n",
    "    rag = vectara.as_rag(config)\n",
    "    return rag.invoke(query)['answer']\n",
    "    \n",
    "tools = [vectara_tool]\n",
    "llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/react\")\n",
    "prompt.template = '''\n",
    "Answer the following question as best you can. \n",
    "\n",
    "You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "\n",
    "(This Thought/Action/Action Input/Observation can repeat N times)\n",
    "\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Instructions:\n",
    "- If a tool response with an incorrect response, you can rephrase your question and try again.\n",
    "- Base your response primarily on information provided by tools and not prior knowledge.\n",
    "- Tools respond better to shorter and more concise queries, so try to break down complex questions into simpler sub-questions.\n",
    "\n",
    "\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\n",
    "'''\n",
    "\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "print(agent_executor.invoke({\"input\": \"What is a an API key? What is a JWT token? when should I use one or the other?\"})['output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ee237e",
   "metadata": {},
   "source": [
    "## LangChain Vectara Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac4600",
   "metadata": {},
   "source": [
    "LangChain templates offer a collection of easily deployable reference architectures, and there are two templates for using Vectara:\n",
    "* [RAG](https://github.com/langchain-ai/langchain/tree/master/templates/rag-vectara) template for basic RAG.\n",
    "* [RAG with multi-query](https://github.com/langchain-ai/langchain/tree/master/templates/rag-vectara-multiquery) for using Vectara RAG with the multi-query retriever.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618c999c-930d-4a46-b397-7c32dad08d5f",
   "metadata": {},
   "source": [
    "## Data Ingestion into Vectara with LangChain\n",
    "\n",
    "Even though it is more common to use Vectara with LangChain for query purposes, it is also possible to ingest data into Vectara via LangChain. There are two main functions that are useful for this purpose: `add_texts` (or `add_documents` which is similar with a lightly different interface) and `add_files`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dfba86-8bed-4394-bbd1-667ef38830f4",
   "metadata": {},
   "source": [
    "For `add_texts` the input is simply a set of text strings:\n",
    "\n",
    "```python\n",
    "vectara.add_texts([\"to be or not to be\", \"that is the question\"])\n",
    "```\n",
    "\n",
    "A common pattern is to use one of LangChain's data upload classes, extract the text from there, and then upload the text. Note that no chunking is necessary (although it is optional) in this case since Vectara performs its own optimal chunking.\n",
    "\n",
    "Since Vectara supports [file upload](https://docs.vectara.com/docs/api-reference/indexing-apis/file-upload/file-upload) natively, we also added the ability to upload files (PDF, TXT, HTML, PPT, DOC, etc) directly in the LangChain class. When using this method, the file is uploaded directly to the Vectara platform, processed and chunked optimally there, so you don't have to use the LangChain document loader or chunking mechanism.\n",
    "\n",
    "As an example:\n",
    "\n",
    "```python\n",
    "vectara.add_files([\"path/to/file1.pdf\", \"path/to/file2.pdf\",...])\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
