{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/vectara-python-sdk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara's Python SDK (Beta)\n",
    "\n",
    "In this notebook we are going to show how to use Vectara's new Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. An integrated API for processing input data, including text extraction from documents and ML-based chunking.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedding model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embeddings and retrieves the most relevant text segmentsthrough [hybrid search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and a variety of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking) strategies, including a [multilingual reranker](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker), [maximal marginal relevance (MMR) reranker](https://docs.vectara.com/docs/learn/mmr-reranker), [user-defined function reranker](https://docs.vectara.com/docs/learn/user-defined-function-reranker), and a [chain reranker](https://docs.vectara.com/docs/learn/chain-reranker) that provides a way to chain together multiple reranking methods to achieve better control over the reranking, combining the strengths of various reranking methods.\n",
    "\n",
    "4. An option to create a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview) with a wide selection of LLM summarizers (including Vectara's [Mockingbird](https://vectara.com/blog/mockingbird-is-a-rag-specific-llm-that-beats-gpt-4-gemini-1-5-pro-in-rag-output-quality/), trained specifically for RAG-based tasks), based on the retrieved documents, including citations.\n",
    "\n",
    "The main benefits of using Vectara RAG-as-a-service to build your application are:\n",
    "* **Accuracy and Quality**: Vectara provides an end-to-end platform that focuses on eliminating hallucinations, reducing bias, and safeguarding copyright integrity.\n",
    "* **Security**: Vectara's platform provides acess control--protecting against prompt injection attacks--and meets SOC2 and HIPAA compliance.\n",
    "* **Explainability**: Vectara makes it easy to troubleshoot bad results by clearly explaining rephrased queries, LLM prompts, retrieved results, and agent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "To get started with Vectara, [sign up](https://console.vectara.com/signup?utm_source=vectara&utm_medium=signup&utm_term=DevRel&utm_content=example-notebooks&utm_campaign=vectara-signup-DevRel-example-notebooks) (if you haven't already) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \n",
    "\n",
    "Once you have these, you need to first install the Vectara SDK\n",
    "\n",
    "`pip install vectara`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "corpus_key = os.environ['VECTARA_CORPUS_KEY']\n",
    "api_key = os.environ['VECTARA_API_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ecadff-287c-45a7-b036-2b40bf228c1c",
   "metadata": {},
   "source": [
    "## Creating the SDK client\n",
    "\n",
    "Before using the SDK, you need to create the Vectara client, and authenticate it.\n",
    "This can be done using an API key or OAuth, we will use the API key approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e721620c-14f7-48dd-95cd-328518e42384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectara import Vectara\n",
    "client = Vectara(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Loading Data Into Vectara\n",
    "\n",
    "As with Vectara's API, there are two ways to ingest data into vectara: uploading files or indexing text. Here we will demonstrate the file upload path - we will load PDF documents from Arxiv, using Python's [arxiv](https://github.com/lukasschwab/arxiv.py) library. We will pull in data from the top papers related to \"LLM hallucinations\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40947545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "ax_client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"(ti:LLM hallucinations) OR (ti:LLM hallucinations)\",\n",
    "  max_results = 20,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = list(ax_client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ae7bd09-6569-48cc-8c54-8bf491c0e656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://arxiv.org/abs/2407.00215v1', 'http://arxiv.org/abs/2402.02643v1', 'http://arxiv.org/abs/2409.00159v2']\n"
     ]
    }
   ],
   "source": [
    "urls = [p.entry_id for p in papers]\n",
    "print(urls[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "Next, after we have these papers, let's upload them to Vectara"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from slugify import slugify\n",
    "\n",
    "for url in urls:\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "    \n",
    "    client.upload.file(\n",
    "        corpus_key=corpus_key,\n",
    "        file=content,\n",
    "        filename=slugify(url),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea571",
   "metadata": {},
   "source": [
    "For each file, as always, Vectara processes each file uploaded on the backend, and performs appropriate chunking. So you don't need to apply any local processing, or choose a chunking strategy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Running a query:\n",
    "Once the files are loaded, let's try to run some queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucinations are problematic phenomena that can occur in artificial intelligence systems, particularly in large language models (LLMs) used for various tasks like text generation and question-answering [1]. They involve the generation of false or misleading information, impacting the reliability of AI systems [3]. Research is being conducted to understand the causes of AI hallucinations and their significance in the field of artificial intelligence [1]. Efforts include investigating how LLMs respond when providing correct answers versus when hallucinating, aiming to determine the awareness and extent of hallucinations in these models [2]. Detection of hallucinations in LLMs is challenging and requires innovative approaches, such as benchmarking their hallucination tendencies and utilizing automated frameworks for efficient detection [5].\n"
     ]
    }
   ],
   "source": [
    "from vectara import (\n",
    "    SearchCorporaParameters, ContextConfiguration, GenerationParameters, \n",
    "    KeyedSearchCorpus, CustomerSpecificReranker\n",
    ")\n",
    "\n",
    "search = SearchCorporaParameters(\n",
    "    corpora=[\n",
    "        KeyedSearchCorpus(\n",
    "            corpus_key=corpus_key,\n",
    "            metadata_filter=\"\",\n",
    "            lexical_interpolation=0.005,\n",
    "        )\n",
    "    ],\n",
    "    context_configuration=ContextConfiguration(\n",
    "        sentences_before=2,\n",
    "        sentences_after=2,\n",
    "    ),\n",
    "    reranker=CustomerSpecificReranker(\n",
    "        reranker_id=\"rnk_272725719\"\n",
    "    ),\n",
    ")\n",
    "generation = GenerationParameters(\n",
    "    response_language=\"eng\",\n",
    "    enable_factual_consistency_score=True,\n",
    ")\n",
    "\n",
    "res = client.query(\n",
    "    query=\"What is a hallucination?\",\n",
    "    search=search,\n",
    "    generation=generation\n",
    ")\n",
    "print(res.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760bd62a-d6cb-4899-bfda-29f632c3b482",
   "metadata": {},
   "source": [
    "The citations are also easily accessible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fe7a8fe-cfb8-4132-a676-5aa2b38d21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[IndividualSearchResult(text='Authors:Alessandro Bruno, Pier Luigi Mazzeo, Aladine Chetouani, Marouane Tliba, Mohamed Amine Kerkouri            View a PDF of the paper titled Insights into Classifying and Mitigating LLMs\\' Hallucinations, by Alessandro Bruno and 4 other authors\\n    View PDF\\n\\n\\n\\n    \\n            Abstract:The widespread adoption of large language models (LLMs) across diverse AI applications is proof of the outstanding achievements obtained in several tasks, such as text mining, text generation, and question answering. However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as \"Hallucinations\". They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation. This paper delves into the underlying causes of AI hallucination and elucidates its significance in artificial intelligence.', score=0.9649212956428528, part_metadata={'title': \"Title:Insights into Classifying and Mitigating LLMs' Hallucinations\", 'lang': 'eng', 'section': 10, 'offset': 557, 'len': 104}, document_metadata={'og:image': '/static/browse/0.3.4/images/arxiv-logo-fb.png', 'theme-color': '#ffffff', 'og:image:width': 1200, 'twitter:card': 'summary', 'citation_title': \"Insights into Classifying and Mitigating LLMs' Hallucinations\", 'og:site_name': 'arXiv.org', 'description': \"Abstract page for arXiv paper 2311.08117v1: Insights into Classifying and Mitigating LLMs' Hallucinations\", 'citation_date': '2023/11/14', 'og:description': 'The widespread adoption of large language models (LLMs) across diverse AI applications is proof of the outstanding achievements obtained in several tasks, such as text mining, text generation, and question answering. However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as \"Hallucinations\". They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation. This paper delves into the underlying causes of AI hallucination and elucidates its significance in artificial intelligence. In particular, Hallucination classification is tackled over several tasks (Machine Translation, Question and Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and Visual Question Answer). Additionally, we explore potential strategies to mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our research addresses this critical issue within the HeReFaNMi (Health-Related Fake News Mitigation) project, generously supported by NGI Search, dedicated to combating Health-Related Fake News dissemination on the Internet. This endeavour represents a concerted effort to safeguard the integrity of information dissemination in an age of evolving AI technologies.', 'og:image:secure_url': '/static/browse/0.3.4/images/arxiv-logo-fb.png', 'twitter:image': 'https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png', 'citation_arxiv_id': 2311.08117, 'citation_online_date': '2023/11/14', 'twitter:image:alt': 'arXiv logo', 'twitter:site': '@arxiv', 'dc:title': \"[2311.08117v1] Insights into Classifying and Mitigating LLMs' Hallucinations\", 'Content-Encoding': 'ISO-8859-1', 'citation_pdf_url': 'http://arxiv.org/pdf/2311.08117', 'Content-Type': 'application/xhtml+xml; charset=ISO-8859-1', 'og:type': 'website', 'og:image:alt': 'arXiv logo', 'twitter:title': \"Insights into Classifying and Mitigating LLMs' Hallucinations\", 'og:title': \"Insights into Classifying and Mitigating LLMs' Hallucinations\", 'citation_abstract': 'The widespread adoption of large language models (LLMs) across diverse AI applications is proof of the outstanding achievements obtained in several tasks, such as text mining, text generation, and question answering. However, LLMs are not exempt from drawbacks. One of the most concerning aspects regards the emerging problematic phenomena known as \"Hallucinations\". They manifest in text generation systems, particularly in question-answering systems reliant on LLMs, potentially resulting in false or misleading information propagation. This paper delves into the underlying causes of AI hallucination and elucidates its significance in artificial intelligence. In particular, Hallucination classification is tackled over several tasks (Machine Translation, Question and Answer, Dialog Systems, Summarisation Systems, Knowledge Graph with LLMs, and Visual Question Answer). Additionally, we explore potential strategies to mitigate hallucinations, aiming to enhance the overall reliability of LLMs. Our research addresses this critical issue within the HeReFaNMi (Health-Related Fake News Mitigation) project, generously supported by NGI Search, dedicated to combating Health-Related Fake News dissemination on the Internet. This endeavour represents a concerted effort to safeguard the integrity of information dissemination in an age of evolving AI technologies.', 'og:image:height': 700, 'citation_author': 'Kerkouri, Mohamed Amine', 'msapplication-TileColor': '#da532c', 'viewport': 'width=device-width, initial-scale=1', 'X-TIKA:Parsed-By': 'org.apache.tika.parser.html.HtmlParser', 'twitter:description': 'The widespread adoption of large language models (LLMs) across diverse AI applications is proof of the outstanding achievements obtained in several tasks, such as text mining, text generation, and...', 'og:url': 'https://arxiv.org/abs/2311.08117v1', 'Content-Language': 'en', 'title': \"[2311.08117v1] Insights into Classifying and Mitigating LLMs' Hallucinations\"}, document_id='\"http-arxiv-org-abs-2311-08117v1\"', table=None, request_corpora_index=None), IndividualSearchResult(text=\"Title:Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States Authors:Hanyu Duan, Yi Yang, Kar Yan Tam            View a PDF of the paper titled Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States, by Hanyu Duan and 2 other authors\\n    View PDF\\n\\n\\n\\n    \\n            Abstract:Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates.\", score=0.9408984184265137, part_metadata={'title': \"Title:Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States\", 'lang': 'eng', 'section': 10, 'offset': 122, 'len': 230}, document_metadata={'og:image': '/static/browse/0.3.4/images/arxiv-logo-fb.png', 'theme-color': '#ffffff', 'og:image:width': 1200, 'twitter:card': 'summary', 'citation_title': \"Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States\", 'og:site_name': 'arXiv.org', 'description': \"Abstract page for arXiv paper 2402.09733v1: Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States\", 'citation_date': '2024/02/15', 'og:description': \"Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidden representation space to mitigate hallucination. We believe this work provides insights into how LLMs produce hallucinated answers and how to make them occur less often.\", 'og:image:secure_url': '/static/browse/0.3.4/images/arxiv-logo-fb.png', 'twitter:image': 'https://static.arxiv.org/icons/twitter/arxiv-logo-twitter-square.png', 'citation_arxiv_id': 2402.09733, 'citation_online_date': '2024/02/15', 'twitter:image:alt': 'arXiv logo', 'twitter:site': '@arxiv', 'dc:title': \"[2402.09733v1] Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States\", 'Content-Encoding': 'ISO-8859-1', 'citation_pdf_url': 'http://arxiv.org/pdf/2402.09733', 'Content-Type': 'application/xhtml+xml; charset=ISO-8859-1', 'og:type': 'website', 'og:image:alt': 'arXiv logo', 'twitter:title': 'Do LLMs Know about Hallucination? An Empirical Investigation of...', 'og:title': \"Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States\", 'citation_abstract': \"Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More specifically, we check whether and how an LLM reacts differently in its hidden states when it answers a question right versus when it hallucinates. To do this, we introduce an experimental framework which allows examining LLM's hidden states in different hallucination situations. Building upon this framework, we conduct a series of experiments with language models in the LLaMA family (Touvron et al., 2023). Our empirical findings suggest that LLMs react differently when processing a genuine response versus a fabricated one. We then apply various model interpretation techniques to help understand and explain the findings better. Moreover, informed by the empirical observations, we show great potential of using the guidance derived from LLM's hidden representation space to mitigate hallucination. We believe this work provides insights into how LLMs produce hallucinated answers and how to make them occur less often.\", 'og:image:height': 700, 'citation_author': 'Tam, Kar Yan', 'msapplication-TileColor': '#da532c', 'viewport': 'width=device-width, initial-scale=1', 'X-TIKA:Parsed-By': 'org.apache.tika.parser.html.HtmlParser', 'twitter:description': 'Large Language Models (LLMs) can make up answers that are not real, and this is known as hallucination. This research aims to see if, how, and to what extent LLMs are aware of hallucination. More...', 'og:url': 'https://arxiv.org/abs/2402.09733v1', 'Content-Language': 'en', 'title': \"[2402.09733v1] Do LLMs Know about Hallucination? An Empirical Investigation of LLM's Hidden States\"}, document_id='\"http-arxiv-org-abs-2402-09733v1\"', table=None, request_corpora_index=None)]\n"
     ]
    }
   ],
   "source": [
    "print(res.search_results[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9643b59-2424-4a54-89aa-5116f052a541",
   "metadata": {},
   "source": [
    "The response also include Vectara's FCS (Factual Consistency Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe64d30c-eead-4f40-969e-be18033bdea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7078585"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.factual_consistency_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cce7c7-c664-4001-8c84-f80c5d499796",
   "metadata": {},
   "source": [
    "## Using Streaming with the SDK\n",
    "\n",
    "It's easy to run a streaming query with the SDK. You simple use the `query_stream()` function instead of `query()`.\n",
    "Note that `query_stream()` returns an iterable of chunks (as described in the [query API](https://docs.vectara.com/docs/rest-api/query-corpus)), each with its own type: search_results, generation_chunk, etc. When consuming streamed results, you may want to select which ones you use, as shown in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd30027-d975-4440-a1c2-66d1bd30bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) can experience hallucinations, where they generate responses that are not real or accurate [3]. These hallucinations pose challenges in practical applications, especially in tasks like code generation, where complex contextual dependencies are involved [4]. To understand LLM hallucinations, researchers investigate how LLMs are aware of hallucination and react differently when providing accurate answers versus when hallucinating [1]. Traditional methods that aim to mitigate LLM hallucinations by grounding them in external knowledge sources may not fully eliminate hallucinations in practice [5]. The phenomenon of LLM hallucinations highlights the need for rethinking generalization strategies to address the challenges posed by these inaccuracies in LLM outputs."
     ]
    }
   ],
   "source": [
    "search = SearchCorporaParameters(\n",
    "    corpora=[\n",
    "        KeyedSearchCorpus(\n",
    "            corpus_key=corpus_key,\n",
    "            metadata_filter=\"\",\n",
    "            lexical_interpolation=0.005,\n",
    "        )\n",
    "    ],\n",
    "    context_configuration=ContextConfiguration(\n",
    "        sentences_before=2,\n",
    "        sentences_after=2,\n",
    "    ),\n",
    "    reranker=CustomerSpecificReranker(\n",
    "        reranker_id=\"rnk_272725719\"\n",
    "    ),\n",
    ")\n",
    "generation = GenerationParameters(\n",
    "    response_language=\"eng\",\n",
    "    enable_factual_consistency_score=True,\n",
    ")\n",
    "\n",
    "response = client.query_stream(\n",
    "    query=\"What is an LLM hallucination\",\n",
    "    search=search,\n",
    "    generation=generation\n",
    "    \n",
    ")\n",
    "for chunk in response:\n",
    "    if chunk.type == 'generation_chunk':\n",
    "        print(chunk.generation_chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebad3cf-b09e-4af5-be3d-ba2a3e3e8d73",
   "metadata": {},
   "source": [
    "## Using Vectara Chat with the SDK\n",
    "\n",
    "Now let's see how we can use the SDK's chat functionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e3368be-46a3-4b2f-9589-de3383c65201",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vectara import CitationParameters, ChatParameters\n",
    "\n",
    "search = SearchCorporaParameters(\n",
    "    corpora=[\n",
    "        KeyedSearchCorpus(\n",
    "            corpus_key=corpus_key,\n",
    "            metadata_filter=\"\",\n",
    "            lexical_interpolation=0.005,\n",
    "        )\n",
    "    ],\n",
    "    context_configuration=ContextConfiguration(\n",
    "        sentences_before=2,\n",
    "        sentences_after=2,\n",
    "    ),\n",
    "    reranker=CustomerSpecificReranker(\n",
    "        reranker_id=\"rnk_272725719\"\n",
    "    ),\n",
    ")\n",
    "generation = GenerationParameters(\n",
    "    response_language=\"eng\",\n",
    "    citations=CitationParameters(\n",
    "        style=\"none\",\n",
    "    ),\n",
    "    enable_factual_consistency_score=True,\n",
    ")\n",
    "chat = ChatParameters(store=True)\n",
    "\n",
    "session = client.create_chat_session(\n",
    "    search=search,\n",
    "    generation=generation,\n",
    "    chat_config=chat,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111b1fa-5efc-4ee6-99f0-21c73dbd5433",
   "metadata": {},
   "source": [
    "With chat, we start by creating a chat session. Once it's there, we can ask the first question, and follow with subsequent questions (turns) in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7004f5f6-be7e-4b0a-b395-647fb8837bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucinations in the context of AI refer to the emergence of problematic phenomena in text generation systems, particularly in question-answering systems reliant on Large Language Models (LLMs) [1]. These hallucinations can lead to the generation of false or misleading information, posing a significant challenge to the reliability of LLMs [1]. Researchers have categorized hallucinations in LLM-generated content and explored strategies to detect and mitigate them, aiming to enhance the overall trustworthiness of AI-generated outputs [3]. Despite efforts to ground LLMs in external knowledge sources, conventional approaches have struggled to fully eliminate hallucinations, indicating the complexity of addressing this issue [5]. By studying hallucinations in a structured form such as graphs, researchers have highlighted the diversity of topological hallucinations produced by modern LLMs, underscoring the need for deeper understanding and innovative solutions to tackle this phenomenon [4].\n"
     ]
    }
   ],
   "source": [
    "response = session.chat(query=\"What are hallucinations?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8b24622-bc73-4bee-929e-377f06335a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hallucinations in Artificial Intelligence, particularly in Large Language Models (LLMs), are a concerning phenomenon observed in tasks like code generation and question answering. LLMs can generate outputs that deviate from the intended input, exhibit inconsistencies, or provide factually incorrect information. Traditional approaches have struggled to fully explain why LLMs experience hallucinations in practice, leading researchers to explore the mechanisms and mitigation strategies for these occurrences. Understanding the types and extent of hallucinations in different AI applications is crucial for detecting and addressing these issues effectively [4][5]. The distinction between different types of hallucinations, such as those arising from ignorance or errors in knowledge retrieval, is highlighted as a key aspect in mitigating hallucinations in AI systems [5].\n"
     ]
    }
   ],
   "source": [
    "response = session.chat(query=\"And why do they occur?\")\n",
    "print(response.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a30511-b230-44e6-8663-fec49a7c2eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
