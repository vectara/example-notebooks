{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara and LlamaIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. A way to extract text from document files and chunk them into sentences.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedding model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embeddings and retrieves the most relevant text segments (including support for [hybrid search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking)).\n",
    "\n",
    "4. An option to create a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview) with a wide selection of LLM summarizers (including Vectara's [Mockingbird](https://vectara.com/blog/mockingbird-is-a-rag-specific-llm-that-beats-gpt-4-gemini-1-5-pro-in-rag-output-quality/), trained specifically for RAG-based tasks), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits of using Vectara for a RAG application are:\n",
    "* **Accuracy and Quality**: Vectara provides an end-to-end platform that focuses on eliminating hallucinations, reducing bias, and safeguarding copyright integrity.\n",
    "* **Security**: Vectara's platform provides acess control--protecting against prompt injection attacks--and meets SOC2 and HIPAA compliance.\n",
    "* **Explainability**: Vectara makes it easy to troubleshoot bad results by clearly explaining rephrased queries, LLM prompts, retrieved results, and agent actions.\n",
    "\n",
    "Old Verbage (LIKELY DELETE): \n",
    "* **Ease of use**: Vectara provides an end-to-end, fully functional, highly scalable, and robust RAG pipeline, so as a user you don't have to code up these pieces and maintain them over time.\n",
    "* **Scalability and Security**: Building GenAI applications may seem easy at first, but the DIY approach can become overwhelming beyond simple examples. Vectara provides instant scalablility to millions of documents, while maintaing data security and privacy, as well as latency SLAs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33079b",
   "metadata": {},
   "source": [
    "## About LlamaIndex\n",
    "\n",
    "LlamaIndex is a \"data framework\" to help you build LLM apps:\n",
    "\n",
    "1. It includes **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "2. It provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\n",
    "3. It provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "\n",
    "LlamaIndex's high-level API allows beginner users to use LlamaIndex to ingest and query their data in just a few lines of code, whereas its lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "Vectara is implemented in LlamaIndex as a [Managed Service](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices.html#vectara), abstracting all of Vectara's powerful API so they are easily integrated into LlamaIndex.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\n",
    "\n",
    "To get started with Vectara, [sign up](https://console.vectara.com/signup?utm_source=vectara&utm_medium=signup&utm_term=DevRel&utm_content=example-notebooks&utm_campaign=vectara-signup-DevRel-example-notebooks) (if you haven't already) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \n",
    "\n",
    "Once you have these, you can provide them as environment variables, which will be used by the LlamaIndex code later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U llama-index llama-index-indices-managed-vectara arxiv\n",
    "\n",
    "import os\n",
    "# os.environ['VECTARA_API_KEY'] = \"<YOUR_VECTARA_API_KEY>\"\n",
    "# os.environ['VECTARA_CORPUS_ID'] = \"<YOUR_VECTARA_CORPUS_ID>\"\n",
    "# os.environ['VECTARA_CUSTOMER_ID'] = \"<YOUR_VECTARA_CUSTOMER_ID>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Loading Data Into Vectara\n",
    "\n",
    "As mentioned above, Vectara is a RAG managed service, and in many cases data may be uploaded to the index ahead of time (e.g. by using [Airbyte](https://docs.airbyte.com/integrations/destinations/vectara), directly via Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) or using tools like [vectara-ingest](https://github.com/vectara/vectara-ingest)), but another easy way is via the VectaraIndex constructor: `from_documents()`.\n",
    "\n",
    "For this notebook, we will assume the Vectara corpus is empty and will load PDF documents from Arxiv, using Python's [arxiv](https://github.com/lukasschwab/arxiv.py) library. We will pull in data from the top papers related to \"climate change\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40947545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"(ti:embedding model) OR (ti:sentence embedding)\",\n",
    "  max_results = 100,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae7bd09-6569-48cc-8c54-8bf491c0e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2402.14776v2',\n",
       " 'http://arxiv.org/abs/2007.01852v2',\n",
       " 'http://arxiv.org/abs/1910.13291v1',\n",
       " 'http://arxiv.org/abs/2104.06719v1',\n",
       " 'http://arxiv.org/abs/1511.08198v3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.entry_id for p in papers][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "Next, download the Arxiv paper, and upload them into Vectara using the `add_file()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File temp/1909.03104v2.Efficient_Sentence_Embedding_using_Discrete_Cosine_Transform.pdf failed to load with error HTTP Error 404: Not Found\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m'\u001b[39m: paper\u001b[38;5;241m.\u001b[39mpdf_url,\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m: paper\u001b[38;5;241m.\u001b[39mtitle,\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(paper\u001b[38;5;241m.\u001b[39mauthors[\u001b[38;5;241m0\u001b[39m]),\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpublished\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m(paper\u001b[38;5;241m.\u001b[39mpublished\u001b[38;5;241m.\u001b[39mdate())\n\u001b[1;32m     22\u001b[0m     }\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minsert_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaper_fname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m shutil\u001b[38;5;241m.\u001b[39mrmtree(data_folder)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m papers\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/llama_index/indices/managed/vectara/base.py:326\u001b[0m, in \u001b[0;36mVectaraIndex.insert_file\u001b[0;34m(self, file_path, metadata, corpus_id, **insert_kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m headers\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    325\u001b[0m valid_corpus_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_corpus_id(corpus_id)\n\u001b[0;32m--> 326\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://api.vectara.io/upload?c=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_vectara_customer_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m&o=\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mvalid_corpus_id\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m&d=True\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectara_api_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m res \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m409\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/requests/sessions.py:637\u001b[0m, in \u001b[0;36mSession.post\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\u001b[38;5;28mself\u001b[39m, url, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, json\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 637\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/requests/adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/urllib3/connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/urllib3/connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    467\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/ssl.py:1307\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1305\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1306\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/ssl.py:1163\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1163\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1165\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from llama_index.indices.managed.vectara import VectaraIndex\n",
    "\n",
    "data_folder = 'temp'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Create Vectara Index\n",
    "index = VectaraIndex()\n",
    "\n",
    "# Upload content for all papers\n",
    "for paper in papers:\n",
    "    try:\n",
    "        paper_fname = paper.download_pdf(data_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"File {paper_fname} failed to load with error {e}\")\n",
    "        continue\n",
    "    metadata = {\n",
    "        'url': paper.pdf_url,\n",
    "        'title': paper.title,\n",
    "        'author': str(paper.authors[0]),\n",
    "        'published': str(paper.published.date())\n",
    "    }\n",
    "    index.insert_file(file_path=paper_fname, metadata=metadata)\n",
    "\n",
    "shutil.rmtree(data_folder)\n",
    "del papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea571",
   "metadata": {},
   "source": [
    "Two important things to note here:\n",
    "1. Vectara processes each file uploaded on the backend, and performs appropriate chunking. So you don't need to apply any local processing, or choose a chunking strategy. \n",
    "2. We have used the fields `url`, `title`, `author`, and `published` as metadata fields (for simplicity, author is the first author if there are multiple). You will need to make sure those fields are defined in your Vectara corpus as [filterable metadata fields](https://docs.vectara.com/docs/learn/metadata-search-filtering/filter-overview) to ensure we can filter by them in query time.\n",
    "\n",
    "So that's it for upload. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Querying with the VectaraIndex\n",
    "We can now ask questions using the `VectaraIndex` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb174ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is sentence embedding?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, sentence embedding can be summarized as follows:\n",
      "\n",
      "Sentence embedding is a technique that represents words or sentences in a text in a n-dimensional space, where words or sentences that are closer in the vector space are more similar [2]. It is a vital technique in practical applications, enabling the extraction of parallel data from extensive web corpora across two languages, enhancing the training of high-quality machine translation and multilingual language models [1]. Sentence embedding can be used for various tasks, including zero-shot cross-lingual retrieval, which is essential for enabling cross-lingual searches on e-commerce platforms like Amazon [1].\n",
      "\n",
      "In the context of machine learning, sentence embedding is a form of word or sentence representation by learned representations that prepare texts in an understandable format for a machine [2]. It is a crucial component in various models, such as LSTM encoders and decoders, used for tasks like sentiment analysis, entailment, and classification [3, 4, 5].\n",
      "\n",
      "Overall, sentence embedding is a technique that enables the representation of words or sentences in a text in a way that captures their meaning and relationships, enabling various applications in natural language processing and machine learning.\n",
      "\n",
      "Sources: [1], [2], [3], [4], [5]\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True, summary_num_results=5,\n",
    "    summary_response_lang=\"eng\",\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\"\n",
    ")\n",
    "res = query_engine.query(query)\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52878fd2",
   "metadata": {},
   "source": [
    "Note that the response here is fully generated by Vectara. There is no additional LLM involved (or API key you need to setup). The response also includes citations (marked in square brackets), which provide links to references used to generate this response by Vectara. \n",
    "<br>\n",
    "The `res` object includes the actual response to the user query, but also has the citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b110a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (1, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (2, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (3, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (5, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (6, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (7, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (8, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (9, 'http://arxiv.org/pdf/2404.03921v2')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(res.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cce7c7-c664-4001-8c84-f80c5d499796",
   "metadata": {},
   "source": [
    "## Using Streaming\n",
    "\n",
    "You can also stream the Vectara response simply by specifying `streaming=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abd30027-d975-4440-a1c2-66d1bd30bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, sentence embedding can be summarized as follows:\n",
      "\n",
      "Sentence embedding is a technique that represents words or sentences in a text in a n-dimensional space, where words or sentences that are closer in the vector space are more similar [2]. It is a vital technique in practical applications, enabling the extraction of parallel data from extensive web corpora across two languages, enhancing the training of high-quality machine translation and multilingual language models [1]. Sentence embedding is also instrumental for zero-shot cross-lingual retrieval, an essential method for enabling cross-lingual searches on e-commerce platforms like Amazon [1].\n",
      "\n",
      "In the context of machine learning, sentence embedding is a form of word or sentence representation by learned representations that prepare texts in an understandable format for a machine [2]. It is used in various applications, including entailment, contradiction, and neutral classification, and is often implemented using LSTM (Long Short-Term Memory) encoders and decoders [3, 4, 5, 6, 7].\n",
      "\n",
      "Overall, sentence embedding is a crucial technique in natural language processing and machine learning, enabling the representation of words and sentences in a way that can be understood by machines and used for various applications.\n",
      "\n",
      "Sources: [1], [2], [3], [4], [5], [6], [7]"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True,\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\",\n",
    "    streaming=True)\n",
    "\n",
    "res = query_engine.query(query)\n",
    "\n",
    "# print streamed output chunk by chunk\n",
    "for chunk in res.response_gen:\n",
    "    print(chunk.delta or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cd598-ce73-4c9e-9aa6-5d8604fd6c14",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "Vectara supports three types of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking):\n",
    "1. [Maximal Marginal Relevance](https://docs.vectara.com/docs/learn/mmr-reranker), or MMR, provides a reranking that can promote diversity in results at the cost of relevance.\n",
    "2. [Slingshot](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker) is a mulitilingual reranker that increases the accuracy of retrieved results across 100+ languages and is available to Vectara Scale customers.\n",
    "3. [User Defined Functions](https://docs.vectara.com/docs/learn/user-defined-function-reranker) allow you to create your own functions for reranking search results, unlocking better retrieval in a wide variety of use cases, such as sorting by recency or price of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49914a",
   "metadata": {},
   "source": [
    " Let's see an example of how to use MMR: We will run the same query but this time we will use MMR where `mmr_diversity_bias=0.3` provides a tradeoff between relevance and diversity (0.0 is full relevance, 1.0 is only diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72832e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a technique used to represent sentences in a text in a way that encodes the meaning of the sentence in a multi-dimensional space. It allows for the extraction of parallel data across languages, aids in machine translation, multilingual language models, and cross-lingual retrieval. Various models exist, such as LASER, SBERT-distill, and LaBSE, each with different training efficiencies and architectures. Sentence embedding is crucial in natural language processing tasks like document classification and sentiment analysis, ensuring the preservation of original sentence meanings in embedded vectors.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    reranker=\"mmr\",\n",
    "    rerank_k=50,\n",
    "    mmr_diversity_bias=0.3,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17020d34-5176-4910-bb88-5c5685513804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (1, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (2, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (3, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/pdf/1808.05505v3')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e76fd1",
   "metadata": {},
   "source": [
    "As you can see, the results are now reranked in a way that provides more diversity instead of maximizing pure relevance. This in turn results in a different set of chunks used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562956f5-b9b7-4288-8100-f443a5dc3e4d",
   "metadata": {},
   "source": [
    "Now let's see an example with a user defined function. We may be interested in getting results that are the most semantically similar to our question, but we also want the most up-to-date information. Thus, we can bias our search results so that the papers that are not only semantically similar but also published more recently are used to answer our query. We can do this by using the available time functions (to see other built-in functions, see the UDF Reranker [documentation](https://docs.vectara.com/docs/learn/user-defined-function-reranker)).\n",
    "\n",
    "Vectara also supports chain-reranking, which provides a way to chain together multiple reranking methods to achieve better control over the reranking, and combining the strengths of various reranking methods. A great way to use the UDF reranker is in a chain: first the multilingual reranker, followed by the maximal marginal relevance (MMR) reranker, and then a user-defined function, as shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c40bb14-05d1-4512-8d2b-52c863bf8aad",
   "metadata": {},
   "source": [
    "ADD CHAIN RERANKER WITH THIS SO THAT IT IS CLEAR HOW UDF RERANKER SHOULD BE USED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ff2f1-ccd2-4fd0-99df-fd977182df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k = 50,\n",
    "    reranker=\"chain\"\n",
    "    rerank_chain=[\n",
    "    {\n",
    "        \"type\": \"slingshot\"\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"mmr\",\n",
    "        \"diversity_bias\": 0.3\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"udf\",\n",
    "        \"user_function\": \"max(0, 10 * get('$.score') - hours(seconds((to_unix_timestamp(now()) - to_unix_timestamp(datetime_parse(get('$.published'), 'yyyy-MM-dd'))))) / 24 / 365)\",\n",
    "        \"limit\": 5\n",
    "    }\n",
    "]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What innovations have been made to sentence embedding models?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c4db9e-26c7-49f8-8025-bdc59953736d",
   "metadata": {},
   "source": [
    "DELETE THE CELL BELOW ONCE THE CHAIN RERANKER WORKS ABOVE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b62145cd-a366-4f04-9e5e-b9c2affcb970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Innovations in sentence embedding models include leveraging pooling strategies, similarity fine-tuning in a contrastive framework, utilizing prompts, implementing a two-step training process, and developing models for different languages like French and Japanese. Additionally, advancements involve open-sourcing code and pre-trained models, exploring unsupervised, task-independent models, and enhancing efficiency by using language models like BERT for sentence embeddings. These innovations aim to improve performance and explore new methods for sentence representation [1][2][3][4][5].\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    reranker=\"udf\",\n",
    "    udf_expression=\"max(0, 10 * get('$.score') - hours(seconds((to_unix_timestamp(now()) - to_unix_timestamp(datetime_parse(get('$.published'), 'yyyy-MM-dd'))))) / 24 / 365)\",\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What innovations have been made to sentence embedding models?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b96eb2-9e88-4c6a-90c4-676dddcc5ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '2024-05-30'),\n",
       " (1, '2023-01-19'),\n",
       " (2, '2017-03-07'),\n",
       " (3, '2020-05-22'),\n",
       " (4, '2024-04-05')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['published']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb7457-50e2-4b24-8416-daf65e9a4d5c",
   "metadata": {},
   "source": [
    "Notice how many of the papers used to generate the final summary were published in the recent past and they still give us information to generate a relevant response that answers our question.\n",
    "\n",
    "Also notice how we use a max() function with 0 in our user-defined expression. This is to ensure that all of our reranking scores are non-negative. Additionally, since we multiplied the original score by 10 and its value ranges from 0 to 1, we throw away any search results that are older than 10 years old for generating our final response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863b914",
   "metadata": {},
   "source": [
    "So far we've used Vectara's internal summarization capability, which is the best way for most users.\n",
    "\n",
    "You can still use Llama-Index's standard VectorStore `as_query_engine()` method, in which case Vectara's summarization won't be used, and you would be using an external LLM (e.g. OpenAI's GPT-4) and a custom prompt from LlamaIndex to generate the summary. For this option just set `summary_enabled=False`\n",
    "\n",
    "For this functionality, you will need to specify your own OpenAI API key in the environment:\n",
    "\n",
    "> `os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b0a49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a technique used in natural language processing to represent sentences as vectors in a high-dimensional space. These vectors encode the meaning of the sentences, allowing for various applications such as machine translation, information retrieval, and text classification. The goal is for sentences with similar meanings to be close to each other in this vector space, facilitating tasks that require understanding of sentence semantics.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    summary_enabled=False,\n",
    "    llm=llm\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebad3cf-b09e-4af5-be3d-ba2a3e3e8d73",
   "metadata": {},
   "source": [
    "## Using Vectara Chat\n",
    "\n",
    "Vectara now fully supports Chat in its platform, where the chat history is maintained by Vectara and so you don't have to worry about keeping history and integrating it with your RAG pipeline. \n",
    "\n",
    "To use it, simply call `as_chat_engine()`.\n",
    "\n",
    "(Chat mode always uses Vectara's summarization so you don't have to explicitly specify `summary_enabled=True` like before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42af08f5-dc13-4991-96f4-fdd294b0ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e3368be-46a3-4b2f-9589-de3383c65201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a sentence embedding model?\n",
      "\n",
      "Response: A sentence embedding model is a method to represent input sentences as fixed-dimensional vectors, regardless of sentence length. These models have shown significant enhancements in various natural language processing tasks like information retrieval, question answering, and machine translation. They are trained to adapt to specific domains by fine-tuning on synthesized data before further fine-tuning on labeled datasets, leading to improved performance [4] [3]. Additionally, some models are designed for cross-lingual applications, training towards both sentence-level and token-level alignment to improve representation translation [5].\n",
      "\n",
      "Question: What are some known models?\n",
      "\n",
      "Response: Well-known sentence embedding models include BERT, RoBERTa, ELMO, GenSen, DSE, FastSent, Quick-Thought, InferSent, Universal Sentence Encoder (USE), XLNet, and Sentence-BERT. These models utilize various methods such as fine-tuning, contextual word representations, and special tokens to generate accurate sentence embeddings for different tasks and domains.\n",
      "\n",
      "Question: How are they different than token embedding models\n",
      "\n",
      "Response: Sentence embedding models differ from token embedding models in the way they generate representations. Sentence embedding models typically pool word or token embeddings, often by averaging, and then fine-tune this process to create embeddings for specific tasks. On the other hand, token embedding models focus on individual token representations without considering the entire sentence context. Additionally, sentence embedding models aim to minimize distances between embeddings of semantically similar phrases, while token embedding models disperse token representations across the embedding space. Overall, the key distinction lies in how sentence embedding models capture the overall meaning of a sentence compared to token embedding models that focus on individual token representations.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'What is a sentence embedding model?',\n",
    "    'What are some known models?',\n",
    "    'How are they different than token embedding models'\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Question: {q}\\n\")\n",
    "    response = ce.chat(q).response\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111b1fa-5efc-4ee6-99f0-21c73dbd5433",
   "metadata": {},
   "source": [
    "Of course streaming works as well with Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8b24622-bc73-4bee-929e-377f06335a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8f16016-be7c-411f-aafd-b03d8816d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The creators behind SBERT are Reimers and Gurevych, as mentioned in the search results [3]."
     ]
    }
   ],
   "source": [
    "response = ce.stream_chat(\"Who is behind SBERT?\")\n",
    "for chunk in response.chat_stream:\n",
    "    print(chunk.delta or \"\", end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362f58b-eb9b-41e7-936b-83098ddece33",
   "metadata": {},
   "source": [
    "## Agentic RAG with Vectara\n",
    "\n",
    "Vectara also offers a framework for creating agentic applications. It can easily be install with [pip](https://pypi.org/project/vectara-agentic/) and allows users to quickly build Agentic RAG applications with their Vectara corpora and other agent tools. Much of the backend code and integrations for this package use tools from LlmaIndex. If you would like to learn more about vectara-agentic, check out the [documentation](https://vectara.github.io/vectara-agentic-docs/) for this package.\n",
    "\n",
    "In this notebook, we will show a simple example of building an assistant with a single RAG tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0fb54dc-7e18-417b-8dc0-6840129b9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U vectara-agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e71698a0-399f-4143-ba83-00f8a759c163",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'ModelProvider.OPENAI' is not a valid ModelProvider",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvectara_agentic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Agent\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvectara_agentic\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_llm\n\u001b[0;32m----> 4\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_corpus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant_specialty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence embeddings research\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtool_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mask_embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/vectara_agentic/agent.py:324\u001b[0m, in \u001b[0;36mAgent.from_corpus\u001b[0;34m(cls, tool_name, data_description, assistant_specialty, vectara_customer_id, vectara_corpus_id, vectara_api_key, agent_progress_callback, verbose, vectara_filter_fields, vectara_lambda_val, vectara_reranker, vectara_rerank_k, vectara_n_sentences_before, vectara_n_sentences_after, vectara_summary_num_results, vectara_summarizer)\u001b[0m\n\u001b[1;32m    301\u001b[0m vectara_tool \u001b[38;5;241m=\u001b[39m vec_factory\u001b[38;5;241m.\u001b[39mcreate_rag_tool(\n\u001b[1;32m    302\u001b[0m     tool_name\u001b[38;5;241m=\u001b[39mtool_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectara_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvectara_corpus_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    303\u001b[0m     tool_description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    315\u001b[0m     include_citations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    316\u001b[0m )\n\u001b[1;32m    318\u001b[0m assistant_instructions \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124m- You are a helpful \u001b[39m\u001b[38;5;132;01m{\u001b[39;00massistant_specialty\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m assistant.\u001b[39m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124m- You can answer questions about \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_description\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;124m- Never discuss politics, and always respond politely.\u001b[39m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mvectara_tool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtopic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massistant_specialty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_instructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massistant_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_progress_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_progress_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/vectara_agentic/agent.py:94\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[0;34m(self, tools, topic, custom_instructions, verbose, update_func, agent_progress_callback, agent_type)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magent_type \u001b[38;5;241m=\u001b[39m agent_type \u001b[38;5;129;01mor\u001b[39;00m AgentType(os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVECTARA_AGENTIC_AGENT_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;241m=\u001b[39m tools\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm \u001b[38;5;241m=\u001b[39m \u001b[43mget_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLLMRole\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMAIN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_custom_instructions \u001b[38;5;241m=\u001b[39m custom_instructions\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_topic \u001b[38;5;241m=\u001b[39m topic\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/vectara_agentic/utils.py:77\u001b[0m, in \u001b[0;36mget_llm\u001b[0;34m(role)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_llm\u001b[39m(role: LLMRole) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLM:\n\u001b[1;32m     76\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the LLM for the specified role.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     model_provider, model_name \u001b[38;5;241m=\u001b[39m \u001b[43m_get_llm_params_for_role\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrole\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_provider \u001b[38;5;241m==\u001b[39m ModelProvider\u001b[38;5;241m.\u001b[39mOPENAI:\n\u001b[1;32m     80\u001b[0m         llm \u001b[38;5;241m=\u001b[39m OpenAI(model\u001b[38;5;241m=\u001b[39mmodel_name, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, is_function_calling_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/site-packages/vectara_agentic/utils.py:41\u001b[0m, in \u001b[0;36m_get_llm_params_for_role\u001b[0;34m(role)\u001b[0m\n\u001b[1;32m     36\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\n\u001b[1;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVECTARA_AGENTIC_TOOL_MODEL_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     38\u001b[0m         provider_to_default_model_name\u001b[38;5;241m.\u001b[39mget(model_provider),\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m     model_provider \u001b[38;5;241m=\u001b[39m \u001b[43mModelProvider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetenv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVECTARA_AGENTIC_MAIN_LLM_PROVIDER\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mDEFAULT_MODEL_PROVIDER\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     model_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVECTARA_AGENTIC_MAIN_MODEL_NAME\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     46\u001b[0m         provider_to_default_model_name\u001b[38;5;241m.\u001b[39mget(model_provider),\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     49\u001b[0m agent_type \u001b[38;5;241m=\u001b[39m AgentType(\n\u001b[1;32m     50\u001b[0m     os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVECTARA_AGENTIC_AGENT_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(AgentType\u001b[38;5;241m.\u001b[39mOPENAI))\n\u001b[1;32m     51\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/enum.py:385\u001b[0m, in \u001b[0;36mEnumMeta.__call__\u001b[0;34m(cls, value, names, module, qualname, type, start)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03mEither returns an existing member, or creates a new enum class.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m`type`, if set, will be mixed in as the first base class.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# simple value lookup\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_create_(\n\u001b[1;32m    388\u001b[0m         value,\n\u001b[1;32m    389\u001b[0m         names,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         start\u001b[38;5;241m=\u001b[39mstart,\n\u001b[1;32m    394\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/llama_index/lib/python3.10/enum.py:710\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[0;34m(cls, value)\u001b[0m\n\u001b[1;32m    708\u001b[0m ve_exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m is not a valid \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (value, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m))\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ve_exc\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    713\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m._missing_: returned \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m instead of None or a valid member\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    714\u001b[0m             \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, result)\n\u001b[1;32m    715\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: 'ModelProvider.OPENAI' is not a valid ModelProvider"
     ]
    }
   ],
   "source": [
    "from vectara_agentic.agent import Agent\n",
    "\n",
    "agent = Agent.from_corpus(\n",
    "    data_description=\"sentence embeddings\",\n",
    "    assistant_specialty=\"sentence embeddings research\",\n",
    "    tool_name=\"ask_embeddings\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d3e6f1-4abc-4863-8a93-240315110456",
   "metadata": {},
   "source": [
    "This creates an AI assistant with a single RAG query tool called `ask_embeddings`. The main LLM has been instructed that it is an expert in `\"sentence embeddings research\"` and should answer the user's questions about `\"sentence embeddings\"`.\n",
    "\n",
    "The Vectara corpus information (customer id, corpus id, api key) is read directly from the environment, so you don't need to explicitly pass in these credentials.\n",
    "\n",
    "If you just want an AI assistant with a single RAG tool, this is all the work that you need to do. It's that simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09e4c32-8572-4ccd-afef-14868b14c902",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.chat(\"Tell me about the latest innovations in sentence embedding models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ad93e-0692-4ad5-99a1-329788252b03",
   "metadata": {},
   "source": [
    "# Advanced RAG with Vectara and LLamaIndex\n",
    "\n",
    "(MAY DELETE THIS IF WE ONLY WANT TO SHOW PY-VECTARA-AGENTIC)\n",
    "## Agentic RAG\n",
    "\n",
    "LlamaIndex provides various agent implementations such as ChainOfThough or React.\n",
    "\n",
    "To use these with Vectara, you would need to use an external LLM as the driver of the agent resoning, and in this example we will be using OpenAI's GPT4o (for this to work, please make sure you have `OPENAI_API_KEY` defined in your environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92019803-7aaa-4199-a322-692d03b374c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "vectara_tool = QueryEngineTool(\n",
    "    query_engine=index.as_query_engine(\n",
    "        summary_enabled=True,\n",
    "        summary_num_results=5,\n",
    "        summary_response_lang=\"en\",\n",
    "        summary_prompt_name=\"mockingbird-1.0-2024-07-16\",\n",
    "    ),\n",
    "    metadata=ToolMetadata(\n",
    "        name=\"Vectara\",\n",
    "        description=\"Vectara tool that can answer Questions about Embedding Models, NLP, and related topics.\",\n",
    "    ),\n",
    ")\n",
    "agent = ReActAgent.from_tools(\n",
    "    tools=[vectara_tool],\n",
    "    llm=llm,\n",
    "    context=\"\"\"\n",
    "        You are a helpful chatbot that answers any user questions around embedding models in NLP using the Vectara tool.\n",
    "        You break down complex questions into simpler ones and use the vectara tool to answer every question or sub-question.\n",
    "        You use the Vectara tool to help answer the user question.\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    "    max_iterations=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0ae3ea3-76f9-4898-a4bf-1788abb5d53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 89445c46-177a-4d07-938b-ef17201c6a81. Step input: \n",
      "    What are the sentence embedding models? \n",
      "    what are the best sentence embedding models, who created each model and in what year was the paper published?\n",
      "    Compare and contrast their architecture, training procedure and performance\n",
      "\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: Vectara\n",
      "Action Input: {'input': 'What are sentence embedding models?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Sentence embedding models are a type of model that represents a given input sentence using a fixed dimensional vector, independent of the length of the input sentence [4]. These models are trained to obtain state-of-the-art sentence representations for general semantic textual similarity tasks [1]. They have significantly improved performance in numerous downstream NLP tasks such as information retrieval, question answering, and machine translation [4]. Sentence embedding models can be domain-specific, and by injecting domain-specific knowledge of adapters into the model, they can be efficiently adapted for semantic textual similarity tasks in different domains [1]. Examples of sentence embedding models include Sentence-BERT, SimCSE-BERT/SimCSE-RoBERTa, Sentence-T5, and MP-Net [3].\n",
      "\u001b[0m> Running step 4bdb3ef1-d8de-462b-b3d5-defab3e57891. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I need to gather more specific information about the best sentence embedding models, their creators, and publication years.\n",
      "Action: Vectara\n",
      "Action Input: {'input': 'What are the best sentence embedding models, who created each model and in what year was the paper published?'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the provided sources, the best sentence embedding models mentioned are:\n",
      "\n",
      "* ELMO + Attn (Wang et al. 2018)\n",
      "* DSE (Wang et al. 2018)\n",
      "* GenSen (Subramanian et al. 2018)\n",
      "* InferSent (Conneau et al. 2017)\n",
      "* SkipThought (Kiros et al. 2015)\n",
      "\n",
      "The creators of each model are:\n",
      "\n",
      "* ELMO + Attn: Wang et al. (2018)\n",
      "* DSE: Wang et al. (2018)\n",
      "* GenSen: Subramanian et al. (2018)\n",
      "* InferSent: Conneau et al. (2017)\n",
      "* SkipThought: Kiros et al. (2015)\n",
      "\n",
      "The papers were published in the following years:\n",
      "\n",
      "* ELMO + Attn: 2018\n",
      "* DSE: 2018\n",
      "* GenSen: 2018\n",
      "* InferSent: 2017\n",
      "* SkipThought: 2015\n",
      "\n",
      "Note that the sources do not provide a comprehensive list of all sentence embedding models, but rather mention these specific models in the context of comparisons and evaluations.\n",
      "\u001b[0m> Running step d5fb743b-15e9-4898-9007-b3e56229882c. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I need to compare and contrast the architecture, training procedure, and performance of the mentioned sentence embedding models.\n",
      "Action: Vectara\n",
      "Action Input: {'input': 'Compare and contrast the architecture, training procedure and performance of ELMO + Attn, DSE, GenSen, InferSent, and SkipThought models.'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Here is a summary of the comparison and contrast of the architecture, training procedure, and performance of ELMO + Attn, DSE, GenSen, InferSent, and SkipThought models based on the provided sources:\n",
      "\n",
      "**Architecture:**\n",
      "\n",
      "* ELMO + Attn employs CA (Compositional Attention) operations [1][4].\n",
      "* DSE is based on a larger transformer architecture and robust BERT pre-training [3].\n",
      "* GenSen is a sentence embedding model [5].\n",
      "* InferSent and SkipThought are not explicitly described in terms of architecture.\n",
      "\n",
      "**Training Procedure:**\n",
      "\n",
      "* ELMO + Attn has two variants: Single-Task (ST) and Multi-Task (MT) Training [5].\n",
      "* DSE is trained in a single-task manner [2].\n",
      "* GenSen is also a sentence embedding model, but its training procedure is not specified [5].\n",
      "* InferSent and SkipThought are not explicitly described in terms of training procedure.\n",
      "\n",
      "**Performance:**\n",
      "\n",
      "* DSE significantly outperforms ELMO + Attn (ST) across all datasets, with an average relative improvement of 15.6% [1][4].\n",
      "* DSE also outperforms GenSen across all datasets, with an average relative improvement of 8.9% [2].\n",
      "* DSE is competitive with InferSent and GenSen, outperforming them on 7 and 3 datasets, respectively [3].\n",
      "* BERT-Large embeddings are outperformed by DSE on 8 datasets [3].\n",
      "\n",
      "Overall, DSE is the best-performing model among the compared models, with significant improvements over ELMO + Attn and GenSen, and competitive performance with InferSent and SkipThought.\n",
      "\u001b[0m> Running step 78838c10-87b0-4d80-ae18-a1d33b02bb83. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer.\n",
      "Answer: Sentence embedding models are designed to represent sentences as fixed-dimensional vectors, which are useful for various NLP tasks. Among the top models, ELMO + Attn, DSE, GenSen, InferSent, and SkipThought have been highlighted.\n",
      "\n",
      "- **ELMO + Attn** and **DSE** were both created by Wang et al. in 2018. ELMO + Attn uses Compositional Attention operations and can be trained in Single-Task or Multi-Task modes. DSE, which utilizes a robust transformer architecture, is trained in a single-task manner and has shown significant performance improvements over other models.\n",
      "\n",
      "- **GenSen**, created by Subramanian et al. in 2018, and **InferSent**, by Conneau et al. in 2017, are both influential models, though specific details on their architecture and training procedures were not fully detailed in the sources.\n",
      "\n",
      "- **SkipThought**, developed by Kiros et al. in 2015, is another key model, though specific details about its architecture and training were not provided.\n",
      "\n",
      "In terms of performance, DSE stands out as the best-performing model, showing significant improvements over ELMO + Attn and GenSen, and competitive performance against InferSent and SkipThought.\n",
      "\u001b[0mSentence embedding models are designed to represent sentences as fixed-dimensional vectors, which are useful for various NLP tasks. Among the top models, ELMO + Attn, DSE, GenSen, InferSent, and SkipThought have been highlighted.\n",
      "\n",
      "- **ELMO + Attn** and **DSE** were both created by Wang et al. in 2018. ELMO + Attn uses Compositional Attention operations and can be trained in Single-Task or Multi-Task modes. DSE, which utilizes a robust transformer architecture, is trained in a single-task manner and has shown significant performance improvements over other models.\n",
      "\n",
      "- **GenSen**, created by Subramanian et al. in 2018, and **InferSent**, by Conneau et al. in 2017, are both influential models, though specific details on their architecture and training procedures were not fully detailed in the sources.\n",
      "\n",
      "- **SkipThought**, developed by Kiros et al. in 2015, is another key model, though specific details about its architecture and training were not provided.\n",
      "\n",
      "In terms of performance, DSE stands out as the best-performing model, showing significant improvements over ELMO + Attn and GenSen, and competitive performance against InferSent and SkipThought.\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"\n",
    "    What are the sentence embedding models? \n",
    "    what are the best sentence embedding models, who created each model and in what year was the paper published?\n",
    "    Compare and contrast their architecture, training procedure and performance\n",
    "\"\"\"\n",
    "\n",
    "print(agent.chat(question).response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43963c4",
   "metadata": {},
   "source": [
    "## Using Auto Retriever with Vectara\n",
    "\n",
    "LlamaIndex's auto-retriever functionality is really cool. \n",
    "It is most useful when you have metadata fields (like in our case of papers from Arxiv), and would like a query that references a metadata field to be automatically interpreted in the right way.\n",
    "\n",
    "For example, if I ask \"what is a paper about climate change risks published after 2020\", the auto-retriever would (behind the scences) interpret ths into a query \"what is a paper about climate change risks\" along with a filter condition of \"published > 2020\"\n",
    "\n",
    "Let's see how this works with the Vectara Index.\n",
    "First, we have to define a `VectorStoreInfo` structure that defines the meta data fields the auto-retriever knows about to do its job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24cc7017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores.types import MetadataInfo, VectorStoreInfo\n",
    "\n",
    "vector_store_info = VectorStoreInfo(\n",
    "    content_info=\"information about a paper\",\n",
    "    metadata_info=[\n",
    "        MetadataInfo(\n",
    "            name=\"published\",\n",
    "            description=\"The date the paper was published\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"author\",\n",
    "            description=\"The author of the paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"title\",\n",
    "            description=\"The title of the papers\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "        MetadataInfo(\n",
    "            name=\"url\",\n",
    "            description=\"The URL for this paper\",\n",
    "            type=\"string\",\n",
    "        ),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23712d46",
   "metadata": {},
   "source": [
    "Auto-retrieval is implemented before calling Vectara as a query transformation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585f0333",
   "metadata": {},
   "source": [
    "Now we can define the `VectaraAutoRetriever`, which can perform auto-retrieval using Vectara:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92de30c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding?\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "\n",
    "retriever = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "res = retriever.retrieve(\"What is sentence embedding, based on papers before 2019?\")\n",
    "[(r.metadata['published'], r.text) for r in res]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7450bc",
   "metadata": {},
   "source": [
    "As you can see, the Auto Retriever was able to translate the natural language text into a shorter query and a proper condition (in this case `doc.published < 2019`).\n",
    "\n",
    "We can also of course ask a question directly: we use the `VectaraQueryEngine` which can work with the `VectaraAutoRetriever` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "327ffbce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using query str: What is sentence embedding\n",
      "Using implicit filters: [('published', '<', '2019')]\n",
      "final filter string: (doc.published < '2019')\n",
      "Sentence embedding is a crucial technique in Natural Language Processing (NLP) that transforms sentences into low-dimensional vector representations to capture their semantic meanings. Various models have been developed to create these embeddings, enhancing the performance of NLP tasks like machine translation, document classification, and sentiment analysis. These models aim to preserve the original sentence meanings effectively in the embedded vectors, allowing for improved analysis and classification of text data.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.indices.managed.vectara.query import VectaraQueryEngine\n",
    "from llama_index.indices.managed.vectara import VectaraAutoRetriever\n",
    "\n",
    "ar = VectaraAutoRetriever(\n",
    "    index,\n",
    "    vector_store_info=vector_store_info,\n",
    "    llm=llm,\n",
    "    summary_enabled=True,\n",
    "    summary_num_results=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "query_engine = VectaraQueryEngine(retriever=ar)\n",
    "response = query_engine.query(\"What is sentence embedding, based on papers before 2019?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba5faef-9fe9-4acc-a55d-ad7c379697fd",
   "metadata": {},
   "source": [
    "## Advanced querying with QueryFusionRetriever\n",
    "\n",
    "The QueryFusion [Retriever](https://docs.llamaindex.ai/en/stable/examples/retrievers/reciprocal_rerank_fusion.html#reciprocal-rerank-fusion-retriever) is an advanced query mechanism whereby the original query is pre-processed to generate N variations. Each of these rephrased queries is then run against the Vectara engine and rank-fusion is used to combine the best results. \n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c662bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT employs a siamese network architecture, which involves two parallel encoders that process two inputs independently. The outputs are then compared or combined in some way, typically to compute similarity or difference. This is distinct from a dual-encoder architecture where the focus is on mapping inputs to a unified embedding space for direct comparison or retrieval tasks.\n"
     ]
    }
   ],
   "source": [
    "query = \"is SBERT a dual encoder? what type of DL architecture does it use?\"\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    summary_enabled=False,\n",
    "    llm=llm,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "896e8b7e-c028-4ec6-bb7c-0c05dfb86321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. What is SBERT and how does it work as a dual encoder?\n",
      "2. Comparison of SBERT with other dual encoder models in natural language processing.\n",
      "3. Deep learning architecture used in SBERT for sentence embeddings.\n",
      "4. Advantages and limitations of using a dual encoder like SBERT in NLP tasks.\n",
      "SBERT is not a dual encoder. It uses a single embedding space instead of a separate embedding space, which means it does not employ a dual-encoder architecture.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "import nest_asyncio\n",
    "\n",
    "rf_retriever = QueryFusionRetriever(\n",
    "    [index.as_retriever(similarity_top_k=2)],\n",
    "    similarity_top_k=2,\n",
    "    num_queries=5,  # this includes the origianl query; set this to 1 to disable query generation\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    use_async=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "nest_asyncio.apply()     # apply nested async to run in a notebook\n",
    "query_engine = RetrieverQueryEngine.from_args(rf_retriever)\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a6b3fd",
   "metadata": {},
   "source": [
    "We can see how the QueryFusionRetriever created additional query variations (they are displayed since we used `verbose=True`) and then the overall response includes the results fused together. This is very helpful in this case because the QueryFusionRetriever creates sub-questions that inquire about the specific architecture of SBERT which is relevant context to answering this question properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c98981",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we've seen various examples for using Vectara with LlamaIndex, which provides the following benefits:\n",
    "* Vectara provides a complete RAG pipeline, so you don't have to deal with a lot of the details around data ingestion: pre-processing, chunking, embedding, etc. Instead all these steps are handled automatically and efficiently in Vectara. \n",
    "* Being a platform, Vectara uses its own internal Embedding model (Boomerang), its own vector storage, and its own LLM (Mockingbird) for summarization, so you don't have to maintain separate API keys and relationships with additional vendors or install other products.\n",
    "* Vectara is built for large scale GenAI applications, and with the tools provided by LlamaIndex like Auto Retrieval and Query Fusion, you can easily build and test advanced RAG applications at enteprise scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
