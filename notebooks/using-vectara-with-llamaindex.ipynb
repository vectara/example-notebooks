{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf7d63d",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/vectara/example-notebooks/blob/main/notebooks/using-vectara-with-llamaindex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bea86",
   "metadata": {},
   "source": [
    "# Vectara and LlamaIndex\n",
    "\n",
    "In this notebook we are going to show how to use Vectara with LlamaIndex.\n",
    "\n",
    "Please note: We have updated our integration with LlamaIndex (llama-index-indices-managed-vectara version >= 0.4.0) to use our API v2, so some of the parameters and credentials required for our integration have changed. The latest changes are reflected in this example notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0855d0",
   "metadata": {},
   "source": [
    "## About Vectara\n",
    "\n",
    "[Vectara](https://vectara.com/) is the trusted AI Assistant and Agent platform which focuses on enterprise readiness for mission-critical applications. \n",
    "\n",
    "Vectara provides an end-to-end managed service for Retrieval Augmented Generation or [RAG](https://vectara.com/grounded-generation/), which includes:\n",
    "\n",
    "1. An integrated API for processing input data, including text extraction from documents and ML-based chunking.\n",
    "\n",
    "2. The state-of-the-art [Boomerang](https://vectara.com/how-boomerang-takes-retrieval-augmented-generation-to-the-next-level-via-grounded-generation/) embeddings model. Each text chunk is encoded into a vector embedding using Boomerang, and stored in the Vectara internal knowledge (vector+text) store. Thus, when using Vectara with LlamaIndex you do not need to call a separate embedding model - this happens automatically within the Vectara backend.\n",
    "\n",
    "3. A query service that automatically encodes the query into embeddings and retrieves the most relevant text segmentsthrough [hybrid search](https://docs.vectara.com/docs/api-reference/search-apis/lexical-matching) and a variety of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking) strategies, including a [multilingual reranker](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker), [maximal marginal relevance (MMR) reranker](https://docs.vectara.com/docs/learn/mmr-reranker), [user-defined function reranker](https://docs.vectara.com/docs/learn/user-defined-function-reranker), and a [chain reranker](https://docs.vectara.com/docs/learn/chain-reranker) that provides a way to chain together multiple reranking methods to achieve better control over the reranking, combining the strengths of various reranking methods.\n",
    "\n",
    "4. An option to create a [generative summary](https://docs.vectara.com/docs/learn/grounded-generation/grounded-generation-overview) with a wide selection of LLM summarizers (including Vectara's [Mockingbird](https://vectara.com/blog/mockingbird-is-a-rag-specific-llm-that-beats-gpt-4-gemini-1-5-pro-in-rag-output-quality/), trained specifically for RAG-based tasks), based on the retrieved documents, including citations.\n",
    "\n",
    "See the [Vectara API documentation](https://docs.vectara.com/docs/) for more information on how to use the API.\n",
    "\n",
    "The main benefits of using Vectara RAG-as-a-service to build your application are:\n",
    "* **Accuracy and Quality**: Vectara provides an end-to-end platform that focuses on eliminating hallucinations, reducing bias, and safeguarding copyright integrity.\n",
    "* **Security**: Vectara's platform provides acess control--protecting against prompt injection attacks--and meets SOC2 and HIPAA compliance.\n",
    "* **Explainability**: Vectara makes it easy to troubleshoot bad results by clearly explaining rephrased queries, LLM prompts, retrieved results, and agent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33079b",
   "metadata": {},
   "source": [
    "## About LlamaIndex\n",
    "\n",
    "LlamaIndex is a \"data framework\" to help you build LLM apps:\n",
    "\n",
    "1. It includes **data connectors** to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.)\n",
    "2. It provides ways to **structure your data** (indices, graphs) so that this data can be easily used with LLMs.\n",
    "3. It provides an **advanced retrieval/query interface over your data**: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\n",
    "\n",
    "LlamaIndex's high-level API allows beginner users to use LlamaIndex to ingest and query their data in just a few lines of code, whereas its lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.\n",
    "\n",
    "Vectara is implemented in LlamaIndex as a [Managed Service](https://docs.llamaindex.ai/en/stable/community/integrations/managed_indices.html#vectara), abstracting all of Vectara's powerful API so they are easily integrated into LlamaIndex.\n",
    "\n",
    "In this notebook, we will demonstrate some of the great ways you can use Vectara together with LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2497c",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "If you're opening this Notebook on colab, you will probably need to install LlamaIndex ðŸ¦™.\n",
    "\n",
    "To get started with Vectara, [sign up](https://console.vectara.com/signup?utm_source=vectara&utm_medium=signup&utm_term=DevRel&utm_content=example-notebooks&utm_campaign=vectara-signup-DevRel-example-notebooks) (if you haven't already) and follow our [quickstart](https://docs.vectara.com/docs/quickstart) guide to create a corpus and an API key. \n",
    "\n",
    "Once you have these, you can provide them as environment variables or explicitly define them in the code cell below. These will be used by the LlamaIndex code later on to connect the index to your Vectara corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6019e01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U llama-index llama-index-indices-managed-vectara arxiv\n",
    "\n",
    "import os\n",
    "# os.environ['VECTARA_API_KEY'] = \"<YOUR_VECTARA_API_KEY>\"\n",
    "# os.environ['VECTARA_CORPUS_KEY'] = \"<YOUR_VECTARA_CORPUS_KEY>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7010b1d-d1bb-4f08-9309-a328bb4ea396",
   "metadata": {},
   "source": [
    "## Loading Data Into Vectara\n",
    "\n",
    "As mentioned above, Vectara is a RAG managed service, and in many cases data may be uploaded to the index ahead of time (e.g. by using [Airbyte](https://docs.airbyte.com/integrations/destinations/vectara), directly via Vectara's [indexing API](https://docs.vectara.com/docs/api-reference/indexing-apis/indexing) or using tools like [vectara-ingest](https://github.com/vectara/vectara-ingest)), but another easy way is via the VectaraIndex constructor: `from_documents()`.\n",
    "\n",
    "For this notebook, we will assume the Vectara corpus is empty and will load PDF documents from Arxiv, using Python's [arxiv](https://github.com/lukasschwab/arxiv.py) library. We will pull in data from the top papers related to \"embedding model\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40947545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"(ti:embedding model) OR (ti:sentence embedding)\",\n",
    "  max_results = 100,\n",
    "  sort_by = arxiv.SortCriterion.Relevance\n",
    ")\n",
    "papers = list(client.results(search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae7bd09-6569-48cc-8c54-8bf491c0e656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://arxiv.org/abs/2402.14776v3',\n",
       " 'http://arxiv.org/abs/2007.01852v2',\n",
       " 'http://arxiv.org/abs/1910.13291v1',\n",
       " 'http://arxiv.org/abs/2104.06719v1',\n",
       " 'http://arxiv.org/abs/1511.08198v3']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[p.entry_id for p in papers][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c163ade",
   "metadata": {},
   "source": [
    "Next, download the Arxiv paper, and upload them into Vectara using the `add_file()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c154dd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File temp/1909.03104v2.Efficient_Sentence_Embedding_using_Discrete_Cosine_Transform.pdf failed to load with error HTTP Error 404: Not Found\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from llama_index.indices.managed.vectara import VectaraIndex\n",
    "\n",
    "data_folder = 'temp'\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Create Vectara Index\n",
    "index = VectaraIndex()\n",
    "\n",
    "# Upload content for all papers\n",
    "for paper in papers:\n",
    "    try:\n",
    "        paper_fname = paper.download_pdf(data_folder)\n",
    "    except Exception as e:\n",
    "        print(f\"File {paper_fname} failed to load with error {e}\")\n",
    "        continue\n",
    "    metadata = {\n",
    "        'url': paper.pdf_url,\n",
    "        'title': paper.title,\n",
    "        'author': str(paper.authors[0]),\n",
    "        'published': str(paper.published.date())\n",
    "    }\n",
    "    index.insert_file(file_path=paper_fname, metadata=metadata)\n",
    "\n",
    "shutil.rmtree(data_folder)\n",
    "del papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94ea571",
   "metadata": {},
   "source": [
    "Two important things to note here:\n",
    "1. Vectara processes each file uploaded on the backend, and performs appropriate chunking. So you don't need to apply any local processing, or choose a chunking strategy. If you would like to have more control over how these documents are indexed, such as specifying the maximum number of characters per chunk, you can add the `chunking_strategy` argument to the `insert_file()` function used in the above cell.\n",
    "2. We have used the fields `url`, `title`, `author`, and `published` as metadata fields (for simplicity, author is the first author if there are multiple). You will need to make sure those fields are defined in your Vectara corpus as [filterable metadata fields](https://docs.vectara.com/docs/learn/metadata-search-filtering/filter-overview) to ensure we can filter by them in query time.\n",
    "\n",
    "So that's it for upload. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee4473a-094f-4d0a-a825-e1213db07240",
   "metadata": {},
   "source": [
    "## Querying with the VectaraIndex\n",
    "We can now ask questions using the `VectaraIndex` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb174ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is sentence embedding?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21facbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, here is a summary that answers the query \"What is sentence embedding?\":\n",
      "\n",
      "Sentence embedding is a technique that represents words or sentences in a text in a n-dimensional space, encoding the meaning of the word or sentence [2]. It is a vital technique in practical applications, enabling the extraction of parallel data from extensive web corpora across two languages, enhancing the training of high-quality machine translation and multilingual language models [1]. Sentence embedding is also instrumental for zero-shot cross-lingual retrieval, an essential method for enabling cross-lingual searches on e-commerce platforms like Amazon [1]. In the context of machine learning, sentence embedding is a form of word or sentence representation by learned representations that prepare texts in an understandable format for a machine [2].\n",
      "\n",
      "Sources: [1], [2]\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True, summary_num_results=5,\n",
    "    summary_response_lang=\"eng\",\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\"\n",
    ")\n",
    "res = query_engine.query(query)\n",
    "print(res.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52878fd2",
   "metadata": {},
   "source": [
    "Note that the response here is fully generated by Vectara. There is no additional LLM involved (or API key you need to setup). The response also includes citations (marked in square brackets), which provide links to references used to generate this response by Vectara. \n",
    "<br>\n",
    "The `res` object includes the actual response to the user query, but also has the citations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b110a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (1, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (2, 'http://arxiv.org/pdf/2305.03010v1'),\n",
       " (3, 'http://arxiv.org/pdf/2305.03010v1'),\n",
       " (4, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (5, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (6, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (7, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (8, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (9, 'http://arxiv.org/pdf/1904.05542v1')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(res.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cce7c7-c664-4001-8c84-f80c5d499796",
   "metadata": {},
   "source": [
    "## Using Streaming\n",
    "\n",
    "You can also stream the Vectara response simply by specifying `streaming=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abd30027-d975-4440-a1c2-66d1bd30bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided sources, here is a summary that answers the query \"What is sentence embedding?\":\n",
      "\n",
      "Sentence embedding is a technique that represents words or sentences in a text in a way that encodes their meaning in n-dimensional space [2]. It is a vital technique in practical applications, enabling the extraction of parallel data from extensive web corpora across two languages, enhancing the training of high-quality machine translation and multilingual language models [1]. Sentence embedding is also instrumental for zero-shot cross-lingual retrieval, an essential method for enabling cross-lingual searches on e-commerce platforms like Amazon [1].\n",
      "\n",
      "In the context of machine learning, sentence embedding is a form of word or sentence representation by learned representations that prepare texts in an understandable format for a machine [2]. It maps text data into vectors that can be a set of real numbers (a vector), where words or sentences that are closer in the vector space are more similar [2].\n",
      "\n",
      "There are different approaches to sentence embedding, including traditional word embedding, static word embedding, contextualized word embedding, and two-sentence embeddings approach, with non-parameterized and parameterized models [2].\n",
      "\n",
      "Sources: [1], [2]"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    summary_enabled=True,\n",
    "    summary_prompt_name=\"mockingbird-1.0-2024-07-16\",\n",
    "    streaming=True)\n",
    "\n",
    "res = query_engine.query(query)\n",
    "\n",
    "# print streamed output chunk by chunk\n",
    "res.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cd598-ce73-4c9e-9aa6-5d8604fd6c14",
   "metadata": {},
   "source": [
    "## Reranking\n",
    "\n",
    "Vectara supports three types of [reranking](https://docs.vectara.com/docs/api-reference/search-apis/reranking):\n",
    "1. [Maximal Marginal Relevance](https://docs.vectara.com/docs/learn/mmr-reranker), or MMR, provides a reranking that can promote diversity in results at the cost of relevance.\n",
    "2. [Slingshot](https://docs.vectara.com/docs/learn/vectara-multi-lingual-reranker) is a mulitilingual reranker that increases the accuracy of retrieved results across 100+ languages and is available to Vectara Scale customers.\n",
    "3. [User Defined Functions](https://docs.vectara.com/docs/learn/user-defined-function-reranker) allow you to create your own functions for reranking search results, unlocking better retrieval in a wide variety of use cases, such as sorting by recency or price of a product."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e49914a",
   "metadata": {},
   "source": [
    " Let's see an example of how to use MMR: We will run the same query but this time we will use MMR where `mmr_diversity_bias=0.3` provides a tradeoff between relevance and diversity (0.0 is full relevance, 1.0 is only diversity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72832e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a technique used in natural language processing to represent sentences as vectors in an n-dimensional space. This representation encodes the meaning of the sentence, allowing for the comparison of semantic similarity between sentences. Sentence embeddings are crucial for various applications, such as machine translation, multilingual language models, and cross-lingual retrieval tasks. They enable efficient processing and understanding of text data by machines, facilitating tasks like document classification and sentiment analysis [1], [3], [5].\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    reranker=\"mmr\",\n",
    "    rerank_k=50,\n",
    "    mmr_diversity_bias=0.3,\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17020d34-5176-4910-bb88-5c5685513804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (1, 'http://arxiv.org/pdf/2205.15744v2'),\n",
       " (2, 'http://arxiv.org/pdf/2206.02690v3'),\n",
       " (3, 'http://arxiv.org/pdf/1904.05542v1'),\n",
       " (4, 'http://arxiv.org/pdf/1808.05505v3')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['url']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e76fd1",
   "metadata": {},
   "source": [
    "As you can see, the results are now reranked in a way that provides more diversity instead of maximizing pure relevance. This in turn results in a different set of chunks used to generate the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562956f5-b9b7-4288-8100-f443a5dc3e4d",
   "metadata": {},
   "source": [
    "Now let's see an example with a user defined function. We may be interested in getting results that are the most semantically similar to our question, but we also want the most up-to-date information. Thus, we can bias our search results so that the papers that are not only semantically similar but also published more recently are used to answer our query. We can do this by using the available time functions (to see other built-in functions, see the UDF Reranker [documentation](https://docs.vectara.com/docs/learn/user-defined-function-reranker)).\n",
    "\n",
    "Vectara also supports chain-reranking, which provides a way to chain together multiple reranking methods to achieve better control over the reranking, and combining the strengths of various reranking methods. A great way to use the UDF reranker is in a chain: first the multilingual reranker, followed by the maximal marginal relevance (MMR) reranker, and then a user-defined function, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d32ff2f1-ccd2-4fd0-99df-fd977182df41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent innovations in sentence embedding models include the development of models like HiT, which learns the hierarchical semantic structure in language models and is trained on WordNet to provide unseen subsumptions and hypernyms for words in sentences. Sentence-BERT and gte-base models have also been recognized for their effectiveness in unsupervised text retrieval tasks. Additionally, there is a focus on improving cross-lingual model alignment by incorporating parallel training datasets, which is particularly beneficial for low-resource languages. These advancements highlight the importance of sentence embedding models in applications such as Bi-text Mining, Information Retrieval, and Retrieval Augmented Generation [1], [2].\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k = 50,\n",
    "    reranker=\"chain\",\n",
    "    rerank_chain=[\n",
    "        {\n",
    "            \"type\": \"slingshot\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"mmr\",\n",
    "            \"diversity_bias\": 0.3\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"userfn\",\n",
    "            \"user_function\": \"max(0, 5 * get('$.score') - hours(seconds((to_unix_timestamp(now()) - to_unix_timestamp(datetime_parse(get('$.document_metadata.published'), 'yyyy-MM-dd'))))) / 24 / 365)\",\n",
    "            \"limit\": 5\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What innovations have been made to sentence embedding models?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01b96eb2-9e88-4c6a-90c4-676dddcc5ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '2024-11-25'),\n",
       " (1, '2024-12-04'),\n",
       " (2, '2024-11-25'),\n",
       " (3, '2024-11-25'),\n",
       " (4, '2024-05-30')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(inx, n.node.metadata['published']) for inx, n in enumerate(response.source_nodes)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccb7457-50e2-4b24-8416-daf65e9a4d5c",
   "metadata": {},
   "source": [
    "Notice how many of the papers used to generate the final summary were published recently, and they still give us information to generate a relevant response that answers our question.\n",
    "\n",
    "Also notice how we use a max() function with 0 in our user-defined expression. This is to ensure that all of our reranking scores are non-negative. Additionally, since we multiplied the original score by 10 and its value ranges from 0 to 1, we throw away any search results that are older than 10 years old for generating our final response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863b914",
   "metadata": {},
   "source": [
    "So far we've used Vectara's internal summarization capability, which is the best way for most users.\n",
    "\n",
    "You can still use Llama-Index's standard VectorStore `as_query_engine()` method, in which case Vectara's summarization won't be used, and you would be using an external LLM (e.g. OpenAI's GPT-4) and a custom prompt from LlamaIndex to generate the summary. For this option just set `summary_enabled=False`\n",
    "\n",
    "For this functionality, you will need to specify your own OpenAI API key in the environment:\n",
    "\n",
    "> `os.environ['OPENAI_API_KEY'] = '<YOUR_OPENAI_API_KEY>'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b0a49d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embedding is a technique used in machine learning to represent sentences as vectors in a high-dimensional space. These vectors are designed to capture the semantic meaning of the sentences, allowing machines to understand and process text data more effectively. Sentence embeddings are used in various natural language processing tasks such as machine translation, text classification, and information retrieval. The goal is for sentences with similar meanings to be close to each other in the vector space, despite being composed of different words or structures. This representation facilitates the comparison and manipulation of textual data by algorithms.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-4-turbo\", temperature=0)\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    summary_enabled=False,\n",
    "    llm=llm\n",
    ")\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebad3cf-b09e-4af5-be3d-ba2a3e3e8d73",
   "metadata": {},
   "source": [
    "## Using Vectara Chat\n",
    "\n",
    "Vectara now fully supports Chat in its platform, where the chat history is maintained by Vectara and so you don't have to worry about keeping history and integrating it with your RAG pipeline. \n",
    "\n",
    "To use it, simply call `as_chat_engine()`.\n",
    "\n",
    "(Chat mode always uses Vectara's summarization so you don't have to explicitly specify `summary_enabled=True` like before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42af08f5-dc13-4991-96f4-fdd294b0ce97",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e3368be-46a3-4b2f-9589-de3383c65201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a sentence embedding model?\n",
      "\n",
      "Response: A sentence embedding model is a type of model that represents a given input sentence as a fixed-dimensional vector, regardless of the sentence's length. These embeddings are particularly useful in various natural language processing (NLP) tasks, such as information retrieval, question answering, and machine translation. Sentence embeddings have shown significant improvements in performance for these tasks compared to static word embeddings, which typically have fewer dimensions [4].\n",
      "\n",
      "Question: What are some known models?\n",
      "\n",
      "Response: Some known sentence embedding models include HiT, Sentence-BERT, and gte-base, which are effective in unsupervised text retrieval tasks [1]. Other models include GenSen and DSE, which have been compared across different test sets [4]. Additionally, InferSent, with its two versions based on GloVe and fastText word embeddings, and the Universal Sentence Encoder (USE), which comes in versions based on Deep Average Networks and Transformer networks, are also well-known sentence embedding models [7].\n",
      "\n",
      "Question: How are they different than token embedding models\n",
      "\n",
      "Response: Sentence embedding models and token embedding models differ primarily in their focus and application. Sentence embedding models aim to generate a single vector representation for an entire sentence, capturing its overall meaning. This is often achieved by pooling word or token embeddings, such as averaging them, and then fine-tuning the resulting representation for specific tasks or general purposes [1]. In contrast, token embedding models focus on representing individual words or tokens within a sentence, often used in tasks that require understanding the role of each word in context, such as translation language modeling [2]. Sentence embeddings are typically used for tasks like semantic textual similarity, where the goal is to compare the meanings of entire sentences [6].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    'What is a sentence embedding model?',\n",
    "    'What are some known models?',\n",
    "    'How are they different than token embedding models'\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Question: {q}\\n\")\n",
    "    response = ce.chat(q).response\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e111b1fa-5efc-4ee6-99f0-21c73dbd5433",
   "metadata": {},
   "source": [
    "Of course streaming works as well with Chat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8b24622-bc73-4bee-929e-377f06335a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce = index.as_chat_engine(streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8f16016-be7c-411f-aafd-b03d8816d21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SBERT, or Sentence-BERT, was developed by Nils Reimers and Iryna Gurevych in 2019. It is based on transformer models like BERT, which was introduced by Devlin et al. in 2018, and fine-tunes these models using a siamese network structure to create sentence embeddings efficiently [3]."
     ]
    }
   ],
   "source": [
    "response = ce.stream_chat(\"Who is behind SBERT?\")\n",
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01ad93e-0692-4ad5-99a1-329788252b03",
   "metadata": {},
   "source": [
    "# Advanced RAG with Vectara and LLamaIndex\n",
    "\n",
    "## Agentic RAG\n",
    "\n",
    "Vectara also has its own package, [vectara-agentic](https://github.com/vectara/py-vectara-agentic), built on top of many features from LlamaIndex to easily implement agentic RAG applications. It allows you to create your own AI assistant with RAG query tools and other custom tools, such as making API calls to retrieve information from financial websites. You can find the full documentation for vectara-agentic [here](https://vectara.github.io/vectara-agentic-docs/).\n",
    "\n",
    "Let's create a ReAct Agent with a single RAG tool using vectara-agentic (to create a ReAct agent, specify `VECTARA_AGENTIC_AGENT_TYPE` as `\"REACT\"` in your environment).\n",
    "\n",
    "Vectara does not yet have an LLM capable of acting as an agent for planning and tool use, so we will need to use another LLM as the driver of the agent resoning.\n",
    "\n",
    "In this demo, we are using OpenAI's GPT4o. Please make sure you have `OPENAI_API_KEY` defined in your environment or specify another LLM with the corresponding key (for the full list of supported LLMs, check out our [documentation](https://vectara.github.io/vectara-agentic-docs/introduction.html#try-it-yourself) for setting up your environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "648b415f-303a-4d0a-aef1-9d58aa6380ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U vectara-agentic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c77fd093-522d-435e-954e-f571a16a64bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to set up observer (No module named 'phoenix.otel'), ignoring\n",
      "> Running step d739bbed-4386-4da0-91ba-7fbe877e38b2. Step input: Tell me about the latest innovations in sentence embedding models.\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question.\n",
      "Action: ask_embeddings\n",
      "Action Input: {'query': 'latest innovations in sentence embedding models'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: \n",
      "                    Response: '''The latest innovations in sentence embedding models include HiT, Sentence-BERT, and gte-base models, which have achieved notable breakthroughs in fine-tuning scenarios [2]. However, there is a research gap in computationally efficient direct inference methods for sentence representation [1]. Recent studies have shown that sentence embedding models with both lower alignment and uniformity achieve better performance in general [3]. Additionally, contrastive learning has been applied to sentence embedding models, leading to performance improvements, as demonstrated in Japanese language models [5]. Furthermore, the use of large language models such as LLaMA and Mistral has also contributed to advancements in sentence embedding [1].'''\n",
      "                    References:\n",
      "                    [1]: PTEX.Fullbanner='This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'; CreationDate='D:20240516003614Z'; Keywords=''; Producer='pdfTeX-1.40.25'; Author=''; Title=''; Creator='LaTeX with hyperref'; ModDate='D:20240516003614Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2404.03921v2'; title='Simple Techniques for Enhancing Sentence Embeddings in Generative Language Models'; author='Bowen Zhang'; published='2024-04-05'; framework='llama_index'.\n",
      "[2]: PTEX.Fullbanner='This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5'; CreationDate='D:20241126022050Z'; Keywords=''; Producer='pdfTeX-1.40.25'; Author=''; Title=''; Creator='LaTeX with hyperref'; ModDate='D:20241126022050Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2411.16236v1'; title='Introduction'; author='Hong Liu'; published='2024-11-25'; framework='llama_index'.\n",
      "[3]: PTEX.Fullbanner='This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2'; CreationDate='D:20220426000538Z'; Keywords=''; Producer='pdfTeX-1.40.21'; Author=''; Title=''; Creator='LaTeX with hyperref'; ModDate='D:20220426000538Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2204.10931v1'; title='arXiv:2204.10931v1  [cs.CL]  22 Apr 2022'; author='Miaoran Zhang'; published='2022-04-22'; framework='llama_index'.\n",
      "[5]: PTEX.Fullbanner='This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2'; CreationDate='D:20230120013023Z'; Keywords=''; Producer='pdfTeX-1.40.21'; Author='Zihao Chen; Hisashi Handa; Kimiaki Shirahama; '; Title='JCSE: Contrastive Learning of Japanese Sentence Embeddings and Its Applications'; Creator='LaTeX with hyperref'; ModDate='D:20230120013023Z'; Trapped='/False'; Subject=''; url='http://arxiv.org/pdf/2301.08193v1'; title='JCSE: Contrastive Learning of Japanese Sentence Embeddings and Its Applications'; author='Zihao Chen'; published='2023-01-19'; framework='llama_index'.\n",
      "\n",
      "                \n",
      "\u001b[0m> Running step 22bc039e-1a53-451f-9c21-0f8faa0aea00. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: Recent innovations in sentence embedding models include the development of models like HiT, Sentence-BERT, and gte-base, which have made significant advancements in fine-tuning scenarios. However, there is still a need for more computationally efficient direct inference methods for sentence representation. Studies have shown that models with lower alignment and uniformity tend to perform better. Additionally, contrastive learning has been applied to improve performance, as seen in Japanese language models. The use of large language models such as LLaMA and Mistral has also contributed to advancements in this field. For more details, you can refer to the following sources: [1](http://arxiv.org/pdf/2404.03921v2), [2](http://arxiv.org/pdf/2411.16236v1), [3](http://arxiv.org/pdf/2204.10931v1), and [5](http://arxiv.org/pdf/2301.08193v1).\n",
      "\u001b[0mTime taken: 12.01492714881897\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Recent innovations in sentence embedding models include the development of models like HiT, Sentence-BERT, and gte-base, which have made significant advancements in fine-tuning scenarios. However, there is still a need for more computationally efficient direct inference methods for sentence representation. Studies have shown that models with lower alignment and uniformity tend to perform better. Additionally, contrastive learning has been applied to improve performance, as seen in Japanese language models. The use of large language models such as LLaMA and Mistral has also contributed to advancements in this field. For more details, you can refer to the following sources: [1](http://arxiv.org/pdf/2404.03921v2), [2](http://arxiv.org/pdf/2411.16236v1), [3](http://arxiv.org/pdf/2204.10931v1), and [5](http://arxiv.org/pdf/2301.08193v1)."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from vectara_agentic.agent import Agent\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "agent = Agent.from_corpus(\n",
    "    data_description=\"sentence embeddings\",\n",
    "    assistant_specialty=\"sentence embeddings research\",\n",
    "    tool_name=\"ask_embeddings\",\n",
    "    vectara_summary_num_results=5,\n",
    "    vectara_summarizer=\"mockingbird-1.0-2024-07-16\",\n",
    "    vectara_reranker=\"mmr\",\n",
    "    vectara_rerank_k=50,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "response = agent.chat(\n",
    "    \"Tell me about the latest innovations in sentence embedding models.\"\n",
    ")\n",
    "\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c98981",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we've seen various examples for using Vectara with LlamaIndex, which provides the following benefits:\n",
    "* Vectara provides a complete RAG pipeline, so you don't have to deal with a lot of the details around data ingestion: pre-processing, chunking, embedding, etc. Instead all these steps are handled automatically and efficiently in Vectara. \n",
    "* Being a platform, Vectara uses its own internal Embedding model (Boomerang), its own vector storage, and its own LLM (Mockingbird) for summarization, so you don't have to maintain separate API keys and relationships with additional vendors or install other products.\n",
    "* Vectara is built for large scale GenAI applications, and with the vectara-agentic package, you can easily build and test advanced RAG applications at an enteprise scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
